From 9c29ac842efedda96c2fe090b98a4f85830ba93f Mon Sep 17 00:00:00 2001
From: Miguel Bernal Marin <miguel.bernal.marin@linux.intel.com>
Date: Thu, 1 Feb 2018 11:05:28 -0600
Subject: [PATCH 2001/2002] opae: add intel fpga drivers

This driver are taken from https://github.com/OPAE/opae-sdk and ported
to mainline kernel tree.

Signed-off-by: Miguel Bernal Marin <miguel.bernal.marin@linux.intel.com>
---
 drivers/fpga/intel/Kconfig         |   38 +
 drivers/fpga/intel/Makefile        |   24 +
 drivers/fpga/intel/afu-check.c     |  149 +++
 drivers/fpga/intel/afu-error.c     |  276 ++++++
 drivers/fpga/intel/afu.c           | 1177 +++++++++++++++++++++++
 drivers/fpga/intel/afu.h           |   80 ++
 drivers/fpga/intel/altera-asmip2.c |  466 +++++++++
 drivers/fpga/intel/altera-asmip2.h |   24 +
 drivers/fpga/intel/dma-region.c    |  372 ++++++++
 drivers/fpga/intel/feature-dev.c   |  368 ++++++++
 drivers/fpga/intel/feature-dev.h   | 1839 ++++++++++++++++++++++++++++++++++++
 drivers/fpga/intel/fme-dperf.c     |  415 ++++++++
 drivers/fpga/intel/fme-error.c     |  452 +++++++++
 drivers/fpga/intel/fme-iperf.c     |  771 +++++++++++++++
 drivers/fpga/intel/fme-main.c      | 1124 ++++++++++++++++++++++
 drivers/fpga/intel/fme-perf.c      |  715 ++++++++++++++
 drivers/fpga/intel/fme-pr.c        |  487 ++++++++++
 drivers/fpga/intel/fme.h           |   88 ++
 drivers/fpga/intel/pcie.c          | 1404 +++++++++++++++++++++++++++
 drivers/fpga/intel/pcie_check.c    |  141 +++
 drivers/fpga/intel/region.c        |  130 +++
 include/uapi/linux/intel-fpga.h    |  354 +++++++
 22 files changed, 10894 insertions(+)
 create mode 100644 drivers/fpga/intel/Kconfig
 create mode 100644 drivers/fpga/intel/Makefile
 create mode 100644 drivers/fpga/intel/afu-check.c
 create mode 100644 drivers/fpga/intel/afu-error.c
 create mode 100644 drivers/fpga/intel/afu.c
 create mode 100644 drivers/fpga/intel/afu.h
 create mode 100644 drivers/fpga/intel/altera-asmip2.c
 create mode 100644 drivers/fpga/intel/altera-asmip2.h
 create mode 100644 drivers/fpga/intel/dma-region.c
 create mode 100644 drivers/fpga/intel/feature-dev.c
 create mode 100644 drivers/fpga/intel/feature-dev.h
 create mode 100644 drivers/fpga/intel/fme-dperf.c
 create mode 100644 drivers/fpga/intel/fme-error.c
 create mode 100644 drivers/fpga/intel/fme-iperf.c
 create mode 100644 drivers/fpga/intel/fme-main.c
 create mode 100644 drivers/fpga/intel/fme-perf.c
 create mode 100644 drivers/fpga/intel/fme-pr.c
 create mode 100644 drivers/fpga/intel/fme.h
 create mode 100644 drivers/fpga/intel/pcie.c
 create mode 100644 drivers/fpga/intel/pcie_check.c
 create mode 100644 drivers/fpga/intel/region.c
 create mode 100644 include/uapi/linux/intel-fpga.h

diff --git a/drivers/fpga/intel/Kconfig b/drivers/fpga/intel/Kconfig
new file mode 100644
index 000000000000..3a7226811629
--- /dev/null
+++ b/drivers/fpga/intel/Kconfig
@@ -0,0 +1,38 @@
+#
+# Open Programmable Acceleration Engine (OPAE) kernel driver
+#
+
+config FPGA_INTEL_OPAE
+	bool "Enabling Open Programmable Acceleration Engine (OPAE) "
+	help
+	  Enable the Open Programmable Acceleration Engine (OPAE) driver.
+	  More info at https://01.org/OPAE
+
+
+if FPGA_INTEL_OPAE
+
+config FPGA_ALTERA_ASMIP2
+	tristate "OPAE ALTERA ASMIP2"
+	default m
+	help
+	  Enable OPAE ALTERA ASMIP2
+
+config FPGA_INTEL_PCI
+	tristate "OPAE PCI"
+	default m
+	help
+	  Enable OPAE PCI
+
+config FPGA_INTEL_FME
+	tristate "OPAE FME"
+	default m
+	help
+	  Enable OPAE FME
+
+config FPGA_INTEL_AFU
+	tristate "OPAE AFU"
+	default m
+	help
+	  Enable OPAE AFU
+
+endif # FPGA_INTEL_OPAE
diff --git a/drivers/fpga/intel/Makefile b/drivers/fpga/intel/Makefile
new file mode 100644
index 000000000000..2f83f1b20c27
--- /dev/null
+++ b/drivers/fpga/intel/Makefile
@@ -0,0 +1,24 @@
+
+ccflags-y += -Wno-unused-value -Wno-unused-label
+ccflags-y += -DCONFIG_AS_AVX512
+
+obj-$(CONFIG_FPGA_ALTERA_ASMIP2) += altera-asmip2.o
+obj-$(CONFIG_FPGA_INTEL_PCI) += intel-fpga-pci.o
+obj-$(CONFIG_FPGA_INTEL_FME) += intel-fpga-fme.o
+obj-$(CONFIG_FPGA_INTEL_AFU) += intel-fpga-afu.o
+
+intel-fpga-pci-y := pcie.o       \
+		    pcie_check.o \
+		    feature-dev.o
+
+intel-fpga-fme-y := fme-pr.o    \
+		    fme-iperf.o  \
+		    fme-dperf.o  \
+		    fme-error.o \
+		    fme-main.o
+
+intel-fpga-afu-y := afu.o        \
+		    region.o     \
+		    dma-region.o \
+		    afu-error.o  \
+		    afu-check.o
diff --git a/drivers/fpga/intel/afu-check.c b/drivers/fpga/intel/afu-check.c
new file mode 100644
index 000000000000..a75871e30c9e
--- /dev/null
+++ b/drivers/fpga/intel/afu-check.c
@@ -0,0 +1,149 @@
+#include "afu.h"
+
+static void port_check_reg(struct device *dev, void __iomem *addr,
+				const char *reg_name, u64 dflt)
+{
+	u64 value = readq(addr);
+
+	if (value != dflt)
+		dev_dbg(dev, "%s: incorrect value 0x%llx vs defautl 0x%llx\n",
+				reg_name, (unsigned long long)value,
+				(unsigned long long)dflt);
+}
+
+struct feature_port_header hdr_dflt = {
+	.port_mailbox		= 0x0000000000000000,
+	.scratchpad		= 0x0000000000000000,
+	.capability = {
+		.csr		= 0x0000000100010000,
+	},
+	.control = {
+		/* Port Reset Bit is cleared in PCIe driver */
+		.csr		= 0x0000000000000004,
+	},
+	.status = {
+		.csr		= 0x0000000000000000,
+	},
+	.rsvd2			= 0x0000000000000000,
+	.user_clk_freq_cmd0	= 0x0000000000000000,
+	.user_clk_freq_cmd1	= 0x0000000000000000,
+	.user_clk_freq_sts0	= 0x0000000000000000,
+	.user_clk_freq_sts1	= 0x0000000000000000,
+};
+
+int port_hdr_test(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_port_header *port_hdr = feature->ioaddr;
+
+	/* Check if default value of hardware registers matches with spec */
+	port_check_reg(&pdev->dev, &port_hdr->port_mailbox,
+			"hdr:port_mailbox", hdr_dflt.port_mailbox);
+	port_check_reg(&pdev->dev, &port_hdr->scratchpad,
+			"hdr:scratchpad", hdr_dflt.scratchpad);
+	port_check_reg(&pdev->dev, &port_hdr->capability,
+			"hdr:capability", hdr_dflt.capability.csr);
+	port_check_reg(&pdev->dev, &port_hdr->control,
+			"hdr:control", hdr_dflt.control.csr);
+	port_check_reg(&pdev->dev, &port_hdr->status,
+			"hdr:status", hdr_dflt.status.csr);
+	port_check_reg(&pdev->dev, &port_hdr->rsvd2,
+			"hdr:rsvd2", hdr_dflt.rsvd2);
+	port_check_reg(&pdev->dev, &port_hdr->user_clk_freq_cmd0,
+			"hdr:user_clk_cmd0", hdr_dflt.user_clk_freq_cmd0);
+	port_check_reg(&pdev->dev, &port_hdr->user_clk_freq_cmd1,
+			"hdr:user_clk_cmd1", hdr_dflt.user_clk_freq_cmd1);
+	port_check_reg(&pdev->dev, &port_hdr->user_clk_freq_sts0,
+			"hdr:user_clk_sts0", hdr_dflt.user_clk_freq_sts0);
+	port_check_reg(&pdev->dev, &port_hdr->user_clk_freq_sts1,
+			"hdr:user_clk_sts1", hdr_dflt.user_clk_freq_sts1);
+
+	dev_dbg(&pdev->dev, "%s finished\n", __func__);
+
+	return 0;
+}
+
+struct feature_port_error err_dflt = {
+	.error_mask = {
+		.csr		= 0x0000000000000000,
+	},
+	.port_error = {
+		.csr		= 0x0000000000000000,
+	},
+	.port_first_error = {
+		.csr		= 0x0000000000000000,
+	},
+	.malreq0 = {
+		.header_lsb	= 0x0000000000000000,
+	},
+	.malreq1 = {
+		.header_msb	= 0x0000000000000000,
+	},
+	.port_debug = {
+		.port_debug	= 0x0000000000000000,
+	},
+};
+
+int port_err_test(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_port_error *port_err = feature->ioaddr;
+
+	port_check_reg(&pdev->dev, &port_err->error_mask,
+			"err:error_mask", err_dflt.error_mask.csr);
+	port_check_reg(&pdev->dev, &port_err->port_error,
+			"err:port_error", err_dflt.port_error.csr);
+	port_check_reg(&pdev->dev, &port_err->port_first_error,
+			"err:port_first_err", err_dflt.port_first_error.csr);
+	port_check_reg(&pdev->dev, &port_err->malreq0,
+			"err:malreq0", err_dflt.malreq0.header_lsb);
+	port_check_reg(&pdev->dev, &port_err->malreq1,
+			"err:malreq1", err_dflt.malreq1.header_msb);
+	port_check_reg(&pdev->dev, &port_err->port_debug,
+			"err:port_debug", err_dflt.port_debug.port_debug);
+
+	dev_dbg(&pdev->dev, "%s finished\n", __func__);
+	return 0;
+}
+
+struct feature_port_umsg umsg_dflt = {
+	.capability = {
+		.csr		= 0x0000000000000008,
+	},
+	.baseaddr = {
+		.csr		= 0x0000000000000000,
+	},
+	.mode = {
+		.csr		= 0x0000000000000000,
+	},
+};
+
+int port_umsg_test(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_port_umsg *port_umsg = feature->ioaddr;
+
+	port_check_reg(&pdev->dev, &port_umsg->capability,
+				"umsg:capaiblity", umsg_dflt.capability.csr);
+	port_check_reg(&pdev->dev, &port_umsg->baseaddr,
+				"umsg:baseaddr", umsg_dflt.baseaddr.csr);
+	port_check_reg(&pdev->dev, &port_umsg->mode,
+				"umsg:mode", umsg_dflt.mode.csr);
+
+	dev_dbg(&pdev->dev, "%s finished\n", __func__);
+	return 0;
+}
+
+struct feature_port_stp stp_dflt = {
+	.stp_status = {
+		.csr		= 0x0000000000000000,
+	},
+};
+
+int port_stp_test(struct platform_device *pdev,	struct feature *feature)
+{
+	struct feature_port_stp *port_stp = feature->ioaddr;
+
+	port_check_reg(&pdev->dev, &port_stp->stp_status,
+				"stp:stp_csr", stp_dflt.stp_status.csr);
+
+	dev_dbg(&pdev->dev, "%s finished\n", __func__);
+	return 0;
+}
diff --git a/drivers/fpga/intel/afu-error.c b/drivers/fpga/intel/afu-error.c
new file mode 100644
index 000000000000..32b151c11098
--- /dev/null
+++ b/drivers/fpga/intel/afu-error.c
@@ -0,0 +1,276 @@
+/*
+ * Driver for FPGA Accelerated Function Unit (AFU) Error Handling
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Wu Hao <hao.wu@linux.intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include <linux/uaccess.h>
+#include "afu.h"
+
+/* Mask / Unmask Port Errors by the Error Mask register. */
+static void port_err_mask(struct device *dev, bool mask)
+{
+	struct feature_port_error *port_err;
+	struct feature_port_err_key err_mask;
+
+	port_err = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_ERROR);
+
+	if (mask)
+		err_mask.csr = PORT_ERR_MASK;
+	else
+		err_mask.csr = 0;
+
+	writeq(err_mask.csr, &port_err->error_mask);
+}
+
+/* Clear All Port Errors. */
+static int port_err_clear(struct device *dev, u64 err)
+{
+	struct feature_port_header *port_hdr;
+	struct feature_port_error *port_err;
+	struct feature_port_err_key mask;
+	struct feature_port_first_err_key first;
+	struct feature_port_status status;
+	int ret = 0;
+
+	port_err = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_ERROR);
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	/*
+	 * Clear All Port Errors
+	 *
+	 * - Check for AP6 State
+	 * - Halt Port by keeping Port in reset
+	 * - Set PORT Error mask to all 1 to mask errors
+	 * - Clear all errors
+	 * - Set Port mask to all 0 to enable errors
+	 * - All errors start capturing new errors
+	 * - Enable Port by pulling the port out of reset
+	 */
+
+	/* If device is still in AP6 state, can not clear any error.*/
+	status.csr = readq(&port_hdr->status);
+	if (status.power_state == PORT_POWER_STATE_AP6) {
+		dev_err(dev, "Could not clear errors, device in AP6 state.\n");
+		return -EBUSY;
+	}
+
+	/* Halt Port by keeping Port in reset */
+	ret = __fpga_port_disable(to_platform_device(dev));
+	if (ret)
+		return ret;
+
+	/* Mask all errors */
+	port_err_mask(dev, true);
+
+	/* Clear errors if err input matches with current port errors.*/
+	mask.csr = readq(&port_err->port_error);
+
+	if (mask.csr == err) {
+		writeq(mask.csr, &port_err->port_error);
+
+		first.csr = readq(&port_err->port_first_error);
+		writeq(first.csr, &port_err->port_first_error);
+	} else
+		ret = -EBUSY;
+
+	/* Clear mask */
+	port_err_mask(dev, false);
+
+	/* Enable the Port by clear the reset */
+	__fpga_port_enable(to_platform_device(dev));
+
+	return ret;
+}
+
+static ssize_t
+revision_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct feature_port_error *port_err;
+	struct feature_header header;
+
+	port_err = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_ERROR);
+
+	header.csr = readq(&port_err->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+static DEVICE_ATTR_RO(revision);
+
+static ssize_t
+errors_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct feature_port_error *port_err;
+	struct feature_port_err_key error;
+
+	port_err = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_ERROR);
+
+	error.csr = readq(&port_err->port_error);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n",
+				(unsigned long long)error.csr);
+}
+static DEVICE_ATTR_RO(errors);
+
+static ssize_t
+first_error_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct feature_port_error *port_err;
+	struct feature_port_first_err_key first_error;
+
+	port_err = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_ERROR);
+
+	first_error.csr = readq(&port_err->port_first_error);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n",
+				(unsigned long long)first_error.csr);
+}
+static DEVICE_ATTR_RO(first_error);
+
+static ssize_t first_malformed_req_show(struct device *dev,
+				struct device_attribute *attr, char *buf)
+{
+	struct feature_port_error *port_err;
+	struct feature_port_malformed_req0 malreq0;
+	struct feature_port_malformed_req1 malreq1;
+
+	port_err = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_ERROR);
+
+	malreq0.header_lsb = readq(&port_err->malreq0);
+	malreq1.header_msb = readq(&port_err->malreq1);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%016llx%016llx\n",
+				(unsigned long long)malreq1.header_msb,
+				(unsigned long long)malreq0.header_lsb);
+}
+static DEVICE_ATTR_RO(first_malformed_req);
+
+static ssize_t clear_store(struct device *dev,
+		struct device_attribute *attr, const char *buff, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	int ret;
+	u64 value;
+
+	if (kstrtou64(buff, 0, &value))
+		return -EINVAL;
+
+	WARN_ON(!is_feature_present(dev, PORT_FEATURE_ID_HEADER));
+
+	mutex_lock(&pdata->lock);
+	ret = port_err_clear(dev, value);
+	mutex_unlock(&pdata->lock);
+
+	if (ret)
+		return ret;
+	else
+		return count;
+}
+static DEVICE_ATTR_WO(clear);
+
+static struct attribute *port_err_attrs[] = {
+	&dev_attr_revision.attr,
+	&dev_attr_errors.attr,
+	&dev_attr_first_error.attr,
+	&dev_attr_first_malformed_req.attr,
+	&dev_attr_clear.attr,
+	NULL,
+};
+
+static struct attribute_group port_err_attr_group = {
+	.attrs = port_err_attrs,
+	.name = "errors",
+};
+
+static int port_err_init(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_afu *afu;
+
+	dev_dbg(&pdev->dev, "PORT ERR Init.\n");
+
+	mutex_lock(&pdata->lock);
+	port_err_mask(&pdev->dev, false);
+	afu = fpga_pdata_get_private(pdata);
+	if (feature->ctx_num)
+		afu->capability |= FPGA_PORT_CAP_ERR_IRQ;
+	mutex_unlock(&pdata->lock);
+
+	return sysfs_create_group(&pdev->dev.kobj, &port_err_attr_group);
+}
+
+static void port_err_uinit(struct platform_device *pdev,
+					struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "PORT ERR UInit.\n");
+
+	sysfs_remove_group(&pdev->dev.kobj, &port_err_attr_group);
+}
+
+static long port_err_set_irq(struct platform_device *pdev,
+			     struct feature *feature, unsigned long arg)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_port_err_irq_set hdr;
+	struct fpga_afu *afu;
+	unsigned long minsz;
+	long ret;
+
+	minsz = offsetofend(struct fpga_port_err_irq_set, evtfd);
+
+	if (copy_from_user(&hdr, (void __user *)arg, minsz))
+		return -EFAULT;
+
+	if (hdr.argsz < minsz || hdr.flags)
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	afu = fpga_pdata_get_private(pdata);
+	if (!(afu->capability & FPGA_PORT_CAP_ERR_IRQ)) {
+		mutex_unlock(&pdata->lock);
+		return -ENODEV;
+	}
+
+	ret = fpga_msix_set_block(feature, 0, 1, &hdr.evtfd);
+	mutex_unlock(&pdata->lock);
+
+	return ret;
+}
+
+static long
+port_err_ioctl(struct platform_device *pdev, struct feature *feature,
+	       unsigned int cmd, unsigned long arg)
+{
+	long ret;
+
+	switch (cmd) {
+	case FPGA_PORT_ERR_SET_IRQ:
+		ret = port_err_set_irq(pdev, feature, arg);
+		break;
+	default:
+		dev_dbg(&pdev->dev, "%x cmd not handled", cmd);
+		return -ENODEV;
+	}
+
+	return ret;
+}
+
+struct feature_ops port_err_ops = {
+	.init = port_err_init,
+	.uinit = port_err_uinit,
+	.ioctl = port_err_ioctl,
+	.test = port_err_test,
+};
diff --git a/drivers/fpga/intel/afu.c b/drivers/fpga/intel/afu.c
new file mode 100644
index 000000000000..27e31dfc2d64
--- /dev/null
+++ b/drivers/fpga/intel/afu.c
@@ -0,0 +1,1177 @@
+/*
+ * Driver for FPGA Accelerated Function Unit (AFU)
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Wu Hao <hao.wu@linux.intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/uaccess.h>
+#include <linux/stddef.h>
+#include <linux/errno.h>
+#include <linux/delay.h>
+#include <linux/fs.h>
+#include <linux/dma-mapping.h>
+#include <linux/intel-fpga.h>
+
+#include "afu.h"
+
+/* sysfs attributes for port_hdr feature */
+static ssize_t
+revision_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct feature_port_header *port_hdr
+		= get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+	struct feature_header header;
+
+	header.csr = readq(&port_hdr->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+
+static DEVICE_ATTR_RO(revision);
+
+static ssize_t
+id_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	int id = fpga_port_id(to_platform_device(dev));
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", id);
+}
+static DEVICE_ATTR_RO(id);
+
+static ssize_t
+ltr_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct feature_port_header *port_hdr;
+	struct feature_port_control control;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	control.csr = readq(&port_hdr->control);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", control.latency_tolerance);
+}
+static DEVICE_ATTR_RO(ltr);
+
+static ssize_t
+ap1_event_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_port_header *port_hdr;
+	struct feature_port_status status;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	mutex_lock(&pdata->lock);
+	status.csr = readq(&port_hdr->status);
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", status.ap1_event);
+}
+
+static ssize_t
+ap1_event_store(struct device *dev, struct device_attribute *attr,
+		const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_port_header *port_hdr;
+	struct feature_port_status status;
+	u8 ap1_event;
+	int err;
+
+	err = kstrtou8(buf, 0, &ap1_event);
+	if (err)
+		return err;
+
+	if (ap1_event != 1)
+		return -EINVAL;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	mutex_lock(&pdata->lock);
+	status.csr = readq(&port_hdr->status);
+	status.ap1_event = ap1_event;
+	writeq(status.csr, &port_hdr->status);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+static DEVICE_ATTR_RW(ap1_event);
+
+static ssize_t
+ap2_event_show(struct device *dev, struct device_attribute *attr,
+	       char *buf)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_port_header *port_hdr;
+	struct feature_port_status status;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	mutex_lock(&pdata->lock);
+	status.csr = readq(&port_hdr->status);
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", status.ap2_event);
+}
+
+static ssize_t
+ap2_event_store(struct device *dev, struct device_attribute *attr,
+		const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_port_header *port_hdr;
+	struct feature_port_status status;
+	u8 ap2_event;
+	int err;
+
+	err = kstrtou8(buf, 0, &ap2_event);
+	if (err)
+		return err;
+
+	if (ap2_event != 1)
+		return -EINVAL;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	mutex_lock(&pdata->lock);
+	status.csr = readq(&port_hdr->status);
+	status.ap2_event = ap2_event;
+	writeq(status.csr, &port_hdr->status);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+static DEVICE_ATTR_RW(ap2_event);
+
+static ssize_t
+power_state_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_port_header *port_hdr;
+	struct feature_port_status status;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	mutex_lock(&pdata->lock);
+	status.csr = readq(&port_hdr->status);
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%x\n", status.power_state);
+}
+static DEVICE_ATTR_RO(power_state);
+
+static ssize_t
+userclk_freqcmd_show(struct device *dev, struct device_attribute *attr,
+		     char *buf)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_port_header *port_hdr;
+	u64 userclk_freq_cmd;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	mutex_lock(&pdata->lock);
+	userclk_freq_cmd = readq(&port_hdr->user_clk_freq_cmd0);
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", userclk_freq_cmd);
+}
+
+static ssize_t
+userclk_freqcmd_store(struct device *dev, struct device_attribute *attr,
+		      const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_port_header *port_hdr;
+	u64 userclk_freq_cmd;
+	int err;
+
+	err = kstrtou64(buf, 0, &userclk_freq_cmd);
+	if (err)
+		return err;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	mutex_lock(&pdata->lock);
+	writeq(userclk_freq_cmd, &port_hdr->user_clk_freq_cmd0);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+static DEVICE_ATTR_RW(userclk_freqcmd);
+
+static ssize_t
+userclk_freqcntrcmd_show(struct device *dev, struct device_attribute *attr,
+			 char *buf)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_port_header *port_hdr;
+	u64 userclk_freqcntr_cmd;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	mutex_lock(&pdata->lock);
+	userclk_freqcntr_cmd = readq(&port_hdr->user_clk_freq_cmd1);
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", userclk_freqcntr_cmd);
+}
+
+static ssize_t
+userclk_freqcntrcmd_store(struct device *dev, struct device_attribute *attr,
+			  const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_port_header *port_hdr;
+	u64 userclk_freqcntr_cmd;
+	int err;
+
+	err = kstrtou64(buf, 0, &userclk_freqcntr_cmd);
+	if (err)
+		return err;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	mutex_lock(&pdata->lock);
+	writeq(userclk_freqcntr_cmd, &port_hdr->user_clk_freq_cmd1);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+static DEVICE_ATTR_RW(userclk_freqcntrcmd);
+
+static ssize_t
+userclk_freqsts_show(struct device *dev, struct device_attribute *attr,
+		     char *buf)
+{
+	struct feature_port_header *port_hdr;
+	u64 userclk_freq_sts;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	userclk_freq_sts = readq(&port_hdr->user_clk_freq_sts0);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", userclk_freq_sts);
+}
+static DEVICE_ATTR_RO(userclk_freqsts);
+
+static ssize_t
+userclk_freqcntrsts_show(struct device *dev, struct device_attribute *attr,
+			 char *buf)
+{
+	struct feature_port_header *port_hdr;
+	u64 userclk_freqcntr_sts;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	userclk_freqcntr_sts = readq(&port_hdr->user_clk_freq_sts1);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", userclk_freqcntr_sts);
+}
+static DEVICE_ATTR_RO(userclk_freqcntrsts);
+
+static const struct attribute *port_hdr_attrs[] = {
+	&dev_attr_revision.attr,
+	&dev_attr_id.attr,
+	&dev_attr_ltr.attr,
+	&dev_attr_ap1_event.attr,
+	&dev_attr_ap2_event.attr,
+	&dev_attr_power_state.attr,
+	&dev_attr_userclk_freqcmd.attr,
+	&dev_attr_userclk_freqcntrcmd.attr,
+	&dev_attr_userclk_freqsts.attr,
+	&dev_attr_userclk_freqcntrsts.attr,
+	NULL,
+};
+
+static int port_hdr_init(struct platform_device *pdev, struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "PORT HDR Init.\n");
+
+	fpga_port_reset(pdev);
+
+	return sysfs_create_files(&pdev->dev.kobj, port_hdr_attrs);
+}
+
+static void port_hdr_uinit(struct platform_device *pdev,
+					struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "PORT HDR UInit.\n");
+
+	sysfs_remove_files(&pdev->dev.kobj, port_hdr_attrs);
+}
+
+static long
+port_hdr_ioctl(struct platform_device *pdev, struct feature *feature,
+					unsigned int cmd, unsigned long arg)
+{
+	long ret;
+
+	switch (cmd) {
+	case FPGA_PORT_RESET:
+		if (!arg)
+			ret = fpga_port_reset(pdev);
+		else
+			ret = -EINVAL;
+		break;
+	default:
+		dev_dbg(&pdev->dev, "%x cmd not handled", cmd);
+		ret = -ENODEV;
+	}
+
+	return ret;
+}
+
+struct feature_ops port_hdr_ops = {
+	.init = port_hdr_init,
+	.uinit = port_hdr_uinit,
+	.ioctl = port_hdr_ioctl,
+	.test = port_hdr_test,
+};
+
+/* sysfs attributes for port_uafu feature */
+static ssize_t
+afu_id_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_port_header *port_hdr =
+			get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_UAFU);
+	u64 guidl;
+	u64 guidh;
+
+	mutex_lock(&pdata->lock);
+	if (pdata->disable_count) {
+		mutex_unlock(&pdata->lock);
+		return -EBUSY;
+	}
+
+	guidl = readq(&port_hdr->afu_header.guid.b[0]);
+	guidh = readq(&port_hdr->afu_header.guid.b[8]);
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "%016llx%016llx\n", guidh, guidl);
+}
+static DEVICE_ATTR_RO(afu_id);
+
+static const struct attribute *port_uafu_attrs[] = {
+	&dev_attr_afu_id.attr,
+	NULL
+};
+
+static int port_afu_init(struct platform_device *pdev, struct feature *feature)
+{
+	struct resource *res = &pdev->resource[feature->resource_index];
+	u32 flags = FPGA_REGION_READ | FPGA_REGION_WRITE | FPGA_REGION_MMAP;
+	int ret;
+
+	dev_dbg(&pdev->dev, "PORT AFU Init.\n");
+
+	ret = afu_region_add(dev_get_platdata(&pdev->dev),
+			     FPGA_PORT_INDEX_UAFU, resource_size(res),
+			     res->start, flags);
+	if (ret)
+		return ret;
+
+	return sysfs_create_files(&pdev->dev.kobj, port_uafu_attrs);
+}
+
+static void port_afu_uinit(struct platform_device *pdev,
+					struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "PORT AFU UInit.\n");
+
+	sysfs_remove_files(&pdev->dev.kobj, port_uafu_attrs);
+}
+
+static long port_afu_set_irq(struct platform_device *pdev,
+			struct feature *feature, unsigned long arg)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_port_uafu_irq_set hdr;
+	struct fpga_afu *afu;
+	unsigned long minsz;
+	int32_t *fds = NULL;
+	long ret = 0;
+
+	minsz = offsetofend(struct fpga_port_uafu_irq_set, count);
+
+	if (copy_from_user(&hdr, (void __user *)arg, minsz))
+		return -EFAULT;
+
+	if (hdr.argsz < minsz || hdr.flags)
+		return -EINVAL;
+
+	if ((hdr.start + hdr.count > feature->ctx_num) ||
+		(hdr.start + hdr.count < hdr.start) || !hdr.count)
+		return -EINVAL;
+
+	fds = memdup_user((void __user *)(arg + minsz),
+			  hdr.count * sizeof(int32_t));
+	if (IS_ERR(fds))
+		return PTR_ERR(fds);
+
+	mutex_lock(&pdata->lock);
+	afu = fpga_pdata_get_private(pdata);
+	if (!(afu->capability & FPGA_PORT_CAP_UAFU_IRQ)) {
+		mutex_unlock(&pdata->lock);
+		kfree(fds);
+		return -ENODEV;
+	}
+	ret = fpga_msix_set_block(feature, hdr.start, hdr.count, fds);
+	mutex_unlock(&pdata->lock);
+
+	kfree(fds);
+	return ret;
+}
+
+struct feature_ops port_afu_ops = {
+	.init = port_afu_init,
+	.uinit = port_afu_uinit,
+};
+
+static u8 port_umsg_get_num(struct device *dev)
+{
+	struct feature_port_umsg *port_umsg;
+	struct feature_port_umsg_cap capability;
+
+	port_umsg = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_UMSG);
+
+	capability.csr = readq(&port_umsg->capability);
+
+	return capability.umsg_allocated;
+}
+
+#define UMSG_EN_POLL_INVL 10 /* us */
+#define UMSG_EN_POLL_TIMEOUT 1000 /* us */
+
+static int port_umsg_enable(struct device *dev, bool enable)
+{
+	struct feature_port_umsg *port_umsg;
+	struct feature_port_umsg_cap capability;
+
+	port_umsg = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_UMSG);
+
+	capability.csr = readq(&port_umsg->capability);
+
+	/* Return directly if UMSG is already enabled/disabled */
+	if ((enable && capability.umsg_enable) ||
+			!(enable || capability.umsg_enable))
+		return 0;
+
+	capability.umsg_enable = enable;
+	writeq(capability.csr, &port_umsg->capability);
+
+	/*
+	 * Each time umsg engine enabled/disabled, driver polls the
+	 * init_complete bit for confirmation.
+	 */
+	capability.umsg_init_complete = !!enable;
+
+	if (fpga_wait_register_field(umsg_init_complete, capability,
+				     &port_umsg->capability,
+				     UMSG_EN_POLL_TIMEOUT, UMSG_EN_POLL_INVL)) {
+		dev_err(dev, "timeout, fail to %s umsg\n",
+					enable ? "enable" : "disable");
+		return -ETIMEDOUT;
+	}
+
+	return 0;
+}
+
+static bool port_umsg_is_enabled(struct device *dev)
+{
+	struct feature_port_umsg *port_umsg;
+	struct feature_port_umsg_cap capability;
+
+	port_umsg = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_UMSG);
+
+	capability.csr = readq(&port_umsg->capability);
+
+	return capability.umsg_enable;
+}
+
+static void port_umsg_set_mode(struct device *dev, u32 mode)
+{
+	struct feature_port_umsg *port_umsg;
+	struct feature_port_umsg_mode umode;
+
+	port_umsg = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_UMSG);
+
+	umode.csr = readq(&port_umsg->mode);
+	umode.umsg_hint_enable = mode;
+	writeq(umode.csr, &port_umsg->mode);
+}
+
+static u64 port_umsg_get_addr(struct device *dev)
+{
+	struct feature_port_umsg *port_umsg;
+	struct feature_port_umsg_baseaddr baseaddr;
+
+	port_umsg = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_UMSG);
+
+	baseaddr.csr = readq(&port_umsg->baseaddr);
+
+	return baseaddr.base_addr;
+}
+
+static void port_umsg_set_addr(struct device *dev, u64 iova)
+{
+	struct feature_port_umsg *port_umsg;
+	struct feature_port_umsg_baseaddr baseaddr;
+
+	port_umsg = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_UMSG);
+
+	baseaddr.csr = readq(&port_umsg->baseaddr);
+	baseaddr.base_addr = iova;
+	writeq(baseaddr.csr, &port_umsg->baseaddr);
+}
+
+static int afu_port_umsg_enable(struct device *dev, bool enable)
+{
+	if (enable && !port_umsg_get_addr(dev)) {
+		dev_dbg(dev, "umsg base addr is not configured\n");
+		return -EIO;
+	}
+
+	return port_umsg_enable(dev, enable);
+}
+
+static int afu_port_umsg_set_addr(struct device *dev, u64 iova)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+	struct fpga_afu_dma_region *dma_region;
+	u64 size = afu->num_umsgs * PAGE_SIZE;
+
+	/* Make sure base addr is configured only when umsg is disabled */
+	if (port_umsg_is_enabled(dev)) {
+		dev_dbg(dev, "umsg is still enabled\n");
+		return -EIO;
+	}
+
+	if (iova) {
+		/* Check input, only accept page-aligned region for umsg */
+		if (!PAGE_ALIGNED(iova))
+			return -EINVAL;
+
+		/* Check overflow */
+		if (iova + size < iova)
+			return -EINVAL;
+
+		/* Check if any dma region matches with iova for umsg */
+		dma_region = afu_dma_region_find(pdata, iova, size);
+		if (!dma_region) {
+			dev_dbg(dev, "dma region not found for umsg\n");
+			return -EINVAL;
+		}
+
+		port_umsg_set_addr(dev, iova);
+
+		/* Mark the region to prevent it from unexpected unmapping */
+		dma_region->in_use = true;
+	} else {
+		/* Read current iova from hardware */
+		iova = port_umsg_get_addr(dev);
+		if (!iova)
+			return 0;
+
+		/* Check overflow */
+		if (WARN_ON(iova + size < iova))
+			return -EINVAL;
+
+		/* Check if any dma region matches with iova for umsg */
+		dma_region = afu_dma_region_find(pdata, iova, size);
+		if (WARN_ON(!dma_region))
+			return -ENODEV;
+
+		port_umsg_set_addr(dev, 0);
+
+		/* Unmark the region */
+		dma_region->in_use = false;
+	}
+
+	return 0;
+}
+
+static int afu_port_umsg_set_mode(struct device *dev, u32 mode)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+
+	if (mode >> afu->num_umsgs) {
+		dev_dbg(dev, "invaild UMsg config hint_bitmap\n");
+		return -EINVAL;
+	}
+
+	port_umsg_set_mode(dev, mode);
+
+	return 0;
+}
+
+static void afu_port_umsg_halt(struct device *dev)
+{
+	if (is_feature_present(dev, PORT_FEATURE_ID_UMSG)) {
+		afu_port_umsg_enable(dev, false);
+		afu_port_umsg_set_addr(dev, 0);
+		afu_port_umsg_set_mode(dev, 0);
+	}
+}
+
+static long afu_umsg_ioctl_enable(struct feature_platform_data *pdata,
+				  bool enable, unsigned long arg)
+{
+	long ret;
+
+	if (arg)
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	ret = afu_port_umsg_enable(&pdata->dev->dev, enable);
+	mutex_unlock(&pdata->lock);
+
+	return ret;
+}
+
+static long
+afu_umsg_ioctl_set_mode(struct feature_platform_data *pdata, void __user *arg)
+{
+	struct fpga_port_umsg_cfg uconfig;
+	unsigned long minsz;
+	long ret;
+
+	minsz = offsetofend(struct fpga_port_umsg_cfg, hint_bitmap);
+
+	if (copy_from_user(&uconfig, arg, minsz))
+		return -EFAULT;
+
+	if (uconfig.argsz < minsz || uconfig.flags)
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	ret = afu_port_umsg_set_mode(&pdata->dev->dev, uconfig.hint_bitmap);
+	mutex_unlock(&pdata->lock);
+
+	return ret;
+}
+
+static long afu_umsg_ioctl_set_base_addr(struct feature_platform_data *pdata,
+						void __user *arg)
+{
+	struct fpga_port_umsg_base_addr baseaddr;
+	unsigned long minsz;
+	long ret;
+
+	minsz = offsetofend(struct fpga_port_umsg_base_addr, iova);
+
+	if (copy_from_user(&baseaddr, arg, minsz))
+		return -EFAULT;
+
+	if (baseaddr.argsz < minsz || baseaddr.flags)
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	ret = afu_port_umsg_set_addr(&pdata->dev->dev, baseaddr.iova);
+	mutex_unlock(&pdata->lock);
+
+	return ret;
+}
+
+static int port_umsg_init(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_afu *afu;
+
+	dev_dbg(&pdev->dev, "PORT UMSG Init.\n");
+
+	mutex_lock(&pdata->lock);
+	afu = fpga_pdata_get_private(pdata);
+	afu->num_umsgs = port_umsg_get_num(&pdev->dev);
+	WARN_ON(!afu->num_umsgs || afu->num_umsgs > MAX_PORT_UMSG_NUM);
+	mutex_unlock(&pdata->lock);
+
+	return 0;
+}
+
+static void port_umsg_uinit(struct platform_device *pdev,
+					struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "PORT UMSG UInit.\n");
+}
+
+static long
+port_umsg_ioctl(struct platform_device *pdev, struct feature *feature,
+					unsigned int cmd, unsigned long arg)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	long ret;
+
+	switch (cmd) {
+	case FPGA_PORT_UMSG_ENABLE:
+		return afu_umsg_ioctl_enable(pdata, true, arg);
+	case FPGA_PORT_UMSG_DISABLE:
+		return afu_umsg_ioctl_enable(pdata, false, arg);
+	case FPGA_PORT_UMSG_SET_MODE:
+		return afu_umsg_ioctl_set_mode(pdata, (void __user *)arg);
+	case FPGA_PORT_UMSG_SET_BASE_ADDR:
+		return afu_umsg_ioctl_set_base_addr(pdata, (void __user *)arg);
+	default:
+		dev_dbg(&pdev->dev, "%x cmd not handled", cmd);
+		ret = -ENODEV;
+	}
+
+	return ret;
+}
+
+struct feature_ops port_umsg_ops = {
+	.init = port_umsg_init,
+	.uinit = port_umsg_uinit,
+	.test = port_umsg_test,
+	.ioctl = port_umsg_ioctl,
+};
+
+static int port_stp_init(struct platform_device *pdev, struct feature *feature)
+{
+	struct resource *res = &pdev->resource[feature->resource_index];
+	u32 flags = FPGA_REGION_READ | FPGA_REGION_WRITE | FPGA_REGION_MMAP;
+
+	dev_dbg(&pdev->dev, "PORT STP Init.\n");
+
+	return afu_region_add(dev_get_platdata(&pdev->dev),
+			      FPGA_PORT_INDEX_STP, resource_size(res),
+			      res->start, flags);
+}
+
+static void port_stp_uinit(struct platform_device *pdev,
+					struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "PORT STP UInit.\n");
+}
+
+struct feature_ops port_stp_ops = {
+	.init = port_stp_init,
+	.uinit = port_stp_uinit,
+	.test = port_stp_test,
+};
+
+static int port_uint_init(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_afu *afu;
+
+	mutex_lock(&pdata->lock);
+	afu = fpga_pdata_get_private(pdata);
+	if (feature->ctx_num) {
+		afu->capability |= FPGA_PORT_CAP_UAFU_IRQ;
+		afu->num_uafu_irqs = feature->ctx_num;
+	}
+	mutex_unlock(&pdata->lock);
+
+	return 0;
+}
+
+static void port_uint_uinit(struct platform_device *pdev,
+			    struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "PORT UINT UInit.\n");
+}
+
+static long
+port_uint_ioctl(struct platform_device *pdev, struct feature *feature,
+		unsigned int cmd, unsigned long arg)
+{
+	long ret;
+
+	switch (cmd) {
+	case FPGA_PORT_UAFU_SET_IRQ:
+		ret = port_afu_set_irq(pdev, feature, arg);
+		break;
+	default:
+		dev_dbg(&pdev->dev, "%x cmd not handled", cmd);
+		return -ENODEV;
+	}
+	return ret;
+}
+
+struct feature_ops port_uint_ops = {
+	.init = port_uint_init,
+	.uinit = port_uint_uinit,
+	.ioctl = port_uint_ioctl,
+};
+
+static struct feature_driver port_feature_drvs[] = {
+	{
+		.name = PORT_FEATURE_HEADER,
+		.ops = &port_hdr_ops,
+	},
+	{
+		.name = PORT_FEATURE_UAFU,
+		.ops = &port_afu_ops,
+	},
+	{
+		.name = PORT_FEATURE_ERR,
+		.ops = &port_err_ops,
+	},
+	{
+		.name = PORT_FEATURE_UMSG,
+		.ops = &port_umsg_ops,
+	},
+	{
+		.name = PORT_FEATURE_UINT,
+		.ops = &port_uint_ops,
+	},
+	{
+		.name = PORT_FEATURE_STP,
+		.ops = &port_stp_ops,
+	},
+	{
+		.ops = NULL,
+	}
+};
+
+static int afu_open(struct inode *inode, struct file *filp)
+{
+	struct platform_device *fdev = fpga_inode_to_feature_dev(inode);
+	struct feature_platform_data *pdata;
+	int ret;
+
+	pdata = dev_get_platdata(&fdev->dev);
+	if (WARN_ON(!pdata))
+		return -ENODEV;
+
+	if (filp->f_flags & O_EXCL)
+		ret = feature_dev_use_excl_begin(pdata);
+	else
+		ret = feature_dev_use_begin(pdata);
+
+	if (ret)
+		return ret;
+
+	dev_dbg(&fdev->dev, "Device File Opened %d Times\n", pdata->open_count);
+	filp->private_data = fdev;
+
+	return 0;
+}
+
+static int afu_release(struct inode *inode, struct file *filp)
+{
+	struct platform_device *pdev = filp->private_data;
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+
+	dev_dbg(&pdev->dev, "Device File Release\n");
+	mutex_lock(&pdata->lock);
+	__feature_dev_use_end(pdata);
+
+	if (!pdata->open_count) {
+		fpga_msix_set_block(&pdata->features[PORT_FEATURE_ID_ERROR], 0,
+			pdata->features[PORT_FEATURE_ID_ERROR].ctx_num, NULL);
+		fpga_msix_set_block(&pdata->features[PORT_FEATURE_ID_UINT], 0,
+			pdata->features[PORT_FEATURE_ID_UINT].ctx_num, NULL);
+		afu_port_umsg_halt(&pdata->dev->dev);
+		__fpga_port_reset(pdev);
+		afu_dma_region_destroy(pdata);
+	}
+	mutex_unlock(&pdata->lock);
+	return 0;
+}
+
+static long afu_ioctl_check_extension(struct feature_platform_data *pdata,
+				     unsigned long arg)
+{
+	/* No extension support for now */
+	return 0;
+}
+
+static long
+afu_ioctl_get_info(struct feature_platform_data *pdata, void __user *arg)
+{
+	struct fpga_port_info info;
+	struct fpga_afu *afu;
+	unsigned long minsz;
+
+	minsz = offsetofend(struct fpga_port_info, num_uafu_irqs);
+
+	if (copy_from_user(&info, arg, minsz))
+		return -EFAULT;
+
+	if (info.argsz < minsz)
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	afu = fpga_pdata_get_private(pdata);
+	info.flags = 0;
+	info.capability = afu->capability;
+	info.num_regions = afu->num_regions;
+	info.num_umsgs = afu->num_umsgs;
+	info.num_uafu_irqs = afu->num_uafu_irqs;
+	mutex_unlock(&pdata->lock);
+
+	if (copy_to_user(arg, &info, sizeof(info)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static long
+afu_ioctl_get_region_info(struct feature_platform_data *pdata, void __user *arg)
+{
+	struct fpga_port_region_info rinfo;
+	struct fpga_afu_region region;
+	unsigned long minsz;
+	long ret;
+
+	minsz = offsetofend(struct fpga_port_region_info, offset);
+
+	if (copy_from_user(&rinfo, arg, minsz))
+		return -EFAULT;
+
+	if (rinfo.argsz < minsz || rinfo.padding)
+		return -EINVAL;
+
+	ret = afu_get_region_by_index(pdata, rinfo.index, &region);
+	if (ret)
+		return ret;
+
+	rinfo.flags = region.flags;
+	rinfo.size = region.size;
+	rinfo.offset = region.offset;
+
+	if (copy_to_user(arg, &rinfo, sizeof(rinfo)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static long
+afu_ioctl_dma_map(struct feature_platform_data *pdata, void __user *arg)
+{
+	struct fpga_port_dma_map map;
+	unsigned long minsz;
+	long ret;
+
+	minsz = offsetofend(struct fpga_port_dma_map, iova);
+
+	if (copy_from_user(&map, arg, minsz))
+		return -EFAULT;
+
+	if (map.argsz < minsz || map.flags)
+		return -EINVAL;
+
+	ret = afu_dma_map_region(pdata, map.user_addr, map.length, &map.iova);
+	if (ret)
+		return ret;
+
+	if (copy_to_user(arg, &map, sizeof(map))) {
+		afu_dma_unmap_region(pdata, map.iova);
+		return -EFAULT;
+	}
+
+	dev_dbg(&pdata->dev->dev, "dma map: ua=%llx, len=%llx, iova=%llx\n",
+				(unsigned long long)map.user_addr,
+				(unsigned long long)map.length,
+				(unsigned long long)map.iova);
+
+	return 0;
+}
+
+static long
+afu_ioctl_dma_unmap(struct feature_platform_data *pdata, void __user *arg)
+{
+	struct fpga_port_dma_unmap unmap;
+	unsigned long minsz;
+
+	minsz = offsetofend(struct fpga_port_dma_unmap, iova);
+
+	if (copy_from_user(&unmap, arg, minsz))
+		return -EFAULT;
+
+	if (unmap.argsz < minsz || unmap.flags)
+		return -EINVAL;
+
+	return afu_dma_unmap_region(pdata, unmap.iova);
+}
+
+static long afu_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	struct platform_device *pdev = filp->private_data;
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct feature *f;
+	long ret;
+
+	dev_dbg(&pdev->dev, "%s cmd 0x%x\n", __func__, cmd);
+
+	switch (cmd) {
+	case FPGA_GET_API_VERSION:
+		return FPGA_API_VERSION;
+	case FPGA_CHECK_EXTENSION:
+		return afu_ioctl_check_extension(pdata, arg);
+	case FPGA_PORT_GET_INFO:
+		return afu_ioctl_get_info(pdata, (void __user *)arg);
+	case FPGA_PORT_GET_REGION_INFO:
+		return afu_ioctl_get_region_info(pdata, (void __user *)arg);
+	case FPGA_PORT_DMA_MAP:
+		return afu_ioctl_dma_map(pdata, (void __user *)arg);
+	case FPGA_PORT_DMA_UNMAP:
+		return afu_ioctl_dma_unmap(pdata, (void __user *)arg);
+	default:
+		/*
+		 * Let sub-feature's ioctl function to handle the cmd
+		 * Sub-feature's ioctl returns -ENODEV when cmd is not
+		 * handled in this sub feature, and returns 0 and other
+		 * error code if cmd is handled.
+		 */
+		fpga_dev_for_each_feature(pdata, f)
+			if (f->ops && f->ops->ioctl) {
+				ret = f->ops->ioctl(pdev, f, cmd, arg);
+				if (ret == -ENODEV)
+					continue;
+				else
+					return ret;
+			}
+	}
+
+	return -EINVAL;
+}
+
+static int afu_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	struct fpga_afu_region region;
+	struct platform_device *pdev = filp->private_data;
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	u64 size = vma->vm_end - vma->vm_start;
+	u64 offset;
+	int ret;
+
+	if (!(vma->vm_flags & VM_SHARED) && (vma->vm_flags & VM_WRITE))
+		return -EINVAL;
+
+	offset = vma->vm_pgoff << PAGE_SHIFT;
+	ret = afu_get_region_by_offset(pdata, offset, size, &region);
+	if (ret)
+		return ret;
+
+	if (!(region.flags & FPGA_REGION_MMAP))
+		return -EINVAL;
+
+	if ((vma->vm_flags & VM_READ) && !(region.flags & FPGA_REGION_READ))
+		return -EPERM;
+
+	if ((vma->vm_flags & VM_WRITE) && !(region.flags & FPGA_REGION_WRITE))
+		return -EPERM;
+
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+	return remap_pfn_range(vma, vma->vm_start,
+			(region.phys + (offset - region.offset)) >> PAGE_SHIFT,
+			size, vma->vm_page_prot);
+}
+
+static const struct file_operations afu_fops = {
+	.owner = THIS_MODULE,
+	.open = afu_open,
+	.release = afu_release,
+	.unlocked_ioctl = afu_ioctl,
+	.mmap = afu_mmap,
+};
+
+static int afu_dev_init(struct platform_device *pdev)
+{
+	struct fpga_afu *afu;
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+
+	afu = devm_kzalloc(&pdev->dev, sizeof(*afu), GFP_KERNEL);
+	if (!afu)
+		return -ENOMEM;
+
+	afu->pdata = pdata;
+
+	mutex_lock(&pdata->lock);
+	fpga_pdata_set_private(pdata, afu);
+	afu_region_init(pdata);
+	afu_dma_region_init(pdata);
+	mutex_unlock(&pdata->lock);
+	return 0;
+}
+
+static int afu_dev_destroy(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_afu *afu;
+
+	mutex_lock(&pdata->lock);
+	afu = fpga_pdata_get_private(pdata);
+	afu_region_destroy(pdata);
+	afu_dma_region_destroy(pdata);
+	fpga_pdata_set_private(pdata, NULL);
+	mutex_unlock(&pdata->lock);
+
+	devm_kfree(&pdev->dev, afu);
+	return 0;
+}
+
+static int afu_probe(struct platform_device *pdev)
+{
+	int ret;
+
+	dev_dbg(&pdev->dev, "%s\n", __func__);
+
+	ret = afu_dev_init(pdev);
+	if (ret)
+		goto exit;
+
+	ret = fpga_dev_feature_init(pdev, port_feature_drvs);
+	if (ret)
+		goto dev_destroy;
+
+	ret = fpga_register_dev_ops(pdev, &afu_fops, THIS_MODULE);
+	if (ret) {
+		fpga_dev_feature_uinit(pdev);
+		goto dev_destroy;
+	}
+
+	return 0;
+
+dev_destroy:
+	afu_dev_destroy(pdev);
+exit:
+	return ret;
+}
+
+static int afu_remove(struct platform_device *pdev)
+{
+	dev_dbg(&pdev->dev, "%s\n", __func__);
+
+	fpga_dev_feature_uinit(pdev);
+	fpga_unregister_dev_ops(pdev);
+	afu_dev_destroy(pdev);
+	return 0;
+}
+
+static struct platform_driver afu_driver = {
+	.driver	= {
+		.name    = "intel-fpga-port",
+	},
+	.probe   = afu_probe,
+	.remove  = afu_remove,
+};
+
+module_platform_driver(afu_driver);
+
+MODULE_DESCRIPTION("FPGA Accelerated Function Unit driver");
+MODULE_AUTHOR("Intel Corporation");
+MODULE_LICENSE("GPL v2");
+MODULE_ALIAS("platform:intel-fpga-port");
diff --git a/drivers/fpga/intel/afu.h b/drivers/fpga/intel/afu.h
new file mode 100644
index 000000000000..c10fac2bbb1c
--- /dev/null
+++ b/drivers/fpga/intel/afu.h
@@ -0,0 +1,80 @@
+/*
+ * FPGA Accelerated Function Unit (AFU) Header
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *     Wu Hao <hao.wu@linux.intel.com>
+ *     Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *     Joseph Grecco <joe.grecco@intel.com>
+ *     Enno Luebbers <enno.luebbers@intel.com>
+ *     Tim Whisonant <tim.whisonant@intel.com>
+ *     Ananda Ravuri <ananda.ravuri@intel.com>
+ *     Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#ifndef __INTEL_AFU_H
+#define __INTEL_AFU_H
+
+#include "feature-dev.h"
+
+struct fpga_afu_region {
+	u32 index;
+	u32 flags;
+	u64 size;
+	u64 offset;
+	u64 phys;
+	struct list_head node;
+};
+
+struct fpga_afu_dma_region {
+	u64 user_addr;
+	u64 length;
+	u64 iova;
+	struct page **pages;
+	struct rb_node node;
+	bool in_use;
+};
+
+struct fpga_afu {
+	u64 region_cur_offset;
+	u32 capability;
+	int num_regions;
+	u8 num_umsgs;
+	u8 num_uafu_irqs;
+	struct list_head regions;
+	struct rb_root dma_regions;
+
+	struct feature_platform_data *pdata;
+};
+
+void afu_region_init(struct feature_platform_data *pdata);
+int afu_region_add(struct feature_platform_data *pdata, u32 region_index,
+		   u64 region_size, u64 phys, u32 flags);
+void afu_region_destroy(struct feature_platform_data *pdata);
+int afu_get_region_by_index(struct feature_platform_data *pdata,
+			    u32 region_index, struct fpga_afu_region *pregion);
+int afu_get_region_by_offset(struct feature_platform_data *pdata,
+			    u64 offset, u64 size,
+			    struct fpga_afu_region *pregion);
+
+void afu_dma_region_init(struct feature_platform_data *pdata);
+void afu_dma_region_destroy(struct feature_platform_data *pdata);
+long afu_dma_map_region(struct feature_platform_data *pdata,
+		       u64 user_addr, u64 length, u64 *iova);
+long afu_dma_unmap_region(struct feature_platform_data *pdata, u64 iova);
+struct fpga_afu_dma_region *afu_dma_region_find(
+		struct feature_platform_data *pdata, u64 iova, u64 size);
+
+int port_hdr_test(struct platform_device *pdev, struct feature *feature);
+int port_err_test(struct platform_device *pdev, struct feature *feature);
+int port_umsg_test(struct platform_device *pdev, struct feature *feature);
+int port_stp_test(struct platform_device *pdev, struct feature *feature);
+
+extern struct feature_ops port_err_ops;
+
+#endif
diff --git a/drivers/fpga/intel/altera-asmip2.c b/drivers/fpga/intel/altera-asmip2.c
new file mode 100644
index 000000000000..a0b5768023f3
--- /dev/null
+++ b/drivers/fpga/intel/altera-asmip2.c
@@ -0,0 +1,466 @@
+/*
+ * Copyright (C) 2017 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/iopoll.h>
+#include <linux/module.h>
+#include <linux/mtd/mtd.h>
+#include <linux/mtd/spi-nor.h>
+#include <linux/of_device.h>
+
+#include "altera-asmip2.h"
+
+#define QSPI_ACTION_REG			0
+#define QSPI_ACTION_RST			BIT(0)
+#define QSPI_ACTION_EN			BIT(1)
+#define QSPI_ACTION_SC			BIT(2)
+#define QSPI_ACTION_CHIP_SEL_SFT	4
+#define QSPI_ACTION_DUMMY_SFT		8
+#define QSPI_ACTION_READ_BACK_SFT	16
+
+#define QSPI_FIFO_CNT_REG		4
+#define QSPI_FIFO_DEPTH			0x200
+#define QSPI_FIFO_CNT_MSK		0x3ff
+#define QSPI_FIFO_CNT_RX_SFT		0
+#define QSPI_FIFO_CNT_TX_SFT		12
+
+#define QSPI_DATA_REG			0x8
+
+#define QSPI_POLL_TIMEOUT_US		10000000
+#define QSPI_POLL_INTERVAL_US		5
+
+struct altera_asmip2 {
+	void __iomem *csr_base;
+	u32 num_flashes;
+	struct device *dev;
+	struct altera_asmip2_flash *flash[ALTERA_ASMIP2_MAX_NUM_FLASH_CHIP];
+	struct mutex bus_mutex;
+};
+
+struct altera_asmip2_flash {
+	struct spi_nor nor;
+	struct altera_asmip2 *q;
+};
+
+static int altera_asmip2_write_reg(struct spi_nor *nor, u8 opcode, u8 *val,
+				    int len)
+{
+	struct altera_asmip2_flash *flash = nor->priv;
+	struct altera_asmip2 *q = flash->q;
+	u32 reg;
+	int ret;
+
+	if ((len + 1) > QSPI_FIFO_DEPTH) {
+		dev_err(q->dev, "%s bad len %d > %d\n",
+			__func__, len + 1, QSPI_FIFO_DEPTH);
+		return -EINVAL;
+	}
+
+	writeb(opcode, q->csr_base + QSPI_DATA_REG);
+
+	iowrite8_rep(q->csr_base + QSPI_DATA_REG, val, len);
+
+	reg = QSPI_ACTION_EN | QSPI_ACTION_SC;
+
+	writel(reg, q->csr_base + QSPI_ACTION_REG);
+
+	ret = readl_poll_timeout(q->csr_base + QSPI_FIFO_CNT_REG, reg,
+				 (((reg >> QSPI_FIFO_CNT_TX_SFT) &
+				 QSPI_FIFO_CNT_MSK) == 0),
+				 QSPI_POLL_INTERVAL_US, QSPI_POLL_TIMEOUT_US);
+	if (ret)
+		dev_err(q->dev, "%s timed out\n", __func__);
+
+	reg = QSPI_ACTION_EN;
+
+	writel(reg, q->csr_base + QSPI_ACTION_REG);
+
+	return ret;
+}
+
+static int altera_asmip2_read_reg(struct spi_nor *nor, u8 opcode, u8 *val,
+				   int len)
+{
+	struct altera_asmip2_flash *flash = nor->priv;
+	struct altera_asmip2 *q = flash->q;
+	u32 reg;
+	int ret;
+
+	if (len > QSPI_FIFO_DEPTH) {
+		dev_err(q->dev, "%s bad len %d > %d\n",
+			__func__, len, QSPI_FIFO_DEPTH);
+		return -EINVAL;
+	}
+
+	writeb(opcode, q->csr_base + QSPI_DATA_REG);
+
+	reg = QSPI_ACTION_EN | QSPI_ACTION_SC |
+		(len << QSPI_ACTION_READ_BACK_SFT);
+
+	writel(reg, q->csr_base + QSPI_ACTION_REG);
+
+	ret = readl_poll_timeout(q->csr_base + QSPI_FIFO_CNT_REG, reg,
+				 ((reg & QSPI_FIFO_CNT_MSK) == len),
+				 QSPI_POLL_INTERVAL_US, QSPI_POLL_TIMEOUT_US);
+
+	if (!ret)
+		ioread8_rep(q->csr_base + QSPI_DATA_REG, val, len);
+	else
+		dev_err(q->dev, "%s timeout\n", __func__);
+
+	writel(QSPI_ACTION_EN, q->csr_base + QSPI_ACTION_REG);
+
+	return ret;
+}
+
+static inline void altera_asmip2_push_offset(struct altera_asmip2 *q,
+					     struct spi_nor *nor,
+					     loff_t offset)
+{
+	int i;
+	u32 val;
+
+	for (i = (nor->addr_width - 1) * 8; i >= 0; i -= 8) {
+		val = (offset & (0xff << i)) >> i;
+		writeb(val, q->csr_base + QSPI_DATA_REG);
+	}
+}
+
+static ssize_t altera_asmip2_read(struct spi_nor *nor, loff_t from, size_t len,
+				   u_char *buf)
+{
+	struct altera_asmip2_flash *flash = nor->priv;
+	struct altera_asmip2 *q = flash->q;
+	size_t bytes_to_read;
+	u32 reg;
+	int ret;
+
+	bytes_to_read = min_t(size_t, len, QSPI_FIFO_DEPTH);
+
+	writeb(nor->read_opcode, q->csr_base + QSPI_DATA_REG);
+
+	altera_asmip2_push_offset(q, nor, from);
+
+	reg = QSPI_ACTION_EN | QSPI_ACTION_SC |
+		(10 << QSPI_ACTION_DUMMY_SFT) |
+		(bytes_to_read << QSPI_ACTION_READ_BACK_SFT);
+
+	writel(reg, q->csr_base + QSPI_ACTION_REG);
+
+	ret = readl_poll_timeout(q->csr_base + QSPI_FIFO_CNT_REG, reg,
+				 ((reg & QSPI_FIFO_CNT_MSK) ==
+				 bytes_to_read), QSPI_POLL_INTERVAL_US,
+				 QSPI_POLL_TIMEOUT_US);
+	if (ret) {
+		dev_err(q->dev, "%s timed out\n", __func__);
+		bytes_to_read = 0;
+	} else
+		ioread8_rep(q->csr_base + QSPI_DATA_REG, buf, bytes_to_read);
+
+	writel(QSPI_ACTION_EN, q->csr_base + QSPI_ACTION_REG);
+
+	return bytes_to_read;
+}
+
+static ssize_t altera_asmip2_write(struct spi_nor *nor, loff_t to,
+				    size_t len, const u_char *buf)
+{
+	struct altera_asmip2_flash *flash = nor->priv;
+	struct altera_asmip2 *q = flash->q;
+	size_t bytes_to_write;
+	u32 reg;
+	int ret;
+
+	bytes_to_write = min_t(size_t, len,
+			       (QSPI_FIFO_DEPTH - (nor->addr_width + 1)));
+
+	writeb(nor->program_opcode, q->csr_base + QSPI_DATA_REG);
+
+	altera_asmip2_push_offset(q, nor, to);
+
+	iowrite8_rep(q->csr_base + QSPI_DATA_REG, buf, bytes_to_write);
+
+	reg = QSPI_ACTION_EN | QSPI_ACTION_SC;
+
+	writel(reg, q->csr_base + QSPI_ACTION_REG);
+
+	ret = readl_poll_timeout(q->csr_base + QSPI_FIFO_CNT_REG, reg,
+				 (((reg >> QSPI_FIFO_CNT_TX_SFT) &
+				 QSPI_FIFO_CNT_MSK) == 0),
+				 QSPI_POLL_INTERVAL_US, QSPI_POLL_TIMEOUT_US);
+
+	if (ret) {
+		dev_err(q->dev,
+			"%s timed out waiting for fifo to clear\n",
+			__func__);
+		bytes_to_write = 0;
+	}
+
+	writel(QSPI_ACTION_EN, q->csr_base + QSPI_ACTION_REG);
+
+	return bytes_to_write;
+}
+
+static int altera_asmip2_prep(struct spi_nor *nor, enum spi_nor_ops ops)
+{
+	struct altera_asmip2_flash *flash = nor->priv;
+	struct altera_asmip2 *q = flash->q;
+
+	mutex_lock(&q->bus_mutex);
+
+	return 0;
+}
+
+static void altera_asmip2_unprep(struct spi_nor *nor, enum spi_nor_ops ops)
+{
+	struct altera_asmip2_flash *flash = nor->priv;
+	struct altera_asmip2 *q = flash->q;
+
+	mutex_unlock(&q->bus_mutex);
+}
+
+static int altera_asmip2_setup_banks(struct device *dev,
+				      u32 bank, struct device_node *np)
+{
+	struct altera_asmip2 *q = dev_get_drvdata(dev);
+	struct altera_asmip2_flash *flash;
+	struct spi_nor *nor;
+	int ret = 0;
+
+	const struct spi_nor_hwcaps hwcaps = {
+		.mask = SNOR_HWCAPS_READ |
+			SNOR_HWCAPS_READ_FAST |
+			SNOR_HWCAPS_PP,
+	};
+
+	if (bank > q->num_flashes - 1)
+		return -EINVAL;
+
+	flash = devm_kzalloc(q->dev, sizeof(*flash), GFP_KERNEL);
+	if (!flash)
+		return -ENOMEM;
+
+	q->flash[bank] = flash;
+	flash->q = q;
+
+	nor = &flash->nor;
+	nor->dev = dev;
+	nor->priv = flash;
+	nor->mtd.priv = nor;
+	spi_nor_set_flash_node(nor, np);
+
+	/* spi nor framework*/
+	nor->read_reg = altera_asmip2_read_reg;
+	nor->write_reg = altera_asmip2_write_reg;
+	nor->read = altera_asmip2_read;
+	nor->write = altera_asmip2_write;
+	nor->prepare = altera_asmip2_prep;
+	nor->unprepare = altera_asmip2_unprep;
+
+	ret = spi_nor_scan(nor, NULL, &hwcaps);
+	if (ret) {
+		dev_err(nor->dev, "flash not found\n");
+		return ret;
+	}
+
+	ret =  mtd_device_register(&nor->mtd, NULL, 0);
+
+	return ret;
+}
+
+static int altera_asmip2_create(struct device *dev, void __iomem *csr_base)
+{
+	struct altera_asmip2 *q;
+	u32 reg;
+
+	q = devm_kzalloc(dev, sizeof(*q), GFP_KERNEL);
+	if (!q)
+		return -ENOMEM;
+
+	q->dev = dev;
+	q->csr_base = csr_base;
+
+	mutex_init(&q->bus_mutex);
+
+	dev_set_drvdata(dev, q);
+
+	reg = readl(q->csr_base + QSPI_ACTION_REG);
+	if (!(reg & QSPI_ACTION_RST)) {
+		writel((reg | QSPI_ACTION_RST), q->csr_base + QSPI_ACTION_REG);
+		dev_info(dev, "%s asserting reset\n", __func__);
+		udelay(10);
+	}
+
+	writel((reg & ~QSPI_ACTION_RST), q->csr_base + QSPI_ACTION_REG);
+	udelay(10);
+
+	return 0;
+}
+
+static int altera_asmip2_add_bank(struct device *dev,
+			 u32 bank, struct device_node *np)
+{
+	struct altera_asmip2 *q = dev_get_drvdata(dev);
+
+	if (q->num_flashes >= ALTERA_ASMIP2_MAX_NUM_FLASH_CHIP)
+		return -ENOMEM;
+
+	q->num_flashes++;
+
+	return altera_asmip2_setup_banks(dev, bank, np);
+}
+
+static int altera_asmip2_remove_banks(struct device *dev)
+{
+	struct altera_asmip2 *q = dev_get_drvdata(dev);
+	struct altera_asmip2_flash *flash;
+	int i;
+	int ret = 0;
+
+	if (!q)
+		return -EINVAL;
+
+	/* clean up for all nor flash */
+	for (i = 0; i < q->num_flashes; i++) {
+		flash = q->flash[i];
+		if (!flash)
+			continue;
+
+		/* clean up mtd stuff */
+		ret = mtd_device_unregister(&flash->nor.mtd);
+		if (ret) {
+			dev_err(dev, "error removing mtd\n");
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
+static int altera_asmip2_probe_with_pdata(struct platform_device *pdev,
+			     struct altera_asmip2_plat_data *qdata)
+{
+	struct device *dev = &pdev->dev;
+	int ret, i;
+
+	ret = altera_asmip2_create(dev, qdata->csr_base);
+
+	if (ret) {
+		dev_err(dev, "failed to create qspi device %d\n", ret);
+		return ret;
+	}
+
+	for (i = 0; i < qdata->num_chip_sel; i++) {
+		ret = altera_asmip2_add_bank(dev, i, NULL);
+		if (ret) {
+			dev_err(dev, "failed to add qspi bank %d\n", ret);
+			break;
+		}
+	}
+
+	return ret;
+}
+
+static int altera_asmip2_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct altera_asmip2_plat_data *qdata;
+#ifdef CONFIG_OF
+	struct device_node *np = pdev->dev.of_node;
+	struct resource *res;
+	void __iomem *csr_base;
+	u32 bank;
+	int ret;
+	struct device_node *pp;
+#endif
+
+	qdata = dev_get_platdata(dev);
+
+	if (qdata)
+		return altera_asmip2_probe_with_pdata(pdev, qdata);
+
+#ifdef CONFIG_OF
+	if (!np) {
+		dev_err(dev, "no device tree found %p\n", pdev);
+		return -ENODEV;
+	}
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	csr_base = devm_ioremap_resource(dev, res);
+	if (IS_ERR(csr_base)) {
+		dev_err(dev, "%s: ERROR: failed to map csr base\n", __func__);
+		return PTR_ERR(csr_base);
+	}
+
+	ret = altera_asmip2_create(dev, csr_base);
+
+	if (ret) {
+		dev_err(dev, "failed to create qspi device\n");
+		return ret;
+	}
+
+	for_each_available_child_of_node(np, pp) {
+		of_property_read_u32(pp, "reg", &bank);
+		if (bank >= ALTERA_ASMIP2_MAX_NUM_FLASH_CHIP) {
+			dev_err(dev, "bad reg value %u >= %u\n", bank,
+				ALTERA_ASMIP2_MAX_NUM_FLASH_CHIP);
+			goto error;
+		}
+
+		if (altera_asmip2_add_bank(dev, bank, pp)) {
+			dev_err(dev, "failed to add bank %u\n", bank);
+			goto error;
+		}
+	}
+
+	return 0;
+
+error:
+	altera_asmip2_remove_banks(dev);
+	return -EIO;
+#else
+	return -EINVAL;
+#endif
+}
+
+static int altera_asmip2_remove(struct platform_device *pdev)
+{
+	struct altera_asmip2 *q = dev_get_drvdata(&pdev->dev);
+
+	mutex_destroy(&q->bus_mutex);
+
+	return altera_asmip2_remove_banks(&pdev->dev);
+}
+
+static const struct of_device_id altera_asmip2_id_table[] = {
+	{ .compatible = "altr,asmi-parallel2-spi-nor",},
+	{}
+};
+MODULE_DEVICE_TABLE(of, altera_asmip2_id_table);
+
+static struct platform_driver altera_asmip2_driver = {
+	.driver = {
+		.name = ALTERA_ASMIP2_DRV_NAME,
+		.of_match_table = altera_asmip2_id_table,
+	},
+	.probe = altera_asmip2_probe,
+	.remove = altera_asmip2_remove,
+};
+module_platform_driver(altera_asmip2_driver);
+
+MODULE_AUTHOR("Matthew Gerlach <matthew.gerlach@linux.intel.com>");
+MODULE_DESCRIPTION("Altera ASMI Parallel II");
+MODULE_LICENSE("GPL v2");
+MODULE_ALIAS("platform:" ALTERA_ASMIP2_DRV_NAME);
diff --git a/drivers/fpga/intel/altera-asmip2.h b/drivers/fpga/intel/altera-asmip2.h
new file mode 100644
index 000000000000..580c43c456e5
--- /dev/null
+++ b/drivers/fpga/intel/altera-asmip2.h
@@ -0,0 +1,24 @@
+/*
+ *
+ * Copyright 2017 Intel Corporation, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+#ifndef __ALTERA_QUADSPI_H
+#define __ALTERA_QUADSPI_H
+
+#include <linux/device.h>
+
+#define ALTERA_ASMIP2_DRV_NAME "altr-asmip2"
+#define ALTERA_ASMIP2_MAX_NUM_FLASH_CHIP 3
+#define ALTERA_ASMIP2_RESOURCE_SIZE 0x10
+
+struct altera_asmip2_plat_data {
+	void __iomem *csr_base;
+	u32 num_chip_sel;
+};
+
+#endif
diff --git a/drivers/fpga/intel/dma-region.c b/drivers/fpga/intel/dma-region.c
new file mode 100644
index 000000000000..e6bba9479f2f
--- /dev/null
+++ b/drivers/fpga/intel/dma-region.c
@@ -0,0 +1,372 @@
+/*
+ * Driver for FPGA Accelerated Function Unit (AFU) DMA Region Management
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Wu Hao <hao.wu@linux.intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include <linux/uaccess.h>
+#include <linux/sched/signal.h>
+
+#include "afu.h"
+
+static void put_all_pages(struct page **pages, int npages)
+{
+	int i;
+
+	for (i = 0; i < npages; i++)
+		if (pages[i] != NULL)
+			put_page(pages[i]);
+}
+
+void afu_dma_region_init(struct feature_platform_data *pdata)
+{
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+
+	afu->dma_regions = RB_ROOT;
+}
+
+static long afu_dma_adjust_locked_vm(struct device *dev, long npages, bool incr)
+{
+	unsigned long locked, lock_limit;
+	int ret = 0;
+
+	/* the task is exiting. */
+	if (!current->mm)
+		return 0;
+
+	down_write(&current->mm->mmap_sem);
+
+	if (incr) {
+		locked = current->mm->locked_vm + npages;
+		lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
+
+		if (locked > lock_limit && !capable(CAP_IPC_LOCK))
+			ret = -ENOMEM;
+		else
+			current->mm->locked_vm += npages;
+	} else {
+
+		if (WARN_ON_ONCE(npages > current->mm->locked_vm))
+			npages = current->mm->locked_vm;
+		current->mm->locked_vm -= npages;
+	}
+
+	dev_dbg(dev, "[%d] RLIMIT_MEMLOCK %c%ld %ld/%ld%s\n", current->pid,
+				incr ? '+' : '-',
+				npages << PAGE_SHIFT,
+				current->mm->locked_vm << PAGE_SHIFT,
+				rlimit(RLIMIT_MEMLOCK),
+				ret ? "- execeeded" : "");
+
+	up_write(&current->mm->mmap_sem);
+
+	return ret;
+}
+
+static long afu_dma_pin_pages(struct feature_platform_data *pdata,
+				struct fpga_afu_dma_region *region)
+{
+	long npages = region->length >> PAGE_SHIFT;
+	struct device *dev = &pdata->dev->dev;
+	long ret, pinned;
+
+	ret = afu_dma_adjust_locked_vm(dev, npages, true);
+	if (ret)
+		return ret;
+
+	region->pages = kcalloc(npages, sizeof(struct page *), GFP_KERNEL);
+	if (!region->pages) {
+		afu_dma_adjust_locked_vm(dev, npages, false);
+		return -ENOMEM;
+	}
+
+	pinned = get_user_pages_fast(region->user_addr, npages, 1,
+					region->pages);
+	if (pinned < 0) {
+		ret = pinned;
+		goto err_put_pages;
+	} else if (pinned != npages) {
+		ret = -EFAULT;
+		goto err;
+	}
+
+	dev_dbg(dev, "%ld pages pinned\n", pinned);
+
+	return 0;
+
+err_put_pages:
+	put_all_pages(region->pages, pinned);
+err:
+	kfree(region->pages);
+	afu_dma_adjust_locked_vm(dev, npages, false);
+	return ret;
+}
+
+static void afu_dma_unpin_pages(struct feature_platform_data *pdata,
+				struct fpga_afu_dma_region *region)
+{
+	long npages = region->length >> PAGE_SHIFT;
+	struct device *dev = &pdata->dev->dev;
+
+	put_all_pages(region->pages, npages);
+	kfree(region->pages);
+	afu_dma_adjust_locked_vm(dev, npages, false);
+
+	dev_dbg(dev, "%ld pages unpinned\n", npages);
+}
+
+static bool afu_dma_check_continuous_pages(struct fpga_afu_dma_region *region)
+{
+	int npages = region->length >> PAGE_SHIFT;
+	int i;
+
+	for (i = 0; i < npages - 1; i++)
+		if (page_to_pfn(region->pages[i]) + 1 !=
+					page_to_pfn(region->pages[i+1]))
+			return false;
+
+	return true;
+}
+
+static bool dma_region_check_iova(struct fpga_afu_dma_region *region,
+				  u64 iova, u64 size)
+{
+	if (!size && region->iova != iova)
+		return false;
+
+	return (region->iova <= iova) &&
+		(region->length + region->iova >= iova + size);
+}
+
+/* Need to be called with pdata->lock held */
+static int afu_dma_region_add(struct feature_platform_data *pdata,
+					struct fpga_afu_dma_region *region)
+{
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+	struct rb_node **new, *parent = NULL;
+
+	dev_dbg(&pdata->dev->dev, "add region (iova = %llx)\n",
+					(unsigned long long)region->iova);
+
+	new = &(afu->dma_regions.rb_node);
+
+	while (*new) {
+		struct fpga_afu_dma_region *this;
+
+		this = container_of(*new, struct fpga_afu_dma_region, node);
+
+		parent = *new;
+
+		if (dma_region_check_iova(this, region->iova, region->length))
+			return -EEXIST;
+
+		if (region->iova < this->iova)
+			new = &((*new)->rb_left);
+		else if (region->iova > this->iova)
+			new = &((*new)->rb_right);
+		else
+			return -EEXIST;
+	}
+
+	rb_link_node(&region->node, parent, new);
+	rb_insert_color(&region->node, &afu->dma_regions);
+
+	return 0;
+}
+
+/* Need to be called with pdata->lock held */
+static void afu_dma_region_remove(struct feature_platform_data *pdata,
+					struct fpga_afu_dma_region *region)
+{
+	struct fpga_afu *afu;
+
+	dev_dbg(&pdata->dev->dev, "del region (iova = %llx)\n",
+					(unsigned long long)region->iova);
+
+	afu = fpga_pdata_get_private(pdata);
+	rb_erase(&region->node, &afu->dma_regions);
+}
+
+/* Need to be called with pdata->lock held */
+void afu_dma_region_destroy(struct feature_platform_data *pdata)
+{
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+	struct rb_node *node = rb_first(&afu->dma_regions);
+	struct fpga_afu_dma_region *region;
+
+	while (node) {
+		region = container_of(node, struct fpga_afu_dma_region, node);
+
+		dev_dbg(&pdata->dev->dev, "del region (iova = %llx)\n",
+					(unsigned long long)region->iova);
+
+		rb_erase(node, &afu->dma_regions);
+
+		if (region->iova)
+			dma_unmap_page(fpga_pdata_to_pcidev(pdata),
+					region->iova, region->length,
+					DMA_BIDIRECTIONAL);
+
+		if (region->pages)
+			afu_dma_unpin_pages(pdata, region);
+
+		node = rb_next(node);
+		kfree(region);
+	}
+}
+
+/*
+ * It finds the dma region from the rbtree based on @iova and @size:
+ * - if @size == 0, it finds the dma region which starts from @iova
+ * - otherwise, it finds the dma region which fully contains
+ *   [@iova, @iova+size)
+ * If nothing is matched returns NULL.
+ *
+ * Need to be called with pdata->lock held.
+ */
+struct fpga_afu_dma_region *
+afu_dma_region_find(struct feature_platform_data *pdata, u64 iova, u64 size)
+{
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+	struct rb_node *node = afu->dma_regions.rb_node;
+	struct device *dev = &pdata->dev->dev;
+
+	while (node) {
+		struct fpga_afu_dma_region *region;
+
+		region = container_of(node, struct fpga_afu_dma_region, node);
+
+		if (dma_region_check_iova(region, iova, size)) {
+			dev_dbg(dev, "find region (iova = %llx)\n",
+				(unsigned long long)region->iova);
+			return region;
+		}
+
+		if (iova < region->iova)
+			node = node->rb_left;
+		else if (iova > region->iova)
+			node = node->rb_right;
+		else
+			/* the iova region is not fully covered. */
+			break;
+	}
+
+	dev_dbg(dev, "region with iova %llx and size %llx is not found\n",
+		(unsigned long long)iova, (unsigned long long)size);
+	return NULL;
+}
+
+static struct fpga_afu_dma_region *
+afu_dma_region_find_iova(struct feature_platform_data *pdata, u64 iova)
+{
+	return afu_dma_region_find(pdata, iova, 0);
+}
+
+long afu_dma_map_region(struct feature_platform_data *pdata,
+		       u64 user_addr, u64 length, u64 *iova)
+{
+	struct fpga_afu_dma_region *region;
+	int ret;
+
+	/*
+	 * Check Inputs, only accept page-aligned user memory region with
+	 * valid length.
+	 */
+	if (!PAGE_ALIGNED(user_addr) || !PAGE_ALIGNED(length) || !length)
+		return -EINVAL;
+
+	/* Check overflow */
+	if (user_addr + length < user_addr)
+		return -EINVAL;
+
+	if (!access_ok(VERIFY_WRITE, user_addr, length))
+		return -EINVAL;
+
+	region = kzalloc(sizeof(*region), GFP_KERNEL);
+	if (!region)
+		return -ENOMEM;
+
+	region->user_addr = user_addr;
+	region->length = length;
+
+	/* Pin the user memory region */
+	ret = afu_dma_pin_pages(pdata, region);
+	if (ret) {
+		dev_err(&pdata->dev->dev, "fail to pin memory region\n");
+		goto free_region;
+	}
+
+	/* Only accept continuous pages, return error if no */
+	if (!afu_dma_check_continuous_pages(region)) {
+		dev_err(&pdata->dev->dev, "pages are not continuous\n");
+		ret = -EINVAL;
+		goto unpin_pages;
+	}
+
+	/* As pages are continuous then start to do DMA mapping */
+	region->iova = dma_map_page(fpga_pdata_to_pcidev(pdata),
+				    region->pages[0], 0,
+				    region->length,
+				    DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(&pdata->dev->dev, region->iova)) {
+		dev_err(&pdata->dev->dev, "fail to map dma mapping\n");
+		ret = -EFAULT;
+		goto unpin_pages;
+	}
+
+	*iova = region->iova;
+
+	mutex_lock(&pdata->lock);
+	ret = afu_dma_region_add(pdata, region);
+	mutex_unlock(&pdata->lock);
+	if (ret) {
+		dev_err(&pdata->dev->dev, "fail to add dma region\n");
+		goto unmap_dma;
+	}
+
+	return 0;
+
+unmap_dma:
+	dma_unmap_page(fpga_pdata_to_pcidev(pdata),
+		       region->iova, region->length, DMA_BIDIRECTIONAL);
+unpin_pages:
+	afu_dma_unpin_pages(pdata, region);
+free_region:
+	kfree(region);
+	return ret;
+}
+
+long afu_dma_unmap_region(struct feature_platform_data *pdata, u64 iova)
+{
+	struct fpga_afu_dma_region *region;
+
+	mutex_lock(&pdata->lock);
+	region = afu_dma_region_find_iova(pdata, iova);
+	if (!region) {
+		mutex_unlock(&pdata->lock);
+		return -EINVAL;
+	}
+
+	if (region->in_use) {
+		mutex_unlock(&pdata->lock);
+		return -EBUSY;
+	}
+
+	afu_dma_region_remove(pdata, region);
+	mutex_unlock(&pdata->lock);
+
+	dma_unmap_page(fpga_pdata_to_pcidev(pdata),
+		       region->iova, region->length, DMA_BIDIRECTIONAL);
+	afu_dma_unpin_pages(pdata, region);
+	kfree(region);
+
+	return 0;
+}
diff --git a/drivers/fpga/intel/feature-dev.c b/drivers/fpga/intel/feature-dev.c
new file mode 100644
index 000000000000..b7ab8ebd0f52
--- /dev/null
+++ b/drivers/fpga/intel/feature-dev.c
@@ -0,0 +1,368 @@
+/*
+ * Intel FPGA Feature Device Framework Header
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Zhang Yi <Yi.Z.Zhang@intel.com>
+ *   Wu Hao <hao.wu@linux.intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include <linux/fs.h>
+
+#include "feature-dev.h"
+
+void feature_platform_data_add(struct feature_platform_data *pdata,
+			       int index, const char *name,
+			       int resource_index, void __iomem *ioaddr,
+			       struct feature_irq_ctx *ctx,
+			       unsigned int ctx_num)
+{
+	WARN_ON(index >= pdata->num);
+	WARN_ON(ctx_num && !ctx);
+
+	pdata->features[index].name = name;
+	pdata->features[index].resource_index = resource_index;
+	pdata->features[index].ioaddr = ioaddr;
+	pdata->features[index].ctx = ctx;
+	pdata->features[index].ctx_num = ctx_num;
+}
+
+int feature_platform_data_size(int num)
+{
+	return sizeof(struct feature_platform_data) +
+		num * sizeof(struct feature);
+}
+
+struct feature_platform_data *
+feature_platform_data_alloc_and_init(struct platform_device *dev, int num)
+{
+	struct feature_platform_data *pdata;
+
+	pdata = kzalloc(feature_platform_data_size(num), GFP_KERNEL);
+	if (pdata) {
+		pdata->dev = dev;
+		pdata->num = num;
+		mutex_init(&pdata->lock);
+	}
+
+	return pdata;
+}
+
+int fme_feature_num(void)
+{
+	return FME_FEATURE_ID_MAX;
+}
+
+int port_feature_num(void)
+{
+	return PORT_FEATURE_ID_MAX;
+}
+
+int fme_feature_to_resource_index(int feature_id)
+{
+	WARN_ON(feature_id >= FME_FEATURE_ID_MAX);
+	return feature_id;
+}
+
+void fpga_dev_feature_uinit(struct platform_device *pdev)
+{
+	struct feature *feature;
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+
+	fpga_dev_for_each_feature(pdata, feature)
+		if (feature->ops) {
+			feature->ops->uinit(pdev, feature);
+			feature->ops = NULL;
+		}
+}
+EXPORT_SYMBOL_GPL(fpga_dev_feature_uinit);
+
+static int
+feature_instance_init(struct platform_device *pdev,
+		      struct feature_platform_data *pdata,
+		      struct feature *feature, struct feature_driver *drv)
+{
+	int ret;
+
+	WARN_ON(!feature->ioaddr);
+
+	if (drv->ops->test) {
+		ret = drv->ops->test(pdev, feature);
+		if (ret)
+			return ret;
+	}
+
+	ret = drv->ops->init(pdev, feature);
+	if (ret)
+		return ret;
+
+	feature->ops = drv->ops;
+	return ret;
+}
+
+int fpga_dev_feature_init(struct platform_device *pdev,
+			  struct feature_driver *feature_drvs)
+{
+	struct feature *feature;
+	struct feature_driver *drv = feature_drvs;
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	int ret;
+
+	while (drv->ops) {
+		fpga_dev_for_each_feature(pdata, feature) {
+			/* skip the feature which is not initialized. */
+			if (!feature->name)
+				continue;
+
+			if (!strcmp(drv->name, feature->name)) {
+				ret = feature_instance_init(pdev, pdata,
+							    feature, drv);
+				if (ret)
+					goto exit;
+			}
+		}
+		drv++;
+	}
+	return 0;
+exit:
+	fpga_dev_feature_uinit(pdev);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(fpga_dev_feature_init);
+
+struct fpga_chardev_info {
+	const char *name;
+	dev_t devt;
+};
+
+/* indexe by enum fpga_devt_type */
+struct fpga_chardev_info fpga_chrdevs[] = {
+	{.name = FPGA_FEATURE_DEV_FME},		/* FPGA_DEVT_FME */
+	{.name = FPGA_FEATURE_DEV_PORT},	/* FPGA_DEVT_AFU */
+};
+
+void fpga_chardev_uinit(void)
+{
+	int i;
+
+	for (i = 0; i < FPGA_DEVT_MAX; i++)
+		if (MAJOR(fpga_chrdevs[i].devt)) {
+			unregister_chrdev_region(fpga_chrdevs[i].devt,
+						 MINORMASK);
+			fpga_chrdevs[i].devt = MKDEV(0, 0);
+		}
+}
+
+int fpga_chardev_init(void)
+{
+	int i, ret;
+
+	for (i = 0; i < FPGA_DEVT_MAX; i++) {
+		ret = alloc_chrdev_region(&fpga_chrdevs[i].devt, 0, MINORMASK,
+					  fpga_chrdevs[i].name);
+		if (ret)
+			goto exit;
+	}
+
+	return 0;
+
+exit:
+	fpga_chardev_uinit();
+	return ret;
+}
+
+dev_t fpga_get_devt(enum fpga_devt_type type, int id)
+{
+	WARN_ON(type >= FPGA_DEVT_MAX);
+
+	return MKDEV(MAJOR(fpga_chrdevs[type].devt), id);
+}
+
+int fpga_register_dev_ops(struct platform_device *pdev,
+			  const struct file_operations *fops,
+			  struct module *owner)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+
+	cdev_init(&pdata->cdev, fops);
+	pdata->cdev.owner = owner;
+
+	/*
+	 * set parent to the feature device so that its refcount is
+	 * decreased after the last refcount of cdev is gone, that
+	 * makes sure the feature device is valid during device
+	 * file's life-cycle.
+	 */
+	pdata->cdev.kobj.parent = &pdev->dev.kobj;
+	return cdev_add(&pdata->cdev, pdev->dev.devt, 1);
+}
+EXPORT_SYMBOL_GPL(fpga_register_dev_ops);
+
+void fpga_unregister_dev_ops(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+
+	cdev_del(&pdata->cdev);
+}
+EXPORT_SYMBOL_GPL(fpga_unregister_dev_ops);
+
+int fpga_port_id(struct platform_device *pdev)
+{
+	struct feature_port_header *port_hdr;
+	struct feature_port_capability capability;
+
+	port_hdr = get_feature_ioaddr_by_index(&pdev->dev,
+					       PORT_FEATURE_ID_HEADER);
+	WARN_ON(!port_hdr);
+
+	capability.csr = readq(&port_hdr->capability);
+	return capability.port_number;
+}
+EXPORT_SYMBOL_GPL(fpga_port_id);
+
+/*
+ * Enable Port by clear the port soft reset bit, which is set by default.
+ * The AFU is unable to respond to any MMIO access while in reset.
+ * __fpga_port_enable function should only be used after __fpga_port_disable
+ * function.
+ */
+void __fpga_port_enable(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct feature_port_header *port_hdr;
+	struct feature_port_control control;
+
+	WARN_ON(!pdata->disable_count);
+
+	if (--pdata->disable_count != 0)
+		return;
+
+	port_hdr = get_feature_ioaddr_by_index(&pdev->dev,
+					       PORT_FEATURE_ID_HEADER);
+	WARN_ON(!port_hdr);
+
+	control.csr = readq(&port_hdr->control);
+	control.port_sftrst = 0x0;
+	writeq(control.csr, &port_hdr->control);
+}
+EXPORT_SYMBOL_GPL(__fpga_port_enable);
+
+#define RST_POLL_INVL 10 /* us */
+#define RST_POLL_TIMEOUT 1000 /* us */
+
+int __fpga_port_disable(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct feature_port_header *port_hdr;
+	struct feature_port_control control;
+
+	if (pdata->disable_count++ != 0)
+		return 0;
+
+	port_hdr = get_feature_ioaddr_by_index(&pdev->dev,
+					       PORT_FEATURE_ID_HEADER);
+	WARN_ON(!port_hdr);
+
+	/* Set port soft reset */
+	control.csr = readq(&port_hdr->control);
+	control.port_sftrst = 0x1;
+	writeq(control.csr, &port_hdr->control);
+
+	/*
+	 * HW sets ack bit to 1 when all outstanding requests have been drained
+	 * on this port and minimum soft reset pulse width has elapsed.
+	 * Driver polls port_soft_reset_ack to determine if reset done by HW.
+	 */
+	control.port_sftrst_ack = 1;
+
+	if (fpga_wait_register_field(port_sftrst_ack, control,
+		&port_hdr->control, RST_POLL_TIMEOUT, RST_POLL_INVL)) {
+		dev_err(&pdev->dev, "timeout, fail to reset device\n");
+		return -ETIMEDOUT;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(__fpga_port_disable);
+
+static irqreturn_t fpga_msix_handler(int irq, void *arg)
+{
+	struct eventfd_ctx *trigger = arg;
+
+	eventfd_signal(trigger, 1);
+	return IRQ_HANDLED;
+}
+
+static int fpga_set_vector_signal(struct feature *feature, int vector, int fd)
+{
+	struct eventfd_ctx *trigger;
+	int irq, ret;
+
+	if (vector < 0 || vector >= feature->ctx_num)
+		return -EINVAL;
+
+	irq = feature->ctx[vector].irq;
+
+	if (feature->ctx[vector].trigger) {
+		free_irq(irq, feature->ctx[vector].trigger);
+		kfree(feature->ctx[vector].name);
+		eventfd_ctx_put(feature->ctx[vector].trigger);
+		feature->ctx[vector].trigger = NULL;
+	}
+
+	if (fd < 0)
+		return 0;
+
+	feature->ctx[vector].name = kasprintf(GFP_KERNEL, "fpga-msix[%d](%s)",
+						vector, feature->name);
+	if (!feature->ctx[vector].name)
+		return -ENOMEM;
+
+	trigger = eventfd_ctx_fdget(fd);
+	if (IS_ERR(trigger)) {
+		kfree(feature->ctx[vector].name);
+		return PTR_ERR(trigger);
+	}
+
+	ret = request_irq(irq, fpga_msix_handler, 0, feature->ctx[vector].name,
+			  trigger);
+	if (ret) {
+		kfree(feature->ctx[vector].name);
+		eventfd_ctx_put(trigger);
+		return ret;
+	}
+
+	feature->ctx[vector].trigger = trigger;
+
+	return 0;
+}
+
+int fpga_msix_set_block(struct feature *feature, unsigned int start,
+			unsigned int count, int32_t *fds)
+{
+	int i, j, ret = 0;
+
+	if (start >= feature->ctx_num || start + count > feature->ctx_num)
+		return -EINVAL;
+
+	for (i = 0, j = start; i < count && !ret; i++, j++) {
+		int fd = fds ? fds[i] : -1;
+
+		ret = fpga_set_vector_signal(feature, j, fd);
+	}
+
+	if (ret) {
+		for (--j; j >= (int)start; j--)
+			fpga_set_vector_signal(feature, j, -1);
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(fpga_msix_set_block);
diff --git a/drivers/fpga/intel/feature-dev.h b/drivers/fpga/intel/feature-dev.h
new file mode 100644
index 000000000000..98dd2a85ad3e
--- /dev/null
+++ b/drivers/fpga/intel/feature-dev.h
@@ -0,0 +1,1839 @@
+/*
+ * Intel FPGA Feature Device Framework Header
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Zhang Yi <Yi.Z.Zhang@intel.com>
+ *   Wu Hao <hao.wu@linux.intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#ifndef __INTEL_FPGA_FEATURE_H
+#define __INTEL_FPGA_FEATURE_H
+
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <linux/pci.h>
+#include <linux/uuid.h>
+#include <linux/delay.h>
+#include <linux/platform_device.h>
+#include <linux/intel-fpga.h>
+#include <linux/interrupt.h>
+#include <linux/eventfd.h>
+
+/* each FPGA device has 4 ports at most. */
+#define MAX_FPGA_PORT_NUM 4
+/*
+ * Num of umsgs is up to 255, but only 32 umsgs allow hint mode per spec
+ * so limit max num to 32 for now.
+ */
+#define MAX_PORT_UMSG_NUM 32
+/* one for fme device */
+#define MAX_FEATURE_DEV_NUM	(MAX_FPGA_PORT_NUM + 1)
+
+#define FME_FEATURE_HEADER          "fme_hdr"
+#define FME_FEATURE_THERMAL_MGMT    "fme_thermal"
+#define FME_FEATURE_POWER_MGMT      "fme_power"
+#define FME_FEATURE_GLOBAL_IPERF    "fme_iperf"
+#define FME_FEATURE_GLOBAL_ERR      "fme_error"
+#define FME_FEATURE_PR_MGMT         "fme_pr"
+#define FME_FEATURE_HSSI_ETH        "fme_hssi"
+#define FME_FEATURE_GLOBAL_DPERF    "fme_dperf"
+#define FME_FEATURE_QSPI_FLASH	    "fme_qspi_flash"
+
+#define PORT_FEATURE_HEADER         "port_hdr"
+#define PORT_FEATURE_UAFU           "port_uafu"
+#define PORT_FEATURE_ERR            "port_err"
+#define PORT_FEATURE_UMSG           "port_umsg"
+#define PORT_FEATURE_UINT           "port_uint"
+#define PORT_FEATURE_STP            "port_stp"
+
+/*
+ * do not check the revision id as id may be dynamic under
+ * some cases, e.g, UAFU.
+ */
+#define SKIP_REVISION_CHECK		0xff
+
+#define FME_HEADER_REVISION		1
+#define FME_THERMAL_MGMT_REVISION	0
+#define FME_POWER_MGMT_REVISION		1
+#define FME_GLOBAL_IPERF_REVISION	1
+#define FME_GLOBAL_ERR_REVISION		1
+#define FME_PR_MGMT_REVISION		2
+#define FME_HSSI_ETH_REVISION		0
+#define FME_GLOBAL_DPERF_REVISION	0
+#define FME_QSPI_REVISION		0
+
+#define PORT_HEADER_REVISION		0
+/* UAFU's header info depends on the downloaded GBS */
+#define PORT_UAFU_REVISION		SKIP_REVISION_CHECK
+#define PORT_ERR_REVISION		1
+#define PORT_UMSG_REVISION		0
+#define PORT_UINT_REVISION		0
+#define PORT_STP_REVISION		1
+
+/*
+ * All headers and structures must be byte-packed to match the
+ * SAS spec.
+ */
+#pragma pack(1)
+
+#define FEATURE_TYPE_AFU	0x1
+#define FEATURE_TYPE_PRIVATE	0x3
+#define FEATURE_TYPE_FIU	0x4
+
+#define FEATURE_FIU_ID_FME	0x0
+#define FEATURE_FIU_ID_PORT	0x1
+
+struct feature_header {
+	union {
+		u64 csr;
+		struct {
+			u16 id:12;
+			u8  revision:4;
+			u32 next_header_offset:24;
+			u32 eol:1;
+			u32 reserved:19;
+			u8  type:4;
+		};
+	};
+};
+
+struct feature_afu_header {
+	uuid_le guid;
+	union {
+		u64 csr;
+		struct {
+			u64 next_afu:24;
+			u64 reserved:40;
+		};
+	};
+};
+
+struct feature_fiu_header {
+	uuid_le guid;
+	union {
+		u64 csr;
+		struct {
+			u64 next_afu:24;
+			u64 reserved:40;
+		};
+	};
+};
+
+struct feature_fme_capability {
+	union {
+		u64 csr;
+		struct {
+			u8  fabric_verid;	/* Fabric version ID */
+			u8  socket_id:1;	/* Socket id */
+			u8  rsvd1:3;		/* Reserved */
+			/* pci0 link available yes /no */
+			u8  pci0_link_avile:1;
+			/* pci1 link available yes /no */
+			u8  pci1_link_avile:1;
+			/* Coherent (QPI/UPI) link available yes /no */
+			u8  qpi_link_avile:1;
+			u8  rsvd2:1;		/* Reserved */
+			/* IOMMU or VT-d supported  yes/no */
+			u8  iommu_support:1;
+			u8  num_ports:3;	/* Number of ports */
+			u8  sf_fab_ctl:1;	/* Internal validation bit */
+			u8  rsvd3:3;		/* Reserved */
+			/*
+			 * Address width supported in bits
+			 * BXT -0x26 , SKX -0x30
+			 */
+			u8  address_width_bits:6;
+			u8  rsvd4:2;		/* Reserved */
+			/* Size of cache supported in kb */
+			u16 cache_size:12;
+			u8  cache_assoc:4;	/* Cache Associativity */
+			u16 rsvd5:15;		/* Reserved */
+			u8  lock_bit:1;		/* Lock bit */
+		};
+	};
+};
+
+#define FME_AFU_ACCESS_PF		0
+#define FME_AFU_ACCESS_VF		1
+
+struct feature_fme_port {
+	union {
+		u64 csr;
+		struct {
+			u32 port_offset:24;
+			u8  reserved1;
+			u8  port_bar:3;
+			u32 reserved2:20;
+			u8  afu_access_control:1;
+			u8  reserved3:4;
+			u8  port_implemented:1;
+			u8  reserved4:3;
+		};
+	};
+};
+
+struct feature_fme_fab_status {
+	union {
+		u64 csr;
+		struct {
+			u8  upilink_status:4;   /* UPI Link Status */
+			u8  rsvd1:4;		/* Reserved */
+			u8  pci0link_status:1;  /* pci0 link status */
+			u8  rsvd2:3;            /* Reserved */
+			u8  pci1link_status:1;  /* pci1 link status */
+			u64 rsvd3:51;           /* Reserved */
+		};
+	};
+};
+
+struct feature_fme_genprotrange2_base {
+	union {
+		u64 csr;
+		struct {
+			u16 rsvd1;           /* Reserved */
+			/* Base Address of memory range */
+			u8  protected_base_addrss:4;
+			u64 rsvd2:44;           /* Reserved */
+		};
+	};
+};
+
+struct feature_fme_genprotrange2_limit {
+	union {
+		u64 csr;
+		struct {
+			u16 rsvd1;           /* Reserved */
+			/* Limit Address of memory range */
+			u8  protected_limit_addrss:4;
+			u16 rsvd2:11;           /* Reserved */
+			u8  enable:1;        /* Enable GENPROTRANGE check */
+			u32 rsvd3;           /* Reserved */
+		};
+	};
+};
+
+struct feature_fme_dxe_lock {
+	union {
+		u64 csr;
+		struct {
+			/*
+			 * Determines write access to the DXE region CSRs
+			 * 1 - CSR region is locked;
+			 * 0 - it is open for write access.
+			 */
+			u8  dxe_early_lock:1;
+			/*
+			 * Determines write access to the HSSI CSR
+			 * 1 - CSR region is locked;
+			 * 0 - it is open for write access.
+			 */
+			u8  dxe_late_lock:1;
+			u64 rsvd:62;
+		};
+	};
+};
+
+#define HSSI_ID_NO_HASSI	0
+#define HSSI_ID_PCIE_RP		1
+#define HSSI_ID_ETHERNET	2
+
+struct feature_fme_bitstream_id {
+	union {
+		u64 csr;
+		struct {
+			u32 gitrepo_hash:32;	/* GIT repository hash */
+			/*
+			 * HSSI configuration identifier:
+			 * 0 - No HSSI
+			 * 1 - PCIe-RP
+			 * 2 - Ethernet
+			 */
+			u8  hssi_id:4;
+			u16 rsvd1:12;		/* Reserved */
+			/* Bitstream version patch number */
+			u8  bs_verpatch:4;
+			/* Bitstream version minor number */
+			u8  bs_verminor:4;
+			/* Bitstream version major number */
+			u8  bs_vermajor:4;
+			/* Bitstream version debug number */
+			u8  bs_verdebug:4;
+		};
+	};
+};
+
+struct feature_fme_bitstream_md {
+	union {
+		u64 csr;
+		struct {
+			/* Seed number userd for synthesis flow */
+			u8  synth_seed:4;
+			/* Synthesis date(day number - 2 digits) */
+			u8  synth_day:8;
+			/* Synthesis date(month number - 2 digits) */
+			u8  synth_month:8;
+			/* Synthesis date(year number - 2 digits) */
+			u8  synth_year:8;
+			u64 rsvd:36;		/* Reserved */
+		};
+	};
+};
+
+struct feature_fme_iommu_ctrl {
+	union {
+		u64 csr;
+		struct {
+			/* Disables IOMMU prefetcher for C0 channel */
+			u8 prefetch_disableC0:1;
+			/* Disables IOMMU prefetcher for C1 channel */
+			u8 prefetch_disableC1:1;
+			/* Disables IOMMU partial cache line writes */
+			u8 prefetch_wrdisable:1;
+			u8 rsvd1:1;		/* Reserved */
+			/*
+			 * Select counter and read value from register
+			 * iommu_stat.dbg_counters
+			 * 0 - Number of 4K page translation response
+			 * 1 - Number of 2M page translation response
+			 * 2 - Number of 1G page translation response
+			 */
+			u8 counter_sel:2;
+			u32 rsvd2:26;		/* Reserved */
+			/* Connected to IOMMU SIP Capabilities */
+			u32 capecap_defeature;
+		};
+	};
+};
+
+struct feature_fme_iommu_stat {
+	union {
+		u64 csr;
+		struct {
+			/* Translation Enable bit from IOMMU SIP */
+			u8 translation_enable:1;
+			/* Drain request in progress */
+			u8 drain_req_inprog:1;
+			/* Invalidation current state */
+			u8 inv_state:3;
+			/* C0 Response Buffer current state */
+			u8 respbuffer_stateC0:3;
+			/* C1 Response Buffer current state */
+			u8 respbuffer_stateC1:3;
+			/* Last request ID to IOMMU SIP */
+			u8 last_reqID:4;
+			/* Last IOMMU SIP response ID value */
+			u8 last_respID:4;
+			/* Last IOMMU SIP response status value */
+			u8 last_respstatus:3;
+			/* C0 Transaction Buffer is not empty */
+			u8 transbuf_notEmptyC0:1;
+			/* C1 Transaction Buffer is not empty */
+			u8 transbuf_notEmptyC1:1;
+			/* C0 Request FIFO is not empty */
+			u8 reqFIFO_notemptyC0:1;
+			/* C1 Request FIFO is not empty */
+			u8 reqFIFO_notemptyC1:1;
+			/* C0 Response FIFO is not empty */
+			u8 respFIFO_notemptyC0:1;
+			/* C1 Response FIFO is not empty */
+			u8 respFIFO_notemptyC1:1;
+			/* C0 Response FIFO overflow detected */
+			u8 respFIFO_overflowC0:1;
+			/* C1 Response FIFO overflow detected */
+			u8 respFIFO_overflowC1:1;
+			/* C0 Transaction Buffer overflow detected */
+			u8 tranbuf_overflowC0:1;
+			/* C1 Transaction Buffer overflow detected */
+			u8 tranbuf_overflowC1:1;
+			/* Request FIFO overflow detected */
+			u8 reqFIFO_overflow:1;
+			/* IOMMU memory read in progress */
+			u8 memrd_inprog:1;
+			/* IOMMU memory write in progress */
+			u8 memwr_inprog:1;
+			u8 rsvd1:1;	/* Reserved */
+			/* Value of counter selected by iommu_ctl.counter_sel */
+			u16 dbg_counters:16;
+			u16 rsvd2:12;	/* Reserved */
+		};
+	};
+};
+
+struct feature_fme_pcie0_ctrl {
+	union {
+		u64 csr;
+		struct {
+			u64 vtd_bar_lock:1;	/* Lock VT-D BAR register */
+			u64 rsvd1:3;
+			u64 rciep:1;		/* Configure PCIE0 as RCiEP */
+			u64 rsvd2:59;
+		};
+	};
+};
+
+struct feature_fme_llpr_smrr_base {
+	union {
+		u64 csr;
+		struct {
+			u64 rsvd1:12;
+			u64 base:20;	/* SMRR2 memory range base address */
+			u64 rsvd2:32;
+		};
+	};
+};
+
+struct feature_fme_llpr_smrr_mask {
+	union {
+		u64 csr;
+		struct {
+			u64 rsvd1:11;
+			u64 valid:1;	/* LLPR_SMRR rule is valid or not */
+			/*
+			 * SMRR memory range mask which determines the range
+			 * of region being mapped
+			 */
+			u64 phys_mask:20;
+			u64 rsvd2:32;
+		};
+	};
+};
+
+struct feature_fme_llpr_smrr2_base {
+	union {
+		u64 csr;
+		struct {
+			u64 rsvd1:12;
+			u64 base:20;	/* SMRR2 memory range base address */
+			u64 rsvd2:32;
+		};
+	};
+};
+
+struct feature_fme_llpr_smrr2_mask {
+	union {
+		u64 csr;
+		struct {
+			u64 rsvd1:11;
+			u64 valid:1;	/* LLPR_SMRR2 rule is valid or not */
+			/*
+			 * SMRR2 memory range mask which determines the range
+			 * of region being mapped
+			 */
+			u64 phys_mask:20;
+			u64 rsvd2:32;
+		};
+	};
+};
+
+struct feature_fme_llpr_meseg_base {
+	union {
+		u64 csr;
+		struct {
+			/* A[45:19] of base address memory range */
+			u64 me_base:27;
+			u64 rsvd:37;
+		};
+	};
+};
+
+struct feature_fme_llpr_meseg_limit {
+	union {
+		u64 csr;
+		struct {
+			/* A[45:19] of limit address memory range */
+			u64 me_limit:27;
+			u64 rsvd1:4;
+			u64 enable:1;	/* Enable LLPR MESEG rule */
+			u64 rsvd2:32;
+		};
+	};
+};
+
+struct feature_fme_header {
+	struct feature_header header;
+	struct feature_afu_header afu_header;
+	u64 reserved;
+	u64 scratchpad;
+	struct feature_fme_capability capability;
+	struct feature_fme_port port[MAX_FPGA_PORT_NUM];
+	struct feature_fme_fab_status fab_status;
+	struct feature_fme_bitstream_id bitstream_id;
+	struct feature_fme_bitstream_md bitstream_md;
+	struct feature_fme_genprotrange2_base genprotrange2_base;
+	struct feature_fme_genprotrange2_limit genprotrange2_limit;
+	struct feature_fme_dxe_lock dxe_lock;
+	struct feature_fme_iommu_ctrl iommu_ctrl;
+	struct feature_fme_iommu_stat iommu_stat;
+	struct feature_fme_pcie0_ctrl pcie0_control;
+	struct feature_fme_llpr_smrr_base smrr_base;
+	struct feature_fme_llpr_smrr_mask smrr_mask;
+	struct feature_fme_llpr_smrr2_base smrr2_base;
+	struct feature_fme_llpr_smrr2_mask smrr2_mask;
+	struct feature_fme_llpr_meseg_base meseg_base;
+	struct feature_fme_llpr_meseg_limit meseg_limit;
+};
+
+struct feature_port_capability {
+	union {
+		u64 csr;
+		struct {
+			u8 port_number:2;	/* Port Number 0-3 */
+			u8 rsvd1:6;		/* Reserved */
+			u16 mmio_size;		/* User MMIO size in KB */
+			u8 rsvd2;		/* Reserved */
+			u8 sp_intr_num:4;	/* Supported interrupts num */
+			u32 rsvd3:28;		/* Reserved */
+		};
+	};
+};
+
+struct feature_port_control {
+	union {
+		u64 csr;
+		struct {
+			u8 port_sftrst:1;	/* Port Soft Reset */
+			u8 rsvd1:1;		/* Reserved */
+			u8 latency_tolerance:1;/* '1' >= 40us, '0' < 40us */
+			u8 rsvd2:1;		/* Reserved */
+			u8 port_sftrst_ack:1;	/* HW ACK for Soft Reset */
+			u64 rsvd3:59;		/* Reserved */
+		};
+	};
+};
+
+#define PORT_POWER_STATE_NORMAL		0
+#define PORT_POWER_STATE_AP1		1
+#define PORT_POWER_STATE_AP2		2
+#define PORT_POWER_STATE_AP6		6
+
+struct feature_port_status {
+	union {
+		u64 csr;
+		struct {
+			u8 port_freeze:1;	/* '1' - freezed '0' - normal */
+			u8 rsvd1:7;		/* Reserved */
+			u8 power_state:4;	/* Power State */
+			u8 ap1_event:1;		/* AP1 event was detected  */
+			u8 ap2_event:1;		/* AP2 event was detected  */
+			u64 rsvd2:50;		/* Reserved */
+		};
+	};
+};
+
+/* Port Header Register Set */
+struct feature_port_header {
+	struct feature_header header;
+	struct feature_afu_header afu_header;
+	u64 port_mailbox;
+	u64 scratchpad;
+	struct feature_port_capability capability;
+	struct feature_port_control control;
+	struct feature_port_status status;
+	u64 rsvd2;
+	u64 user_clk_freq_cmd0;
+	u64 user_clk_freq_cmd1;
+	u64 user_clk_freq_sts0;
+	u64 user_clk_freq_sts1;
+};
+
+struct feature_fme_tmp_threshold {
+	union {
+		u64 csr;
+		struct {
+			u8  tmp_thshold1:7;	  /* temperature Threshold 1 */
+			/* temperature Threshold 1 enable/disable */
+			u8  tmp_thshold1_enable:1;
+			u8  tmp_thshold2:7;       /* temperature Threshold 2 */
+			/* temperature Threshold 2 enable /disable */
+			u8  tmp_thshold2_enable:1;
+			u8  pro_hot_setpoint:7;   /* Proc Hot set point */
+			u8  rsvd4:1;              /* Reserved */
+			u8  therm_trip_thshold:7; /* Thermeal Trip Threshold */
+			u8  rsvd3:1;              /* Reserved */
+			u8  thshold1_status:1;	  /* Threshold 1 Status */
+			u8  thshold2_status:1;    /* Threshold 2 Status */
+			u8  rsvd5:1;              /* Reserved */
+			/* Thermeal Trip Threshold status */
+			u8  therm_trip_thshold_status:1;
+			u8  rsvd6:4;		  /* Reserved */
+			/* Validation mode- Force Proc Hot */
+			u8  valmodeforce:1;
+			/* Validation mode - Therm trip Hot */
+			u8  valmodetherm:1;
+			u8  rsvd2:2;              /* Reserved */
+			u8  thshold_policy:1;     /* threshold policy */
+			u32 rsvd:19;              /* Reserved */
+		};
+	};
+};
+
+/* Temperature Sensor Read values format 1 */
+struct feature_fme_temp_rdsensor_fmt1 {
+	union {
+		u64 csr;
+		struct {
+			/* Reads out FPGA temperature in celsius */
+			u8  fpga_temp:7;
+			u8  rsvd0:1;			/* Reserved */
+			/* Temperature reading sequence number */
+			u16 tmp_reading_seq_num;
+			/* Temperature reading is valid */
+			u8  tmp_reading_valid:1;
+			u8  rsvd1:7;			/* Reserved */
+			u16 dbg_mode:10;		/* Debug mode */
+			u32 rsvd2:22;			/* Reserved */
+		};
+	};
+};
+
+/* Temperature sensor read values format 2 */
+struct feature_fme_temp_rdsensor_fmt2 {
+	u64 rsvd;	/* Reserved */
+};
+
+/* FME THERNAL FEATURE */
+struct feature_fme_thermal {
+	struct feature_header header;
+	struct feature_fme_tmp_threshold threshold;
+	struct feature_fme_temp_rdsensor_fmt1 rdsensor_fm1;
+	struct feature_fme_temp_rdsensor_fmt2 rdsensor_fm2;
+};
+
+/* Power Status register */
+struct feature_fme_pm_status {
+	union {
+		u64 csr;
+		struct {
+			/* FPGA Power consumed, The format is to be defined */
+			u32 pwr_consumed:18;
+			/* FPGA Latency Tolerance Reporting */
+			u8  fpga_latency_report:1;
+			u64 rsvd:45;			/* Reserved */
+		};
+	};
+};
+
+/* AP Thresholds */
+struct feature_fme_pm_ap_threshold {
+	union {
+		u64 csr;
+		struct {
+			/*
+			 * Number of clocks (5ns period) for assertion
+			 * of FME_data
+			 */
+			u8  threshold1:7;
+			u8  rsvd1:1;
+			u8  threshold2:7;
+			u8  rsvd2:1;
+			u8  threshold1_status:1;
+			u8  threshold2_status:1;
+			u64 rsvd3:46;		/* Reserved */
+		};
+	};
+};
+
+/* Xeon Power Limit */
+struct feature_fme_pm_xeon_limit {
+	union {
+		u64 csr;
+		struct {
+			/* Power limit in Watts in 12.3 format */
+			u16 pwr_limit:15;
+			/* Indicates that power limit has been written */
+			u8  enable:1;
+			/* 0 - Turbe range, 1 - Entire range */
+			u8  clamping:1;
+			/* Time constant in XXYYY format */
+			u8  time:7;
+			u64 rsvd:40;		/* Reserved */
+		};
+	};
+};
+
+/* MCP Power Limit */
+struct feature_fme_pm_fpga_limit {
+	union {
+		u64 csr;
+		struct {
+			/* Power limit in Watts in 12.3 format */
+			u16 pwr_limit:15;
+			/* Indicates that power limit has been written */
+			u8  enable:1;
+			/* 0 - Turbe range, 1 - Entire range */
+			u8  clamping:1;
+			/* Time constant in XXYYY format */
+			u8  time:7;
+			u64 rsvd:40;		/* Reserved */
+		};
+	};
+};
+
+/* FME POWER FEATURE */
+struct feature_fme_power {
+	struct feature_header header;
+	struct feature_fme_pm_status status;
+	struct feature_fme_pm_ap_threshold threshold;
+	struct feature_fme_pm_xeon_limit xeon_limit;
+	struct feature_fme_pm_fpga_limit fpga_limit;
+};
+
+#define CACHE_CHANNEL_RD	0
+#define CACHE_CHANNEL_WR	1
+
+enum iperf_cache_events {
+	IPERF_CACHE_RD_HIT,
+	IPERF_CACHE_WR_HIT,
+	IPERF_CACHE_RD_MISS,
+	IPERF_CACHE_WR_MISS,
+	IPERF_CACHE_RSVD, /* reserved */
+	IPERF_CACHE_HOLD_REQ,
+	IPERF_CACHE_DATA_WR_PORT_CONTEN,
+	IPERF_CACHE_TAG_WR_PORT_CONTEN,
+	IPERF_CACHE_TX_REQ_STALL,
+	IPERF_CACHE_RX_REQ_STALL,
+	IPERF_CACHE_EVICTIONS,
+};
+
+/* FPMON Cache Control */
+struct feature_fme_ifpmon_ch_ctl {
+	union {
+		u64 csr;
+		struct {
+			u8  reset_counters:1;	/* Reset Counters */
+			u8  rsvd1:7;		/* Reserved */
+			u8  freeze:1;		/* Freeze if set to 1 */
+			u8  rsvd2:7;		/* Reserved */
+			u8  cache_event:4;	/* Select the cache event */
+			u8  cci_chsel:1;	/* Select the channel */
+			u64 rsvd3:43;		/* Reserved */
+		};
+	};
+};
+
+/* FPMON Cache Counter */
+struct feature_fme_ifpmon_ch_ctr {
+	union {
+		u64 csr;
+		struct {
+			/* Cache Counter for even addresse */
+			u64 cache_counter:48;
+			u16 rsvd:12;		/* Reserved */
+			/* Cache Event being reported */
+			u8  event_code:4;
+		};
+	};
+};
+
+enum iperf_fab_events {
+	IPERF_FAB_PCIE0_RD,
+	IPERF_FAB_PCIE0_WR,
+	IPERF_FAB_PCIE1_RD,
+	IPERF_FAB_PCIE1_WR,
+	IPERF_FAB_UPI_RD,
+	IPERF_FAB_UPI_WR,
+	IPERF_FAB_MMIO_RD,
+	IPERF_FAB_MMIO_WR,
+};
+
+#define FAB_DISABLE_FILTER     0
+#define FAB_ENABLE_FILTER      1
+
+/* FPMON FAB Control */
+struct feature_fme_ifpmon_fab_ctl {
+	union {
+		u64 csr;
+		struct {
+			u8  reset_counters:1;	/* Reset Counters */
+			u8  rsvd:7;		/* Reserved */
+			u8  freeze:1;		/* Set to 1 frozen counter */
+			u8  rsvd1:7;		/* Reserved */
+			u8  fab_evtcode:4;	/* Fabric Event Code */
+			u8  port_id:2;		/* Port ID */
+			u8  rsvd2:1;		/* Reserved */
+			u8  port_filter:1;	/* Port Filter */
+			u64 rsvd3:40;		/* Reserved */
+		};
+	};
+};
+
+/* FPMON Event Counter */
+struct feature_fme_ifpmon_fab_ctr {
+	union {
+		u64 csr;
+		struct {
+			u64 fab_cnt:60;	/* Fabric event counter */
+			/* Fabric event code being reported */
+			u8  event_code:4;
+		};
+	};
+};
+
+/* FPMON Clock Counter */
+struct feature_fme_ifpmon_clk_ctr {
+	u64 afu_interf_clock;		/* Clk_16UI (AFU clock) counter. */
+};
+
+enum iperf_vtd_events {
+	IPERF_VTD_AFU_MEM_RD_TRANS,
+	IPERF_VTD_AFU_MEM_WR_TRANS,
+	IPERF_VTD_AFU_DEVTLB_RD_HIT,
+	IPERF_VTD_AFU_DEVTLB_WR_HIT,
+	IPERF_VTD_DEVTLB_4K_FILL,
+	IPERF_VTD_DEVTLB_2M_FILL,
+	IPERF_VTD_DEVTLB_1G_FILL,
+};
+
+/* VT-d control register */
+struct feature_fme_ifpmon_vtd_ctl {
+	union {
+		u64 csr;
+		struct {
+			u8  reset_counters:1;	/* Reset Counters */
+			u8  rsvd:7;		/* Reserved */
+			u8  freeze:1;		/* Set to 1 frozen counter */
+			u8  rsvd1:7;		/* Reserved */
+			u8  vtd_evtcode:4;	/* VTd and TLB event code */
+			u64 rsvd2:44;		/* Reserved */
+		};
+	};
+};
+
+/* VT-d event counter */
+struct feature_fme_ifpmon_vtd_ctr {
+	union {
+		u64 csr;
+		struct {
+			u64 vtd_counter:48;	/* VTd event counter */
+			u16 rsvd:12;		/* Reserved */
+			u8  event_code:4;	/* VTd event code */
+		};
+	};
+};
+
+enum iperf_vtd_sip_events {
+	IPERF_VTD_SIP_IOTLB_4K_HIT,
+	IPERF_VTD_SIP_IOTLB_2M_HIT,
+	IPERF_VTD_SIP_IOTLB_1G_HIT,
+	IPERF_VTD_SIP_SLPWC_L3_HIT,
+	IPERF_VTD_SIP_SLPWC_L4_HIT,
+	IPERF_VTD_SIP_RCC_HIT,
+	IPERF_VTD_SIP_IOTLB_4K_MISS,
+	IPERF_VTD_SIP_IOTLB_2M_MISS,
+	IPERF_VTD_SIP_IOTLB_1G_MISS,
+	IPERF_VTD_SIP_SLPWC_L3_MISS,
+	IPERF_VTD_SIP_SLPWC_L4_MISS,
+	IPERF_VTD_SIP_RCC_MISS,
+};
+
+/* VT-d SIP control register */
+struct feature_fme_ifpmon_vtd_sip_ctl {
+	union {
+		u64 csr;
+		struct {
+			u8  reset_counters:1;	/* Reset Counters */
+			u8  rsvd:7;		/* Reserved */
+			u8  freeze:1;		/* Set to 1 frozen counter */
+			u8  rsvd1:7;		/* Reserved */
+			u8  vtd_evtcode:4;	/* VTd and TLB event code */
+			u64 rsvd2:44;		/* Reserved */
+		};
+	};
+};
+
+/* VT-d SIP event counter */
+struct feature_fme_ifpmon_vtd_sip_ctr {
+	union {
+		u64 csr;
+		struct {
+			u64 vtd_counter:48;	/* VTd event counter */
+			u16 rsvd:12;		/* Reserved */
+			u8 event_code:4;	/* VTd event code */
+		};
+	};
+};
+
+/* FME IPERF FEATURE */
+struct feature_fme_iperf {
+	struct feature_header header;
+	struct feature_fme_ifpmon_ch_ctl ch_ctl;
+	struct feature_fme_ifpmon_ch_ctr ch_ctr0;
+	struct feature_fme_ifpmon_ch_ctr ch_ctr1;
+	struct feature_fme_ifpmon_fab_ctl fab_ctl;
+	struct feature_fme_ifpmon_fab_ctr fab_ctr;
+	struct feature_fme_ifpmon_clk_ctr clk;
+	struct feature_fme_ifpmon_vtd_ctl vtd_ctl;
+	struct feature_fme_ifpmon_vtd_ctr vtd_ctr;
+	struct feature_fme_ifpmon_vtd_sip_ctl vtd_sip_ctl;
+	struct feature_fme_ifpmon_vtd_sip_ctr vtd_sip_ctr;
+};
+
+enum dperf_fab_events {
+	DPERF_FAB_PCIE0_RD,
+	DPERF_FAB_PCIE0_WR,
+	DPERF_FAB_MMIO_RD = 6,
+	DPERF_FAB_MMIO_WR,
+};
+
+#define DCP_FAB_DISABLE_FILTER     0
+#define DCP_FAB_ENABLE_FILTER      1
+
+/* FPMON FAB Control */
+struct feature_fme_dfpmon_fab_ctl {
+	union {
+		u64 csr;
+		struct {
+			u8  reset_counters:1;	/* Reset Counters */
+			u8  rsvd:7;		/* Reserved */
+			u8  freeze:1;		/* Set to 1 frozen counter */
+			u8  rsvd1:7;		/* Reserved */
+			u8  fab_evtcode:4;	/* Fabric Event Code */
+			u8  port_id:2;		/* Port ID */
+			u8  rsvd2:1;		/* Reserved */
+			u8  port_filter:1;	/* Port Filter */
+			u64 rsvd3:40;		/* Reserved */
+		};
+	};
+};
+
+/* FPMON Event Counter */
+struct feature_fme_dfpmon_fab_ctr {
+	union {
+		u64 csr;
+		struct {
+			u64 fab_cnt:60;	/* Fabric event counter */
+			/* Fabric event code being reported */
+			u8  event_code:4;
+		};
+	};
+};
+
+/* FPMON Clock Counter */
+struct feature_fme_dfpmon_clk_ctr {
+	u64 afu_interf_clock;		/* Clk_16UI (AFU clock) counter. */
+};
+
+/* FME DPERF FEATURE */
+struct feature_fme_dperf {
+	struct feature_header header;
+	u64 rsvd[3];
+	struct feature_fme_dfpmon_fab_ctl fab_ctl;
+	struct feature_fme_dfpmon_fab_ctr fab_ctr;
+	struct feature_fme_dfpmon_clk_ctr clk;
+};
+
+struct feature_fme_error0 {
+#define FME_ERROR0_MASK        0xFFUL
+#define FME_ERROR0_MASK_DEFAULT 0x40UL  /* pcode workaround */
+	union {
+		u64 csr;
+		struct {
+			u8  fabric_err:1;	/* Fabric error */
+			u8  fabfifo_overflow:1;	/* Fabric fifo overflow */
+			u8  kticdc_parity_err:2;/* KTI CDC Parity Error */
+			u8  iommu_parity_err:1;	/* IOMMU Parity error */
+			/* AFU PF/VF access mismatch detected */
+			u8  afu_acc_mode_err:1;
+			u8  mbp_err:1;		/* Indicates an MBP event */
+			/* PCIE0 CDC Parity Error */
+			u8  pcie0cdc_parity_err:5;
+			/* PCIE1 CDC Parity Error */
+			u8  pcie1cdc_parity_err:5;
+			/* CVL CDC Parity Error */
+			u8  cvlcdc_parity_err:3;
+			u64 rsvd:44;		/* Reserved */
+		};
+	};
+};
+
+/* PCIe0 Error Status register */
+struct feature_fme_pcie0_error {
+#define FME_PCIE0_ERROR_MASK   0xFFUL
+	union {
+		u64 csr;
+		struct {
+			u8  formattype_err:1;	/* TLP format/type error */
+			u8  MWAddr_err:1;	/* TLP MW address error */
+			u8  MWAddrLength_err:1;	/* TLP MW length error */
+			u8  MRAddr_err:1;	/* TLP MR address error */
+			u8  MRAddrLength_err:1;	/* TLP MR length error */
+			u8  cpl_tag_err:1;	/* TLP CPL tag error */
+			u8  cpl_status_err:1;	/* TLP CPL status error */
+			u8  cpl_timeout_err:1;	/* TLP CPL timeout */
+			u8  cci_parity_err:1;	/* CCI bridge parity error */
+			u8  rxpoison_tlp_err:1;	/* Received a TLP with EP set */
+			u64 rsvd:52;		/* Reserved */
+			u8  vfnumb_err:1;	/* Number of error VF */
+			u8  funct_type_err:1;	/* Virtual (1) or Physical */
+		};
+	};
+};
+
+/* PCIe1 Error Status register */
+struct feature_fme_pcie1_error {
+#define FME_PCIE1_ERROR_MASK   0xFFUL
+	union {
+		u64 csr;
+		struct {
+			u8  formattype_err:1;	/* TLP format/type error */
+			u8  MWAddr_err:1;	/* TLP MW address error */
+			u8  MWAddrLength_err:1;	/* TLP MW length error */
+			u8  MRAddr_err:1;	/* TLP MR address error */
+			u8  MRAddrLength_err:1;	/* TLP MR length error */
+			u8  cpl_tag_err:1;	/* TLP CPL tag error */
+			u8  cpl_status_err:1;	/* TLP CPL status error */
+			u8  cpl_timeout_err:1;	/* TLP CPL timeout */
+			u8  cci_parity_err:1;	/* CCI bridge parity error */
+			u8  rxpoison_tlp_err:1;	/* Received a TLP with EP set */
+			u64 rsvd:54;		/* Reserved */
+		};
+	};
+};
+
+
+/* FME First Error register */
+struct feature_fme_first_error {
+#define FME_FIRST_ERROR_MASK   ((1UL << 60) - 1)
+	union {
+		u64 csr;
+		struct {
+			/*
+			 * Indicates the Error Register that was
+			 * triggered first
+			 */
+			u64 err_reg_status:60;
+			/*
+			 * Holds 60 LSBs from the Error register that was
+			 * triggered first
+			 */
+			u8 errReg_id:4;
+		};
+	};
+};
+
+/* FME Next Error register */
+struct feature_fme_next_error {
+#define FME_NEXT_ERROR_MASK    ((1UL << 60) - 1)
+	union {
+		u64 csr;
+		struct {
+			/*
+			 * Indicates the Error Register that was
+			 * triggered second
+			 */
+			u64 err_reg_status:60;
+			/*
+			 * Holds 60 LSBs from the Error register that was
+			 * triggered second
+			 */
+			u8  errReg_id:4;
+		};
+	};
+};
+
+/* RAS Non Fatal Error Status register */
+struct feature_fme_ras_nonfaterror {
+	union {
+		u64 csr;
+		struct {
+			/* thremal threshold AP1 */
+			u8  temp_thresh_ap1:1;
+			/* thremal threshold AP2 */
+			u8  temp_thresh_ap2:1;
+			u8  pcie_error:1;	/* pcie Error */
+			u8  portfatal_error:1;	/* port fatal error */
+			u8  proc_hot:1;		/* Indicates a ProcHot event */
+			/* Indicates an AFU PF/VF access mismatch */
+			u8  afu_acc_mode_err:1;
+			/* Injected nonfata Error */
+			u8  injected_nonfata_err:1;
+			u8  rsvd1:2;
+			/* Temperature threshold triggered AP6*/
+			u8  temp_thresh_AP6:1;
+			/* Power threshold triggered AP1 */
+			u8  power_thresh_AP1:1;
+			/* Power threshold triggered AP2 */
+			u8  power_thresh_AP2:1;
+			/* Indicates a MBP event */
+			u8  mbp_err:1;
+			u64 rsvd2:51;		/* Reserved */
+		};
+	};
+};
+
+/* RAS Catastrophic Fatal Error Status register */
+struct feature_fme_ras_catfaterror {
+	union {
+		u64 csr;
+		struct {
+			/* KTI Link layer error detected */
+			u8  ktilink_fatal_err:1;
+			/* tag-n-cache error detected */
+			u8  tagcch_fatal_err:1;
+			/* CCI error detected */
+			u8  cci_fatal_err:1;
+			/* KTI Protocol error detected */
+			u8  ktiprpto_fatal_err:1;
+			/* Fatal DRAM error detected */
+			u8  dram_fatal_err:1;
+			/* IOMMU detected */
+			u8  iommu_fatal_err:1;
+			/* Fabric Fatal Error */
+			u8  fabric_fatal_err:1;
+			/* PCIe possion Error */
+			u8  pcie_poison_err:1;
+			/* Injected fatal Error */
+			u8  inject_fata_err:1;
+			/* Catastrophic CRC Error */
+			u8  crc_catast_err:1;
+			/* Catastrophic Thermal Error */
+			u8  therm_catast_err:1;
+			/* Injected Catastrophic Error */
+			u8  injected_catast_err:1;
+			u64 rsvd:52;
+		};
+	};
+};
+
+
+/* RAS Error injection register */
+struct feature_fme_ras_error_inj {
+#define FME_RAS_ERROR_INJ_MASK      0x7UL
+	union {
+		u64 csr;
+		struct {
+			u8  catast_error:1;	/* Catastrophic error flag */
+			u8  fatal_error:1;	/* Fatal error flag */
+			u8  nonfatal_error:1;	/* NonFatal error flag */
+			u64 rsvd:61;		/* Reserved */
+		};
+	};
+};
+
+/* FME error capabilities */
+struct feature_fme_error_capability {
+	union {
+	u64 csr;
+		struct {
+			u8 support_intr:1;
+			/* MSI-X vector table entry number */
+			u16 intr_vector_num:12;
+			u64 rsvd:51;	/* Reserved */
+		};
+	};
+};
+
+/* FME ERR FEATURE */
+struct feature_fme_err {
+	struct feature_header header;
+	struct feature_fme_error0 fme_err_mask;
+	struct feature_fme_error0 fme_err;
+	struct feature_fme_pcie0_error pcie0_err_mask;
+	struct feature_fme_pcie0_error pcie0_err;
+	struct feature_fme_pcie1_error pcie1_err_mask;
+	struct feature_fme_pcie1_error pcie1_err;
+	struct feature_fme_first_error fme_first_err;
+	struct feature_fme_next_error fme_next_err;
+	struct feature_fme_ras_nonfaterror ras_nonfat_mask;
+	struct feature_fme_ras_nonfaterror ras_nonfaterr;
+	struct feature_fme_ras_catfaterror ras_catfat_mask;
+	struct feature_fme_ras_catfaterror ras_catfaterr;
+	struct feature_fme_ras_error_inj ras_error_inj;
+	struct feature_fme_error_capability fme_err_capability;
+};
+
+/* FME Partial Reconfiguration Control */
+struct feature_fme_pr_ctl {
+	union {
+		u64 csr;
+		struct {
+			u8  pr_reset:1;		/* Reset PR Engine */
+			u8  rsvd3:3;		/* Reserved */
+			u8  pr_reset_ack:1;	/* Reset PR Engine Ack */
+			u8  rsvd4:3;		/* Reserved */
+			u8  pr_regionid:2;	/* PR Region ID */
+			u8  rsvd1:2;		/* Reserved */
+			u8  pr_start_req:1;	/* PR Start Request */
+			u8  pr_push_complete:1;	/* PR Data push complete */
+			u8  pr_kind:1;		/* PR Data push complete */
+			u32  rsvd:17;		/* Reserved */
+			u32 config_data;	/* Config data TBD */
+		};
+	};
+};
+
+/* FME Partial Reconfiguration Status */
+struct feature_fme_pr_status {
+	union {
+		u64 csr;
+		struct {
+			u16 pr_credit:9;	/* PR Credits */
+			u8  rsvd2:7;		/* Reserved */
+			u8  pr_status:1;	/* PR status */
+			u8  rsvd:3;		/* Reserved */
+			/* Altra PR Controller Block status */
+			u8  pr_contoller_status:3;
+			u8  rsvd1:1;            /* Reserved */
+			u8  pr_host_status:4;   /* PR Host status */
+			u8  rsvd3:4;		/* Reserved */
+			/* Security Block Status fields (TBD) */
+			u32 security_bstatus;
+		};
+	};
+};
+
+/* FME Partial Reconfiguration Data */
+struct feature_fme_pr_data {
+	union {
+		u64 csr;	/* PR data from the raw-binary file */
+		struct {
+			/* PR data from the raw-binary file */
+			u32 pr_data_raw;
+			u32 rsvd;
+		};
+	};
+};
+
+/* FME PR Public Key */
+struct feature_fme_pr_key {
+	u64 key;		/* FME PR Public Hash */
+};
+
+/* FME PR FEATURE */
+struct feature_fme_pr {
+	struct feature_header header;
+	/*Partial Reconfiguration control */
+	struct feature_fme_pr_ctl	ccip_fme_pr_control;
+
+	/* Partial Reconfiguration Status */
+	struct feature_fme_pr_status	ccip_fme_pr_status;
+
+	/* Partial Reconfiguration data */
+	struct feature_fme_pr_data	ccip_fme_pr_data;
+
+	/* Partial Reconfiguration data */
+	u64				ccip_fme_pr_err;
+
+	u64 rsvd1[3];
+
+	/* Partial Reconfiguration data registers */
+	u64 fme_pr_data1;
+	u64 fme_pr_data2;
+	u64 fme_pr_data3;
+	u64 fme_pr_data4;
+	u64 fme_pr_data5;
+	u64 fme_pr_data6;
+	u64 fme_pr_data7;
+	u64 fme_pr_data8;
+
+	u64 rsvd2[5];
+
+	/* PR Interface ID */
+	u64 fme_pr_intfc_id_l;
+	u64 fme_pr_intfc_id_h;
+
+	/* MSIX filed to be Added */
+};
+
+/* FME HSSI Control */
+struct feature_fme_hssi_eth_ctrl {
+	union {
+		u64 csr;
+		struct {
+			u32 data:32;		/* HSSI data */
+			u16 address:16;		/* HSSI address */
+			/*
+			 * HSSI comamnd
+			 * 0x0 - No request
+			 * 0x08 - SW register RD request
+			 * 0x10 - SW register WR request
+			 * 0x40 - Auxiliar bus RD request
+			 * 0x80 - Auxiliar bus WR request
+			 */
+			u16 cmd:16;
+		};
+	};
+};
+
+/* FME HSSI Status */
+struct feature_fme_hssi_eth_stat {
+	union {
+		u64 csr;
+		struct {
+
+			u32 data:32;		/* HSSI data */
+			u8  acknowledge:1;	/* HSSI acknowledge */
+			u8  spare:1;		/* HSSI spare */
+			u32 rsvd:30;		/* Reserved */
+		};
+	};
+};
+
+
+/* FME HSSI FEATURE */
+struct feature_fme_hssi {
+	struct feature_header header;
+	struct feature_fme_hssi_eth_ctrl	hssi_control;
+	struct feature_fme_hssi_eth_stat	hssi_status;
+};
+
+#define PORT_ERR_MASK		0xfff0703ff001f
+struct feature_port_err_key {
+	union {
+		u64 csr;
+		struct {
+			/* Tx Channel0: Overflow */
+			u8 tx_ch0_overflow:1;
+			/* Tx Channel0: Invalid request encoding */
+			u8 tx_ch0_invaldreq :1;
+			/* Tx Channel0: Request with cl_len=3 not supported */
+			u8 tx_ch0_cl_len3:1;
+			/* Tx Channel0: Request with cl_len=2 not aligned 2CL */
+			u8 tx_ch0_cl_len2:1;
+			/* Tx Channel0: Request with cl_len=4 not aligned 4CL */
+			u8 tx_ch0_cl_len4:1;
+
+			u16 rsvd1:4;			/* Reserved */
+
+			/* AFU MMIO RD received while PORT is in reset */
+			u8 mmio_rd_whilerst:1;
+			/* AFU MMIO WR received while PORT is in reset */
+			u8 mmio_wr_whilerst:1;
+
+			u16 rsvd2:5;			/* Reserved */
+
+			/* Tx Channel1: Overflow */
+			u8 tx_ch1_overflow:1;
+			/* Tx Channel1: Invalid request encoding */
+			u8 tx_ch1_invaldreq:1;
+			/* Tx Channel1: Request with cl_len=3 not supported */
+			u8 tx_ch1_cl_len3:1;
+			/* Tx Channel1: Request with cl_len=2 not aligned 2CL */
+			u8 tx_ch1_cl_len2:1;
+			/* Tx Channel1: Request with cl_len=4 not aligned 4CL */
+			u8 tx_ch1_cl_len4:1;
+
+			/* Tx Channel1: Insufficient data payload */
+			u8 tx_ch1_insuff_data:1;
+			/* Tx Channel1: Data payload overrun */
+			u8 tx_ch1_data_overrun:1;
+			/* Tx Channel1 : Incorrect address */
+			u8 tx_ch1_incorr_addr:1;
+			/* Tx Channel1 : NON-Zero SOP Detected */
+			u8 tx_ch1_nzsop:1;
+			/* Tx Channel1 : Illegal VC_SEL, atomic request VLO */
+			u8 tx_ch1_illegal_vcsel:1;
+
+			u8 rsvd3:6;			/* Reserved */
+
+			/* MMIO Read Timeout in AFU */
+			u8 mmioread_timeout:1;
+
+			/* Tx Channel2: FIFO Overflow */
+			u8 tx_ch2_fifo_overflow:1;
+
+			/* MMIO read is not matching pending request */
+			u8 unexp_mmio_resp:1;
+
+			u8 rsvd4:5;			/* Reserved */
+
+			/* Number of pending Requests: counter overflow */
+			u8 tx_req_counter_overflow:1;
+			/* Req with Address violating SMM Range */
+			u8 llpr_smrr_err:1;
+			/* Req with Address violating second SMM Range */
+			u8 llpr_smrr2_err:1;
+			/* Req with Address violating ME Stolen message */
+			u8 llpr_mesg_err:1;
+			/* Req with Address violating Generic Protected Range */
+			u8 genprot_range_err:1;
+			/* Req with Address violating Legacy Range low */
+			u8 legrange_low_err:1;
+			/* Req with Address violating Legacy Range High */
+			u8 legrange_high_err:1;
+			/* Req with Address violating VGA memory range */
+			u8 vgmem_range_err:1;
+			u8 page_fault_err:1;		/* Page fault */
+			u8 pmr_err:1;			/* PMR Error */
+			u8 ap6_event:1;			/* AP6 event */
+			/* VF FLR detected on Port with PF access control */
+			u8 vfflr_access_err:1;
+			u16 rsvd5:12;			/* Reserved */
+		};
+	};
+};
+
+/* Port first error register, not contain all error bits in error register. */
+struct feature_port_first_err_key {
+	union {
+		u64 csr;
+		struct {
+			u8 tx_ch0_overflow:1;
+			u8 tx_ch0_invaldreq :1;
+			u8 tx_ch0_cl_len3:1;
+			u8 tx_ch0_cl_len2:1;
+			u8 tx_ch0_cl_len4:1;
+			u8 rsvd1:4;			/* Reserved */
+			u8 mmio_rd_whilerst:1;
+			u8 mmio_wr_whilerst:1;
+			u8 rsvd2:5;			/* Reserved */
+			u8 tx_ch1_overflow:1;
+			u8 tx_ch1_invaldreq:1;
+			u8 tx_ch1_cl_len3:1;
+			u8 tx_ch1_cl_len2:1;
+			u8 tx_ch1_cl_len4:1;
+			u8 tx_ch1_insuff_data:1;
+			u8 tx_ch1_data_overrun:1;
+			u8 tx_ch1_incorr_addr:1;
+			u8 tx_ch1_nzsop:1;
+			u8 tx_ch1_illegal_vcsel:1;
+			u8 rsvd3:6;			/* Reserved */
+			u8 mmioread_timeout:1;
+			u8 tx_ch2_fifo_overflow:1;
+			u8 rsvd4:6;			/* Reserved */
+			u8 tx_req_counter_overflow:1;
+			u32 rsvd5:23;			/* Reserved */
+		};
+	};
+};
+
+/* Port malformed Req0 */
+struct feature_port_malformed_req0 {
+	u64 header_lsb;
+};
+
+/* Port malformed Req1 */
+struct feature_port_malformed_req1 {
+	u64 header_msb;
+};
+
+/* Port debug register */
+struct feature_port_debug {
+	u64 port_debug;
+};
+
+/* Port error capabilities */
+struct feature_port_err_capability {
+	union {
+		u64 csr;
+		struct {
+			u8  support_intr:1;
+			/* MSI-X vector table entry number */
+			u16 intr_vector_num:12;
+			u64 rsvd:51;            /* Reserved */
+		};
+	};
+};
+
+/* PORT FEATURE ERROR */
+struct feature_port_error {
+	struct feature_header header;
+	struct feature_port_err_key error_mask;
+	struct feature_port_err_key port_error;
+	struct feature_port_first_err_key port_first_error;
+	struct feature_port_malformed_req0 malreq0;
+	struct feature_port_malformed_req1 malreq1;
+	struct feature_port_debug port_debug;
+	struct feature_port_err_capability error_capability;
+};
+
+/* Port UMSG Capability */
+struct feature_port_umsg_cap {
+	union {
+		u64 csr;
+		struct {
+			/* Number of umsg allocated to this port */
+			u8 umsg_allocated;
+			/* Enable / Disable UMsg engine for this port */
+			u8 umsg_enable:1;
+			/* Usmg initialization status */
+			u8 umsg_init_complete:1;
+			/* IOMMU can not translate the umsg base address */
+			u8 umsg_trans_error:1;
+			u64 rsvd:53;		/* Reserved */
+		};
+	};
+};
+
+/* Port UMSG base address */
+struct feature_port_umsg_baseaddr {
+	union {
+		u64 csr;
+		struct {
+			u64 base_addr:48;	/* 48 bit physical address */
+			u16 rsvd;		/* Reserved */
+		};
+	};
+};
+
+struct feature_port_umsg_mode {
+	union {
+		u64 csr;
+		struct {
+			u32 umsg_hint_enable;	/* UMSG hint enable/disable */
+			u32 rsvd;		/* Reserved */
+		};
+	};
+};
+
+/* PORT FEATURE UMSG */
+struct feature_port_umsg {
+	struct feature_header header;
+	struct feature_port_umsg_cap capability;
+	struct feature_port_umsg_baseaddr baseaddr;
+	struct feature_port_umsg_mode mode;
+};
+
+/* Port UINT Capability */
+struct feature_port_uint_cap {
+	union {
+		u64 csr;
+		struct {
+			u16 intr_num:12;	/* Supported interrupts num */
+			/* First MSI-X vector table entry number */
+			u16 first_vec_num:12;
+			u64 rsvd:40;
+		};
+	};
+};
+
+/* PORT FEATURE UINT */
+struct feature_port_uint {
+	struct feature_header header;
+	struct feature_port_uint_cap capability;
+};
+
+/* STP region supports mmap operation, so use page aligned size. */
+#define PORT_FEATURE_STP_REGION_SIZE	PAGE_ALIGN(sizeof(struct feature_port_stp))
+
+/* Port STP status register (for debug only)*/
+struct feature_port_stp_status {
+	union {
+		u64 csr;
+		struct {
+			/* SLD Hub end-point read/write timeout */
+			u8 sld_ep_timeout:1;
+			/* Remote STP in reset/disable */
+			u8 rstp_disabled:1;
+			u8 unsupported_read:1;
+			/* MMIO timeout detected and faked with a response */
+			u8 mmio_timeout:1;
+			u8 txfifo_count:4;
+			u8 rxfifo_count:4;
+			u8 txfifo_overflow:1;
+			u8 txfifo_underflow:1;
+			u8 rxfifo_overflow:1;
+			u8 rxfifo_underflow:1;
+			/* Number of MMIO write requests */
+			u16 write_requests;
+			/* Number of MMIO read requests */
+			u16 read_requests;
+			/* Number of MMIO read responses */
+			u16 read_responses;
+		};
+	};
+};
+
+/*
+ * PORT FEATURE STP
+ * Most registers in STP region are not touched by driver, but mmapped to user
+ * space. So they are not defined in below data structure, as its actual size
+ * is 0x18c per spec.
+ */
+struct feature_port_stp {
+	struct feature_header header;
+	struct feature_port_stp_status stp_status;
+};
+
+#pragma pack()
+
+struct feature_driver {
+	const char *name;
+	struct feature_ops *ops;
+};
+
+struct feature_irq_ctx {
+	struct eventfd_ctx *trigger;
+	char *name;
+	int irq;
+};
+
+struct feature {
+	const char *name;
+	int resource_index;
+	void __iomem *ioaddr;
+	struct feature_irq_ctx *ctx;
+	int ctx_num;
+	struct feature_ops *ops;
+};
+
+struct feature_platform_data {
+	/* list the feature dev to cci_drvdata->port_dev_list. */
+	struct list_head node;
+
+	struct mutex lock;
+	int excl_open;
+	int open_count;
+	struct cdev cdev;
+	struct platform_device *dev;
+	unsigned int disable_count;
+
+	void *private;
+
+	int num;
+	int (*config_port)(struct platform_device *, u32, bool);
+	struct platform_device *(*fpga_for_each_port)(struct platform_device *,
+			void *, int (*match)(struct platform_device *, void *));
+	struct feature features[0];
+};
+
+static inline int
+feature_dev_use_excl_begin(struct feature_platform_data *pdata)
+{
+	/*
+	 * If device file is opened with O_EXCL flag, check the open_count
+	 * and set excl_open and increate open_count to ensure exclusive use.
+	 */
+	mutex_lock(&pdata->lock);
+	if (pdata->open_count) {
+		mutex_unlock(&pdata->lock);
+		return -EBUSY;
+	}
+	pdata->excl_open = 1;
+	pdata->open_count++;
+	mutex_unlock(&pdata->lock);
+
+	return 0;
+}
+
+static inline int feature_dev_use_begin(struct feature_platform_data *pdata)
+{
+	mutex_lock(&pdata->lock);
+	if (pdata->excl_open) {
+		mutex_unlock(&pdata->lock);
+		return -EBUSY;
+	}
+	pdata->open_count++;
+	mutex_unlock(&pdata->lock);
+
+	return 0;
+}
+
+static inline void __feature_dev_use_end(struct feature_platform_data *pdata)
+{
+	pdata->excl_open = 0;
+	pdata->open_count--;
+}
+
+static inline void feature_dev_use_end(struct feature_platform_data *pdata)
+{
+	mutex_lock(&pdata->lock);
+	__feature_dev_use_end(pdata);
+	mutex_unlock(&pdata->lock);
+}
+
+static inline void
+fpga_pdata_set_private(struct feature_platform_data *pdata, void *private)
+{
+	pdata->private = private;
+}
+
+static inline void *fpga_pdata_get_private(struct feature_platform_data *pdata)
+{
+	return pdata->private;
+}
+
+struct feature_ops {
+	int (*init)(struct platform_device *pdev, struct feature *feature);
+	void (*uinit)(struct platform_device *pdev, struct feature *feature);
+	long (*ioctl)(struct platform_device *pdev, struct feature *feature,
+				unsigned int cmd, unsigned long arg);
+	int (*test)(struct platform_device *pdev, struct feature *feature);
+};
+
+enum fme_feature_id {
+	FME_FEATURE_ID_HEADER = 0x0,
+
+	FME_FEATURE_ID_THERMAL_MGMT	= 0x1,
+	FME_FEATURE_ID_POWER_MGMT = 0x2,
+	FME_FEATURE_ID_GLOBAL_IPERF = 0x3,
+	FME_FEATURE_ID_GLOBAL_ERR = 0x4,
+	FME_FEATURE_ID_PR_MGMT = 0x5,
+	FME_FEATURE_ID_HSSI_ETH = 0x6,
+	FME_FEATURE_ID_GLOBAL_DPERF = 0x7,
+	FME_FEATURE_ID_QSPI_FLASH = 0x8,
+
+	/* one for fme header. */
+	FME_FEATURE_ID_MAX = 0x9,
+};
+
+enum port_feature_id {
+	PORT_FEATURE_ID_HEADER = 0x0,
+	PORT_FEATURE_ID_ERROR = 0x1,
+	PORT_FEATURE_ID_UMSG = 0x2,
+	PORT_FEATURE_ID_UINT = 0x3,
+	PORT_FEATURE_ID_STP = 0x4,
+	PORT_FEATURE_ID_UAFU = 0x5,
+	PORT_FEATURE_ID_MAX = 0x6,
+};
+
+
+int fme_feature_num(void);
+int port_feature_num(void);
+
+#define FPGA_FEATURE_DEV_FME		"intel-fpga-fme"
+#define FPGA_FEATURE_DEV_PORT		"intel-fpga-port"
+
+void feature_platform_data_add(struct feature_platform_data *pdata,
+			       int index, const char *name,
+			       int resource_index, void __iomem *ioaddr,
+			       struct feature_irq_ctx *ctx,
+			       unsigned int ctx_num);
+int feature_platform_data_size(int num);
+struct feature_platform_data *
+feature_platform_data_alloc_and_init(struct platform_device *dev, int num);
+
+void fpga_dev_feature_uinit(struct platform_device *pdev);
+int fpga_dev_feature_init(struct platform_device *pdev,
+			  struct feature_driver *feature_drvs);
+
+enum fpga_devt_type {
+	FPGA_DEVT_FME,
+	FPGA_DEVT_PORT,
+	FPGA_DEVT_MAX,
+};
+
+void fpga_chardev_uinit(void);
+int fpga_chardev_init(void);
+dev_t fpga_get_devt(enum fpga_devt_type type, int id);
+int fpga_register_dev_ops(struct platform_device *pdev,
+			  const struct file_operations *fops,
+			  struct module *owner);
+void fpga_unregister_dev_ops(struct platform_device *pdev);
+
+int fpga_port_id(struct platform_device *pdev);
+
+static inline int fpga_port_check_id(struct platform_device *pdev,
+				     void *pport_id)
+{
+	return fpga_port_id(pdev) == *(int *)pport_id;
+}
+
+void __fpga_port_enable(struct platform_device *pdev);
+int __fpga_port_disable(struct platform_device *pdev);
+
+static inline void fpga_port_enable(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+
+	mutex_lock(&pdata->lock);
+	__fpga_port_enable(pdev);
+	mutex_unlock(&pdata->lock);
+}
+
+static inline int fpga_port_disable(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	int ret;
+
+	mutex_lock(&pdata->lock);
+	ret = __fpga_port_disable(pdev);
+	mutex_unlock(&pdata->lock);
+
+	return ret;
+}
+
+static inline int __fpga_port_reset(struct platform_device *pdev)
+{
+	int ret;
+
+	ret = __fpga_port_disable(pdev);
+	if (ret)
+		return ret;
+
+	__fpga_port_enable(pdev);
+	return 0;
+}
+
+static inline int fpga_port_reset(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	int ret;
+
+	mutex_lock(&pdata->lock);
+	ret = __fpga_port_reset(pdev);
+	mutex_unlock(&pdata->lock);
+	return ret;
+}
+
+static inline
+struct platform_device *fpga_inode_to_feature_dev(struct inode *inode)
+{
+	struct feature_platform_data *pdata;
+
+	pdata = container_of(inode->i_cdev, struct feature_platform_data, cdev);
+	return pdata->dev;
+}
+
+static inline void __iomem *
+get_feature_ioaddr_by_index(struct device *dev, int index)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+
+	return pdata->features[index].ioaddr;
+}
+
+static inline bool is_feature_present(struct device *dev, int index)
+{
+	return !!get_feature_ioaddr_by_index(dev, index);
+}
+
+static inline struct device *
+fpga_feature_dev_to_pcidev(struct platform_device *dev)
+{
+	return dev->dev.parent->parent;
+}
+
+static inline struct device *
+fpga_pdata_to_pcidev(struct feature_platform_data *pdata)
+{
+	return fpga_feature_dev_to_pcidev(pdata->dev);
+}
+
+#define fpga_dev_for_each_feature(pdata, feature)			    \
+	for ((feature) = (pdata)->features;				    \
+	   (feature) < (pdata)->features + (pdata)->num; (feature)++)
+
+void check_features_header(struct pci_dev *pdev, struct feature_header *hdr,
+			   enum fpga_devt_type type, int id);
+int fpga_msix_set_block(struct feature *feature, unsigned int start,
+			unsigned int count, int32_t *fds);
+/*
+ * Wait register's _field to be changed to the given value (_expect's _field)
+ * by polling with given interval and timeout.
+ */
+#define fpga_wait_register_field(_field, _expect, _reg_addr, _timeout, _invl)\
+({									     \
+	int wait = 0;							     \
+	int ret = -ETIMEDOUT;						     \
+	typeof(_expect) value;						     \
+	for (; wait <= _timeout; wait += _invl) {			     \
+		value.csr = readq(_reg_addr);				     \
+		if (_expect._field == value._field) {			     \
+			ret = 0;					     \
+			break;						     \
+		}							     \
+		udelay(_invl);						     \
+	}								     \
+	ret;								     \
+})
+
+#endif
diff --git a/drivers/fpga/intel/fme-dperf.c b/drivers/fpga/intel/fme-dperf.c
new file mode 100644
index 000000000000..83d06c4bca40
--- /dev/null
+++ b/drivers/fpga/intel/fme-dperf.c
@@ -0,0 +1,415 @@
+/*
+ * Driver for FPGA Global Performance
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include "feature-dev.h"
+#include "fme.h"
+
+static ssize_t perf_obj_attr_show(struct kobject *kobj,
+				  struct attribute *__attr, char *buf)
+{
+	struct perf_obj_attributte *attr = to_perf_obj_attr(__attr);
+	struct perf_object *pobj = to_perf_obj(kobj);
+	ssize_t ret = -EIO;
+
+	if (attr->show)
+		ret = attr->show(pobj, buf);
+	return ret;
+}
+
+static ssize_t perf_obj_attr_store(struct kobject *kobj,
+				   struct attribute *__attr,
+				   const char *buf, size_t n)
+{
+	struct perf_obj_attributte *attr = to_perf_obj_attr(__attr);
+	struct perf_object *pobj = to_perf_obj(kobj);
+	ssize_t ret = -EIO;
+
+	if (attr->store)
+		ret = attr->store(pobj, buf, n);
+	return ret;
+}
+
+static const struct sysfs_ops perf_obj_sysfs_ops = {
+	.show = perf_obj_attr_show,
+	.store = perf_obj_attr_store,
+};
+
+static void perf_obj_release(struct kobject *kobj)
+{
+	kfree(to_perf_obj(kobj));
+}
+
+static struct kobj_type perf_obj_ktype = {
+	.sysfs_ops = &perf_obj_sysfs_ops,
+	.release = perf_obj_release,
+};
+
+static ssize_t revision_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_dperf *dperf;
+	struct feature_header header;
+
+	dperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_DPERF);
+	header.csr = readq(&dperf->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+static PERF_OBJ_ATTR_RO(revision);
+
+static ssize_t clock_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_dperf *dperf;
+	struct feature_fme_dfpmon_clk_ctr clk;
+
+	dperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_DPERF);
+	clk.afu_interf_clock = readq(&dperf->clk);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", clk.afu_interf_clock);
+}
+static PERF_OBJ_ATTR_RO(clock);
+
+static struct attribute *clock_attrs[] = {
+	&perf_obj_attr_revision.attr,
+	&perf_obj_attr_clock.attr,
+	NULL,
+};
+
+static struct attribute_group clock_attr_group = {
+	.attrs = clock_attrs,
+};
+
+#define DPERF_TIMEOUT	30
+
+static const struct attribute_group *perf_dev_attr_groups[] = {
+	&clock_attr_group,
+	NULL,
+};
+
+static bool fabric_pobj_is_enabled(struct perf_object *pobj,
+				   struct feature_fme_dperf *dperf)
+{
+	struct feature_fme_dfpmon_fab_ctl ctl;
+
+	ctl.csr = readq(&dperf->fab_ctl);
+
+	if (ctl.port_filter == FAB_DISABLE_FILTER)
+		return pobj->id == PERF_OBJ_ROOT_ID;
+
+	return pobj->id == ctl.port_id;
+}
+
+static ssize_t read_fabric_counter(struct perf_object *pobj,
+				   enum dperf_fab_events fab_event, char *buf)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(pobj->fme_dev);
+	struct feature_fme_dfpmon_fab_ctl ctl;
+	struct feature_fme_dfpmon_fab_ctr ctr;
+	struct feature_fme_dperf *dperf;
+	u64 counter = 0;
+
+	mutex_lock(&pdata->lock);
+	dperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_DPERF);
+
+	/* if it is disabled, force the counter to return zero. */
+	if (!fabric_pobj_is_enabled(pobj, dperf))
+		goto exit;
+
+	ctl.csr = readq(&dperf->fab_ctl);
+	ctl.fab_evtcode = fab_event;
+	writeq(ctl.csr, &dperf->fab_ctl);
+
+	ctr.event_code = fab_event;
+
+	if (fpga_wait_register_field(event_code, ctr,
+				     &dperf->fab_ctr, DPERF_TIMEOUT, 1)) {
+		dev_err(pobj->fme_dev, "timeout, unmatched VTd event type in counter registers.\n");
+		mutex_unlock(&pdata->lock);
+		return -ETIMEDOUT;
+	}
+
+	ctr.csr = readq(&dperf->fab_ctr);
+	counter = ctr.fab_cnt;
+exit:
+	mutex_unlock(&pdata->lock);
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", counter);
+}
+
+#define FAB_SHOW(name, event)						\
+static ssize_t name##_show(struct perf_object *pobj, char *buf)		\
+{									\
+	return read_fabric_counter(pobj, event, buf);			\
+}									\
+static PERF_OBJ_ATTR_RO(name)
+
+FAB_SHOW(pcie0_read, DPERF_FAB_PCIE0_RD);
+FAB_SHOW(pcie0_write, DPERF_FAB_PCIE0_WR);
+FAB_SHOW(mmio_read, DPERF_FAB_MMIO_RD);
+FAB_SHOW(mmio_write, DPERF_FAB_MMIO_WR);
+
+static ssize_t fab_enable_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_dperf *dperf;
+	int status;
+
+	dperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_DPERF);
+
+	status = fabric_pobj_is_enabled(pobj, dperf);
+	return scnprintf(buf, PAGE_SIZE, "%d\n", status);
+}
+
+/*
+ * If enable one port or all port event counter in fabric, other
+ * fabric event counter originally enabled will be disable automatically.
+ */
+static ssize_t fab_enable_store(struct perf_object *pobj,
+				const char *buf, size_t n)
+{
+	struct feature_platform_data *pdata  = dev_get_platdata(pobj->fme_dev);
+	struct feature_fme_dfpmon_fab_ctl ctl;
+	struct feature_fme_dperf *dperf;
+	bool state;
+
+	if (strtobool(buf, &state))
+		return -EINVAL;
+
+	if (!state)
+		return -EINVAL;
+
+	dperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_DPERF);
+
+	/* if it is already enabled. */
+	if (fabric_pobj_is_enabled(pobj, dperf))
+		return n;
+
+	mutex_lock(&pdata->lock);
+	ctl.csr = readq(&dperf->fab_ctl);
+	if (pobj->id == PERF_OBJ_ROOT_ID)
+		ctl.port_filter = FAB_DISABLE_FILTER;
+	else {
+		ctl.port_filter = FAB_ENABLE_FILTER;
+		ctl.port_id = pobj->id;
+	}
+
+	writeq(ctl.csr, &dperf->fab_ctl);
+	mutex_unlock(&pdata->lock);
+
+	return n;
+}
+
+static PERF_OBJ_ATTR(fab_enable, enable, 0644, fab_enable_show,
+		     fab_enable_store);
+
+static struct attribute *fabric_attrs[] = {
+	&perf_obj_attr_pcie0_read.attr,
+	&perf_obj_attr_pcie0_write.attr,
+	&perf_obj_attr_mmio_read.attr,
+	&perf_obj_attr_mmio_write.attr,
+	&perf_obj_attr_fab_enable.attr,
+	NULL,
+};
+
+static struct attribute_group fabric_attr_group = {
+	.attrs = fabric_attrs,
+};
+
+static const struct attribute_group *fabric_attr_groups[] = {
+	&fabric_attr_group,
+	NULL,
+};
+
+static ssize_t fab_freeze_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_dperf *dperf;
+	struct feature_fme_dfpmon_fab_ctl ctl;
+
+	dperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_DPERF);
+	ctl.csr = readq(&dperf->fab_ctl);
+	return scnprintf(buf, PAGE_SIZE, "%d\n", ctl.freeze);
+}
+
+static ssize_t fab_freeze_store(struct perf_object *pobj,
+				const char *buf, size_t n)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(pobj->fme_dev);
+	struct feature_fme_dperf *dperf;
+	struct feature_fme_dfpmon_fab_ctl ctl;
+	bool state;
+
+	if (strtobool(buf, &state))
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	dperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_DPERF);
+	ctl.csr = readq(&dperf->fab_ctl);
+	ctl.freeze = state;
+	writeq(ctl.csr, &dperf->fab_ctl);
+	mutex_unlock(&pdata->lock);
+
+	return n;
+}
+
+static PERF_OBJ_ATTR(fab_freeze, freeze, 0644, fab_freeze_show,
+		     fab_freeze_store);
+
+static struct attribute *fabric_top_attrs[] = {
+	&perf_obj_attr_fab_freeze.attr,
+	NULL,
+};
+
+static struct attribute_group fabric_top_attr_group = {
+	.attrs = fabric_top_attrs,
+};
+
+static const struct attribute_group *fabric_top_attr_groups[] = {
+	&fabric_attr_group,
+	&fabric_top_attr_group,
+	NULL,
+};
+
+static struct perf_object *
+create_perf_obj(struct device *fme_dev, struct kobject *parent, int id,
+		const struct attribute_group **groups, const char *name)
+{
+	struct perf_object *pobj;
+	int ret;
+
+	pobj = kzalloc(sizeof(*pobj), GFP_KERNEL);
+	if (!pobj)
+		return ERR_PTR(-ENOMEM);
+
+	pobj->id = id;
+	pobj->fme_dev = fme_dev;
+	pobj->attr_groups = groups;
+	INIT_LIST_HEAD(&pobj->node);
+	INIT_LIST_HEAD(&pobj->children);
+
+	if (id != PERF_OBJ_ROOT_ID)
+		ret = kobject_init_and_add(&pobj->kobj, &perf_obj_ktype,
+					   parent, "%s%d", name, id);
+	else
+		ret = kobject_init_and_add(&pobj->kobj, &perf_obj_ktype,
+					   parent, "%s", name);
+	if (ret)
+		goto put_exit;
+
+	if (pobj->attr_groups) {
+		ret = sysfs_create_groups(&pobj->kobj, pobj->attr_groups);
+		if (ret)
+			goto put_exit;
+	}
+
+	return pobj;
+
+put_exit:
+	kobject_put(&pobj->kobj);
+	return ERR_PTR(ret);
+}
+
+static void destroy_perf_obj(struct perf_object *pobj)
+{
+	struct perf_object *obj, *obj_tmp;
+
+	list_for_each_entry_safe(obj, obj_tmp, &pobj->children, node)
+		destroy_perf_obj(obj);
+
+	list_del(&pobj->node);
+	if (pobj->attr_groups)
+		sysfs_remove_groups(&pobj->kobj, pobj->attr_groups);
+	kobject_put(&pobj->kobj);
+}
+
+#define PERF_MAX_PORT_NUM	1
+
+static int create_perf_fabric_obj(struct perf_object *perf_dev)
+{
+	struct perf_object *pobj;
+	int i;
+
+	pobj = create_perf_obj(perf_dev->fme_dev, &perf_dev->kobj,
+			       PERF_OBJ_ROOT_ID, fabric_top_attr_groups,
+			       "fabric");
+	if (IS_ERR(pobj))
+		return PTR_ERR(pobj);
+
+	list_add(&pobj->node, &perf_dev->children);
+
+	for (i = 0; i < PERF_MAX_PORT_NUM; i++) {
+		struct perf_object *obj;
+
+		obj = create_perf_obj(perf_dev->fme_dev, &pobj->kobj, i,
+				      fabric_attr_groups, "port");
+		if (IS_ERR(obj))
+			return PTR_ERR(obj);
+
+		list_add(&obj->node, &pobj->children);
+	}
+
+	return 0;
+}
+
+static struct perf_object *create_perf_dev(struct platform_device *pdev)
+{
+	return create_perf_obj(&pdev->dev, &pdev->dev.kobj,
+			   PERF_OBJ_ROOT_ID, perf_dev_attr_groups, "dperf");
+}
+
+static int fme_dperf_init(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+	struct perf_object *perf_dev;
+	int ret;
+
+	perf_dev = create_perf_dev(pdev);
+	if (IS_ERR(perf_dev))
+		return PTR_ERR(perf_dev);
+
+	ret = create_perf_fabric_obj(perf_dev);
+	if (ret) {
+		destroy_perf_obj(perf_dev);
+		return ret;
+	}
+
+	fme = fpga_pdata_get_private(pdata);
+	fme->dperf_dev = perf_dev;
+	return 0;
+}
+
+static void
+fme_dperf_uinit(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+
+	fme = fpga_pdata_get_private(pdata);
+	destroy_perf_obj(fme->dperf_dev);
+	fme->dperf_dev = NULL;
+}
+
+struct feature_ops global_dperf_ops = {
+	.init = fme_dperf_init,
+	.uinit = fme_dperf_uinit,
+};
diff --git a/drivers/fpga/intel/fme-error.c b/drivers/fpga/intel/fme-error.c
new file mode 100644
index 000000000000..dec5fd6229e9
--- /dev/null
+++ b/drivers/fpga/intel/fme-error.c
@@ -0,0 +1,452 @@
+/*
+ * Driver for FPGA Management Engine Error Management
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/stddef.h>
+#include <linux/errno.h>
+#include <linux/uaccess.h>
+#include <linux/fpga/fpga-mgr.h>
+
+#include "feature-dev.h"
+#include "fme.h"
+
+static ssize_t errors_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_error0 fme_error0;
+
+	fme_error0.csr = readq(&fme_err->fme_err);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", fme_error0.csr);
+}
+
+static DEVICE_ATTR_RO(errors);
+
+static ssize_t first_error_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_first_error fme_first_err;
+
+	fme_first_err.csr = readq(&fme_err->fme_first_err);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n",
+			(unsigned long long)fme_first_err.err_reg_status);
+}
+
+static DEVICE_ATTR_RO(first_error);
+
+static ssize_t next_error_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_next_error fme_next_err;
+
+	fme_next_err.csr = readq(&fme_err->fme_next_err);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n",
+			(unsigned long long)fme_next_err.err_reg_status);
+}
+
+static DEVICE_ATTR_RO(next_error);
+
+static ssize_t clear_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev->parent);
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_error0 fme_error0;
+	struct feature_fme_first_error fme_first_err;
+	struct feature_fme_next_error fme_next_err;
+	u64 val;
+
+	if (kstrtou64(buf, 0, &val))
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	writeq(FME_ERROR0_MASK, &fme_err->fme_err_mask);
+
+	fme_error0.csr = readq(&fme_err->fme_err);
+	if (val != fme_error0.csr) {
+		count = -EBUSY;
+		goto exit;
+	}
+
+	fme_first_err.csr = readq(&fme_err->fme_first_err);
+	fme_next_err.csr = readq(&fme_err->fme_next_err);
+
+	writeq(fme_error0.csr & FME_ERROR0_MASK, &fme_err->fme_err);
+	writeq(fme_first_err.csr & FME_FIRST_ERROR_MASK,
+		&fme_err->fme_first_err);
+	writeq(fme_next_err.csr & FME_NEXT_ERROR_MASK,
+		&fme_err->fme_next_err);
+
+exit:
+	writeq(FME_ERROR0_MASK_DEFAULT, &fme_err->fme_err_mask);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static DEVICE_ATTR_WO(clear);
+
+static ssize_t revision_show(struct device *dev, struct device_attribute *attr,
+			     char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_header header;
+
+	header.csr = readq(&fme_err->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+
+static DEVICE_ATTR_RO(revision);
+
+static ssize_t pcie0_errors_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_pcie0_error pcie0_err;
+
+	pcie0_err.csr = readq(&fme_err->pcie0_err);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", pcie0_err.csr);
+}
+
+static ssize_t pcie0_errors_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev->parent);
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_pcie0_error pcie0_err;
+	u64 val;
+
+	if (kstrtou64(buf, 0, &val))
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	writeq(FME_PCIE0_ERROR_MASK, &fme_err->pcie0_err_mask);
+
+	pcie0_err.csr = readq(&fme_err->pcie0_err);
+	if (val != pcie0_err.csr)
+		count = -EBUSY;
+	else
+		writeq(pcie0_err.csr & FME_PCIE0_ERROR_MASK,
+				&fme_err->pcie0_err);
+
+	writeq(0UL, &fme_err->pcie0_err_mask);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(pcie0_errors);
+
+static ssize_t pcie1_errors_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_pcie1_error pcie1_err;
+
+	pcie1_err.csr = readq(&fme_err->pcie1_err);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", pcie1_err.csr);
+}
+
+static ssize_t pcie1_errors_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev->parent);
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_pcie1_error pcie1_err;
+	u64 val;
+
+	if (kstrtou64(buf, 0, &val))
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	writeq(FME_PCIE1_ERROR_MASK, &fme_err->pcie1_err_mask);
+
+	pcie1_err.csr = readq(&fme_err->pcie1_err);
+	if (val != pcie1_err.csr)
+		count = -EBUSY;
+	else
+		writeq(pcie1_err.csr & FME_PCIE1_ERROR_MASK,
+				&fme_err->pcie1_err);
+
+	writeq(0UL, &fme_err->pcie1_err_mask);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(pcie1_errors);
+
+static ssize_t nonfatal_errors_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_ras_nonfaterror ras_nonfaterr;
+
+	ras_nonfaterr.csr = readq(&fme_err->ras_nonfaterr);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", ras_nonfaterr.csr);
+}
+
+static DEVICE_ATTR_RO(nonfatal_errors);
+
+static ssize_t catfatal_errors_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_ras_catfaterror ras_catfaterr;
+
+	ras_catfaterr.csr = readq(&fme_err->ras_catfaterr);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", ras_catfaterr.csr);
+}
+
+static DEVICE_ATTR_RO(catfatal_errors);
+
+static ssize_t inject_error_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_ras_error_inj ras_error_inj;
+
+	ras_error_inj.csr = readq(&fme_err->ras_error_inj);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n",
+			ras_error_inj.csr & FME_RAS_ERROR_INJ_MASK);
+}
+
+static ssize_t inject_error_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev->parent);
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_ras_error_inj ras_error_inj;
+	int err;
+	u8 data;
+
+	mutex_lock(&pdata->lock);
+	ras_error_inj.csr = readq(&fme_err->ras_error_inj);
+
+	err = kstrtou8(buf, 0, &data);
+	if (err) {
+		mutex_unlock(&pdata->lock);
+		return err;
+	}
+
+	if (data <= FME_RAS_ERROR_INJ_MASK)
+		ras_error_inj.csr = data;
+	else {
+		mutex_unlock(&pdata->lock);
+		return -EINVAL;
+	}
+
+	writeq(ras_error_inj.csr, &fme_err->ras_error_inj);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(inject_error);
+
+static struct attribute *fme_errors_attrs[] = {
+	&dev_attr_errors.attr,
+	&dev_attr_first_error.attr,
+	&dev_attr_next_error.attr,
+	&dev_attr_clear.attr,
+	NULL,
+};
+
+struct attribute_group fme_errors_attr_group = {
+	.attrs	= fme_errors_attrs,
+	.name	= "fme-errors",
+};
+
+static struct attribute *errors_attrs[] = {
+	&dev_attr_revision.attr,
+	&dev_attr_pcie0_errors.attr,
+	&dev_attr_pcie1_errors.attr,
+	&dev_attr_nonfatal_errors.attr,
+	&dev_attr_catfatal_errors.attr,
+	&dev_attr_inject_error.attr,
+	NULL,
+};
+
+struct attribute_group errors_attr_group = {
+	.attrs	= errors_attrs,
+};
+
+static const struct attribute_group *error_groups[] = {
+	&fme_errors_attr_group,
+	&errors_attr_group,
+	NULL
+};
+
+static void fme_error_enable(struct platform_device *pdev)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(&pdev->dev,
+			FME_FEATURE_ID_GLOBAL_ERR);
+
+	writeq(FME_ERROR0_MASK_DEFAULT, &fme_err->fme_err_mask);
+	writeq(0UL, &fme_err->pcie0_err_mask);
+	writeq(0UL, &fme_err->pcie1_err_mask);
+	writeq(0UL, &fme_err->ras_nonfat_mask);
+	writeq(0UL, &fme_err->ras_catfat_mask);
+}
+
+static int global_error_init(struct platform_device *pdev,
+		struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+	struct device *dev;
+	int ret = 0;
+
+	dev = kzalloc(sizeof(struct device), GFP_KERNEL);
+	if (!dev)
+		return -ENOMEM;
+
+	dev->parent = &pdev->dev;
+	dev->release = (void (*)(struct device *))kfree;
+	dev_set_name(dev, "errors");
+
+	fme_error_enable(pdev);
+
+	ret = device_register(dev);
+	if (ret) {
+		put_device(dev);
+		return ret;
+	}
+
+	ret = sysfs_create_groups(&dev->kobj, error_groups);
+	if (ret) {
+		device_unregister(dev);
+		return ret;
+	}
+
+	mutex_lock(&pdata->lock);
+	fme = fpga_pdata_get_private(pdata);
+	fme->dev_err = dev;
+	if (feature->ctx_num)
+		fme->capability |= FPGA_FME_CAP_ERR_IRQ;
+	mutex_unlock(&pdata->lock);
+
+	return ret;
+}
+
+static void global_error_uinit(struct platform_device *pdev,
+		struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+
+	mutex_lock(&pdata->lock);
+	fme = fpga_pdata_get_private(pdata);
+	sysfs_remove_groups(&fme->dev_err->kobj, error_groups);
+	device_unregister(fme->dev_err);
+	fme->dev_err = NULL;
+	mutex_unlock(&pdata->lock);
+}
+
+static long fme_err_set_irq(struct platform_device *pdev,
+			struct feature *feature, unsigned long arg)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme_err_irq_set hdr;
+	struct fpga_fme *fme;
+	unsigned long minsz;
+	long ret = 0;
+
+	minsz = offsetofend(struct fpga_fme_err_irq_set, evtfd);
+
+	if (copy_from_user(&hdr, (void __user *)arg, minsz))
+		return -EFAULT;
+
+	if (hdr.argsz < minsz || hdr.flags)
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	fme = fpga_pdata_get_private(pdata);
+	if (!(fme->capability & FPGA_FME_CAP_ERR_IRQ)) {
+		mutex_unlock(&pdata->lock);
+		return -ENODEV;
+	}
+	ret = fpga_msix_set_block(feature, 0, 1, &hdr.evtfd);
+	mutex_unlock(&pdata->lock);
+
+	return ret;
+}
+
+static long
+global_error_ioctl(struct platform_device *pdev, struct feature *feature,
+		   unsigned int cmd, unsigned long arg)
+{
+	long ret;
+
+	switch (cmd) {
+	case FPGA_FME_ERR_SET_IRQ:
+		ret = fme_err_set_irq(pdev, feature, arg);
+		break;
+	default:
+		dev_dbg(&pdev->dev, "%x cmd not handled", cmd);
+		ret = -ENODEV;
+	}
+
+	return ret;
+}
+
+struct feature_ops global_error_ops = {
+	.init = global_error_init,
+	.uinit = global_error_uinit,
+	.ioctl = global_error_ioctl,
+};
diff --git a/drivers/fpga/intel/fme-iperf.c b/drivers/fpga/intel/fme-iperf.c
new file mode 100644
index 000000000000..ec070beef7ba
--- /dev/null
+++ b/drivers/fpga/intel/fme-iperf.c
@@ -0,0 +1,771 @@
+/*
+ * Driver for FPGA Global Performance
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include "feature-dev.h"
+#include "fme.h"
+
+static ssize_t perf_obj_attr_show(struct kobject *kobj,
+				  struct attribute *__attr, char *buf)
+{
+	struct perf_obj_attributte *attr = to_perf_obj_attr(__attr);
+	struct perf_object *pobj = to_perf_obj(kobj);
+	ssize_t ret = -EIO;
+
+	if (attr->show)
+		ret = attr->show(pobj, buf);
+	return ret;
+}
+
+static ssize_t perf_obj_attr_store(struct kobject *kobj,
+				   struct attribute *__attr,
+				   const char *buf, size_t n)
+{
+	struct perf_obj_attributte *attr = to_perf_obj_attr(__attr);
+	struct perf_object *pobj = to_perf_obj(kobj);
+	ssize_t ret = -EIO;
+
+	if (attr->store)
+		ret = attr->store(pobj, buf, n);
+	return ret;
+}
+
+static const struct sysfs_ops perf_obj_sysfs_ops = {
+	.show = perf_obj_attr_show,
+	.store = perf_obj_attr_store,
+};
+
+static void perf_obj_release(struct kobject *kobj)
+{
+	kfree(to_perf_obj(kobj));
+}
+
+static struct kobj_type perf_obj_ktype = {
+	.sysfs_ops = &perf_obj_sysfs_ops,
+	.release = perf_obj_release,
+};
+
+static ssize_t revision_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_iperf *iperf;
+	struct feature_header header;
+
+	iperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_IPERF);
+	header.csr = readq(&iperf->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+static PERF_OBJ_ATTR_RO(revision);
+
+static ssize_t clock_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_iperf *iperf;
+	struct feature_fme_ifpmon_clk_ctr clk;
+
+	iperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_IPERF);
+	clk.afu_interf_clock = readq(&iperf->clk);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", clk.afu_interf_clock);
+}
+static PERF_OBJ_ATTR_RO(clock);
+
+static struct attribute *clock_attrs[] = {
+	&perf_obj_attr_revision.attr,
+	&perf_obj_attr_clock.attr,
+	NULL,
+};
+
+static struct attribute_group clock_attr_group = {
+	.attrs = clock_attrs,
+};
+
+static ssize_t freeze_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_iperf *iperf;
+	struct feature_fme_ifpmon_ch_ctl ctl;
+
+	iperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_IPERF);
+	ctl.csr = readq(&iperf->ch_ctl);
+	return scnprintf(buf, PAGE_SIZE, "%d\n", ctl.freeze);
+}
+
+static ssize_t freeze_store(struct perf_object *pobj, const char *buf, size_t n)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(pobj->fme_dev);
+	struct feature_fme_iperf *iperf;
+	struct feature_fme_ifpmon_ch_ctl ctl;
+	bool state;
+
+	if (strtobool(buf, &state))
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	iperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_IPERF);
+	ctl.csr = readq(&iperf->ch_ctl);
+	ctl.freeze = state;
+	writeq(ctl.csr, &iperf->ch_ctl);
+	mutex_unlock(&pdata->lock);
+
+	return n;
+}
+static PERF_OBJ_ATTR_RW(freeze);
+
+#define IPERF_TIMEOUT	30
+
+static ssize_t read_cache_counter(struct perf_object *pobj, char *buf,
+				  u8 channel, enum iperf_cache_events event)
+{
+	struct device *fme_dev = pobj->fme_dev;
+	struct feature_platform_data *pdata = dev_get_platdata(fme_dev);
+	struct feature_fme_iperf *iperf;
+	struct feature_fme_ifpmon_ch_ctl ctl;
+	struct feature_fme_ifpmon_ch_ctr ctr0, ctr1;
+	u64 counter;
+
+	mutex_lock(&pdata->lock);
+	iperf = get_feature_ioaddr_by_index(fme_dev,
+					    FME_FEATURE_ID_GLOBAL_IPERF);
+
+	/* set channel access type and cache event code. */
+	ctl.csr = readq(&iperf->ch_ctl);
+	ctl.cci_chsel = channel;
+	ctl.cache_event = event;
+	writeq(ctl.csr, &iperf->ch_ctl);
+
+	/* check the event type in the counter registers */
+	ctr0.event_code = event;
+
+	if (fpga_wait_register_field(event_code, ctr0,
+				     &iperf->ch_ctr0, IPERF_TIMEOUT, 1)) {
+		dev_err(fme_dev, "timeout, unmatched cache event type in counter registers.\n");
+		mutex_unlock(&pdata->lock);
+		return -ETIMEDOUT;
+	}
+
+	ctr0.csr = readq(&iperf->ch_ctr0);
+	ctr1.csr = readq(&iperf->ch_ctr1);
+	counter = ctr0.cache_counter + ctr1.cache_counter;
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", counter);
+}
+
+#define CACHE_SHOW(name, type, event)					\
+static ssize_t name##_show(struct perf_object *pobj, char *buf)		\
+{									\
+	return read_cache_counter(pobj, buf, type, event);		\
+}									\
+static PERF_OBJ_ATTR_RO(name)
+
+CACHE_SHOW(read_hit, CACHE_CHANNEL_RD, IPERF_CACHE_RD_HIT);
+CACHE_SHOW(read_miss, CACHE_CHANNEL_RD, IPERF_CACHE_RD_MISS);
+CACHE_SHOW(write_hit, CACHE_CHANNEL_WR, IPERF_CACHE_WR_HIT);
+CACHE_SHOW(write_miss, CACHE_CHANNEL_WR, IPERF_CACHE_WR_MISS);
+CACHE_SHOW(hold_request, CACHE_CHANNEL_RD, IPERF_CACHE_HOLD_REQ);
+CACHE_SHOW(tx_req_stall, CACHE_CHANNEL_RD, IPERF_CACHE_TX_REQ_STALL);
+CACHE_SHOW(rx_req_stall, CACHE_CHANNEL_RD, IPERF_CACHE_RX_REQ_STALL);
+CACHE_SHOW(rx_eviction, CACHE_CHANNEL_RD, IPERF_CACHE_EVICTIONS);
+CACHE_SHOW(data_write_port_contention, CACHE_CHANNEL_WR,
+	   IPERF_CACHE_DATA_WR_PORT_CONTEN);
+CACHE_SHOW(tag_write_port_contention, CACHE_CHANNEL_WR,
+	   IPERF_CACHE_TAG_WR_PORT_CONTEN);
+
+static struct attribute *cache_attrs[] = {
+	&perf_obj_attr_read_hit.attr,
+	&perf_obj_attr_read_miss.attr,
+	&perf_obj_attr_write_hit.attr,
+	&perf_obj_attr_write_miss.attr,
+	&perf_obj_attr_hold_request.attr,
+	&perf_obj_attr_data_write_port_contention.attr,
+	&perf_obj_attr_tag_write_port_contention.attr,
+	&perf_obj_attr_tx_req_stall.attr,
+	&perf_obj_attr_rx_req_stall.attr,
+	&perf_obj_attr_rx_eviction.attr,
+	&perf_obj_attr_freeze.attr,
+	NULL,
+};
+
+static struct attribute_group cache_attr_group = {
+	.name = "cache",
+	.attrs = cache_attrs,
+};
+
+static const struct attribute_group *perf_dev_attr_groups[] = {
+	&clock_attr_group,
+	&cache_attr_group,
+	NULL,
+};
+
+ssize_t vtd_freeze_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_ifpmon_vtd_ctl ctl;
+	struct feature_fme_iperf *iperf;
+
+	iperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_IPERF);
+	ctl.csr = readq(&iperf->vtd_ctl);
+	return scnprintf(buf, PAGE_SIZE, "%d\n", ctl.freeze);
+}
+
+ssize_t vtd_freeze_store(struct perf_object *pobj, const char *buf, size_t n)
+{
+	struct feature_platform_data *pdata;
+	struct feature_fme_ifpmon_vtd_ctl ctl;
+	struct feature_fme_iperf *iperf;
+	bool state;
+
+	if (strtobool(buf, &state))
+		return -EINVAL;
+
+	pdata = dev_get_platdata(pobj->fme_dev);
+	mutex_lock(&pdata->lock);
+	iperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_IPERF);
+	ctl.csr = readq(&iperf->vtd_ctl);
+	ctl.freeze = state;
+	writeq(ctl.csr, &iperf->vtd_ctl);
+	mutex_unlock(&pdata->lock);
+
+	return n;
+}
+
+static PERF_OBJ_ATTR(vtd_freeze, freeze, 0644, vtd_freeze_show,
+		     vtd_freeze_store);
+static struct attribute *iommu_top_attrs[] = {
+	&perf_obj_attr_vtd_freeze.attr,
+	NULL,
+};
+
+static struct attribute_group iommu_top_attr_group = {
+	.attrs = iommu_top_attrs,
+};
+
+static ssize_t read_iommu_sip_counter(struct perf_object *pobj,
+				      enum iperf_vtd_sip_events event,
+				      char *buf)
+{
+	struct feature_platform_data *pdata;
+	struct feature_fme_ifpmon_vtd_sip_ctl sip_ctl;
+	struct feature_fme_ifpmon_vtd_sip_ctr sip_ctr;
+	struct feature_fme_iperf *iperf;
+	u64 counter;
+
+	pdata = dev_get_platdata(pobj->fme_dev);
+	mutex_lock(&pdata->lock);
+	iperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_IPERF);
+	sip_ctl.csr = readq(&iperf->vtd_sip_ctl);
+	sip_ctl.vtd_evtcode = event;
+	writeq(sip_ctl.csr, &iperf->vtd_sip_ctl);
+
+	sip_ctr.event_code = event;
+
+	if (fpga_wait_register_field(event_code, sip_ctr,
+				     &iperf->vtd_sip_ctr, IPERF_TIMEOUT, 1)) {
+		dev_err(pobj->fme_dev, "timeout, unmatched VTd SIP event type in counter registers\n");
+		mutex_unlock(&pdata->lock);
+		return -ETIMEDOUT;
+	}
+
+	sip_ctr.csr = readq(&iperf->vtd_sip_ctr);
+	counter = sip_ctr.vtd_counter;
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", counter);
+}
+
+#define VTD_SIP_SHOW(name, event)					\
+static ssize_t name##_show(struct perf_object *pobj, char *buf)		\
+{									\
+	return read_iommu_sip_counter(pobj, event, buf);		\
+}									\
+static PERF_OBJ_ATTR_RO(name)
+
+VTD_SIP_SHOW(iotlb_4k_hit, IPERF_VTD_SIP_IOTLB_4K_HIT);
+VTD_SIP_SHOW(iotlb_2m_hit, IPERF_VTD_SIP_IOTLB_2M_HIT);
+VTD_SIP_SHOW(iotlb_1g_hit, IPERF_VTD_SIP_IOTLB_1G_HIT);
+VTD_SIP_SHOW(slpwc_l3_hit, IPERF_VTD_SIP_SLPWC_L3_HIT);
+VTD_SIP_SHOW(slpwc_l4_hit, IPERF_VTD_SIP_SLPWC_L4_HIT);
+VTD_SIP_SHOW(rcc_hit, IPERF_VTD_SIP_RCC_HIT);
+VTD_SIP_SHOW(iotlb_4k_miss, IPERF_VTD_SIP_IOTLB_4K_MISS);
+VTD_SIP_SHOW(iotlb_2m_miss, IPERF_VTD_SIP_IOTLB_2M_MISS);
+VTD_SIP_SHOW(iotlb_1g_miss, IPERF_VTD_SIP_IOTLB_1G_MISS);
+VTD_SIP_SHOW(slpwc_l3_miss, IPERF_VTD_SIP_SLPWC_L3_MISS);
+VTD_SIP_SHOW(slpwc_l4_miss, IPERF_VTD_SIP_SLPWC_L4_MISS);
+VTD_SIP_SHOW(rcc_miss, IPERF_VTD_SIP_RCC_MISS);
+
+static struct attribute *iommu_sip_attrs[] = {
+	&perf_obj_attr_iotlb_4k_hit.attr,
+	&perf_obj_attr_iotlb_2m_hit.attr,
+	&perf_obj_attr_iotlb_1g_hit.attr,
+	&perf_obj_attr_slpwc_l3_hit.attr,
+	&perf_obj_attr_slpwc_l4_hit.attr,
+	&perf_obj_attr_rcc_hit.attr,
+	&perf_obj_attr_iotlb_4k_miss.attr,
+	&perf_obj_attr_iotlb_2m_miss.attr,
+	&perf_obj_attr_iotlb_1g_miss.attr,
+	&perf_obj_attr_slpwc_l3_miss.attr,
+	&perf_obj_attr_slpwc_l4_miss.attr,
+	&perf_obj_attr_rcc_miss.attr,
+	NULL,
+};
+
+static struct attribute_group iommu_sip_attr_group = {
+	.attrs = iommu_sip_attrs,
+};
+
+static const struct attribute_group *iommu_top_attr_groups[] = {
+	&iommu_top_attr_group,
+	&iommu_sip_attr_group,
+	NULL,
+};
+
+static ssize_t read_iommu_counter(struct perf_object *pobj,
+				  enum iperf_vtd_events base_event, char *buf)
+{
+	struct feature_platform_data *pdata;
+	struct feature_fme_ifpmon_vtd_ctl ctl;
+	struct feature_fme_ifpmon_vtd_ctr ctr;
+	struct feature_fme_iperf *iperf;
+	enum iperf_vtd_events event = base_event + pobj->id;
+	u64 counter;
+
+	pdata = dev_get_platdata(pobj->fme_dev);
+	mutex_lock(&pdata->lock);
+	iperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_IPERF);
+	ctl.csr = readq(&iperf->vtd_ctl);
+	ctl.vtd_evtcode = event;
+	writeq(ctl.csr, &iperf->vtd_ctl);
+
+	ctr.event_code = event;
+
+	if (fpga_wait_register_field(event_code, ctr,
+				     &iperf->vtd_ctr, IPERF_TIMEOUT, 1)) {
+		dev_err(pobj->fme_dev, "timeout, unmatched VTd event type in counter registers.\n");
+		mutex_unlock(&pdata->lock);
+		return -ETIMEDOUT;
+	}
+
+	ctr.csr = readq(&iperf->vtd_ctr);
+	counter = ctr.vtd_counter;
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", counter);
+}
+
+#define VTD_SHOW(name, base_event)					\
+static ssize_t name##_show(struct perf_object *pobj, char *buf)		\
+{									\
+	return read_iommu_counter(pobj, base_event, buf);		\
+}									\
+static PERF_OBJ_ATTR_RO(name)
+
+VTD_SHOW(read_transaction, IPERF_VTD_AFU_MEM_RD_TRANS);
+VTD_SHOW(write_transaction, IPERF_VTD_AFU_MEM_WR_TRANS);
+VTD_SHOW(devtlb_read_hit, IPERF_VTD_AFU_DEVTLB_RD_HIT);
+VTD_SHOW(devtlb_write_hit, IPERF_VTD_AFU_DEVTLB_WR_HIT);
+VTD_SHOW(devtlb_4k_fill, IPERF_VTD_DEVTLB_4K_FILL);
+VTD_SHOW(devtlb_2m_fill, IPERF_VTD_DEVTLB_2M_FILL);
+VTD_SHOW(devtlb_1g_fill, IPERF_VTD_DEVTLB_1G_FILL);
+
+static struct attribute *iommu_attrs[] = {
+	&perf_obj_attr_read_transaction.attr,
+	&perf_obj_attr_write_transaction.attr,
+	&perf_obj_attr_devtlb_read_hit.attr,
+	&perf_obj_attr_devtlb_write_hit.attr,
+	&perf_obj_attr_devtlb_4k_fill.attr,
+	&perf_obj_attr_devtlb_2m_fill.attr,
+	&perf_obj_attr_devtlb_1g_fill.attr,
+	NULL,
+};
+
+static struct attribute_group iommu_attr_group = {
+	.attrs = iommu_attrs,
+};
+
+static const struct attribute_group *iommu_attr_groups[] = {
+	&iommu_attr_group,
+	NULL,
+};
+
+static bool fabric_pobj_is_enabled(struct perf_object *pobj,
+				   struct feature_fme_iperf *iperf)
+{
+	struct feature_fme_ifpmon_fab_ctl ctl;
+
+	ctl.csr = readq(&iperf->fab_ctl);
+
+	if (ctl.port_filter == FAB_DISABLE_FILTER)
+		return pobj->id == PERF_OBJ_ROOT_ID;
+
+	return pobj->id == ctl.port_id;
+}
+
+static ssize_t read_fabric_counter(struct perf_object *pobj,
+				   enum iperf_fab_events fab_event, char *buf)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(pobj->fme_dev);
+	struct feature_fme_ifpmon_fab_ctl ctl;
+	struct feature_fme_ifpmon_fab_ctr ctr;
+	struct feature_fme_iperf *iperf;
+	u64 counter = 0;
+
+	mutex_lock(&pdata->lock);
+	iperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_IPERF);
+
+	/* if it is disabled, force the counter to return zero. */
+	if (!fabric_pobj_is_enabled(pobj, iperf))
+		goto exit;
+
+	ctl.csr = readq(&iperf->fab_ctl);
+	ctl.fab_evtcode = fab_event;
+	writeq(ctl.csr, &iperf->fab_ctl);
+
+	ctr.event_code = fab_event;
+
+	if (fpga_wait_register_field(event_code, ctr,
+				     &iperf->fab_ctr, IPERF_TIMEOUT, 1)) {
+		dev_err(pobj->fme_dev, "timeout, unmatched VTd event type in counter registers.\n");
+		mutex_unlock(&pdata->lock);
+		return -ETIMEDOUT;
+	}
+
+	ctr.csr = readq(&iperf->fab_ctr);
+	counter = ctr.fab_cnt;
+exit:
+	mutex_unlock(&pdata->lock);
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", counter);
+}
+
+#define FAB_SHOW(name, event)						\
+static ssize_t name##_show(struct perf_object *pobj, char *buf)		\
+{									\
+	return read_fabric_counter(pobj, event, buf);			\
+}									\
+static PERF_OBJ_ATTR_RO(name)
+
+FAB_SHOW(pcie0_read, IPERF_FAB_PCIE0_RD);
+FAB_SHOW(pcie0_write, IPERF_FAB_PCIE0_WR);
+FAB_SHOW(pcie1_read, IPERF_FAB_PCIE1_RD);
+FAB_SHOW(pcie1_write, IPERF_FAB_PCIE1_WR);
+FAB_SHOW(upi_read, IPERF_FAB_UPI_RD);
+FAB_SHOW(upi_write, IPERF_FAB_UPI_WR);
+FAB_SHOW(mmio_read, IPERF_FAB_MMIO_RD);
+FAB_SHOW(mmio_write, IPERF_FAB_MMIO_WR);
+
+static ssize_t fab_enable_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_iperf *iperf;
+	int status;
+
+	iperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_IPERF);
+
+	status = fabric_pobj_is_enabled(pobj, iperf);
+	return scnprintf(buf, PAGE_SIZE, "%d\n", status);
+}
+
+/*
+ * If enable one port or all port event counter in fabric, other
+ * fabric event counter originally enabled will be disable automatically.
+ */
+static ssize_t fab_enable_store(struct perf_object *pobj,
+				const char *buf, size_t n)
+{
+	struct feature_platform_data *pdata  = dev_get_platdata(pobj->fme_dev);
+	struct feature_fme_ifpmon_fab_ctl ctl;
+	struct feature_fme_iperf *iperf;
+	bool state;
+
+	if (strtobool(buf, &state))
+		return -EINVAL;
+
+	if (!state)
+		return -EINVAL;
+
+	iperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_IPERF);
+
+	/* if it is already enabled. */
+	if (fabric_pobj_is_enabled(pobj, iperf))
+		return n;
+
+	mutex_lock(&pdata->lock);
+	ctl.csr = readq(&iperf->fab_ctl);
+	if (pobj->id == PERF_OBJ_ROOT_ID)
+		ctl.port_filter = FAB_DISABLE_FILTER;
+	else {
+		ctl.port_filter = FAB_ENABLE_FILTER;
+		ctl.port_id = pobj->id;
+	}
+
+	writeq(ctl.csr, &iperf->fab_ctl);
+	mutex_unlock(&pdata->lock);
+
+	return n;
+}
+
+static PERF_OBJ_ATTR(fab_enable, enable, 0644, fab_enable_show,
+		     fab_enable_store);
+
+static struct attribute *fabric_attrs[] = {
+	&perf_obj_attr_pcie0_read.attr,
+	&perf_obj_attr_pcie0_write.attr,
+	&perf_obj_attr_pcie1_read.attr,
+	&perf_obj_attr_pcie1_write.attr,
+	&perf_obj_attr_upi_read.attr,
+	&perf_obj_attr_upi_write.attr,
+	&perf_obj_attr_mmio_read.attr,
+	&perf_obj_attr_mmio_write.attr,
+	&perf_obj_attr_fab_enable.attr,
+	NULL,
+};
+
+static struct attribute_group fabric_attr_group = {
+	.attrs = fabric_attrs,
+};
+
+static const struct attribute_group *fabric_attr_groups[] = {
+	&fabric_attr_group,
+	NULL,
+};
+
+static ssize_t fab_freeze_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_iperf *iperf;
+	struct feature_fme_ifpmon_fab_ctl ctl;
+
+	iperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_IPERF);
+	ctl.csr = readq(&iperf->fab_ctl);
+	return scnprintf(buf, PAGE_SIZE, "%d\n", ctl.freeze);
+}
+
+static ssize_t fab_freeze_store(struct perf_object *pobj,
+				const char *buf, size_t n)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(pobj->fme_dev);
+	struct feature_fme_iperf *iperf;
+	struct feature_fme_ifpmon_fab_ctl ctl;
+	bool state;
+
+	if (strtobool(buf, &state))
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	iperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_IPERF);
+	ctl.csr = readq(&iperf->fab_ctl);
+	ctl.freeze = state;
+	writeq(ctl.csr, &iperf->fab_ctl);
+	mutex_unlock(&pdata->lock);
+
+	return n;
+}
+
+static PERF_OBJ_ATTR(fab_freeze, freeze, 0644, fab_freeze_show,
+		     fab_freeze_store);
+
+static struct attribute *fabric_top_attrs[] = {
+	&perf_obj_attr_fab_freeze.attr,
+	NULL,
+};
+
+static struct attribute_group fabric_top_attr_group = {
+	.attrs = fabric_top_attrs,
+};
+
+static const struct attribute_group *fabric_top_attr_groups[] = {
+	&fabric_attr_group,
+	&fabric_top_attr_group,
+	NULL,
+};
+
+static struct perf_object *
+create_perf_obj(struct device *fme_dev, struct kobject *parent, int id,
+		const struct attribute_group **groups, const char *name)
+{
+	struct perf_object *pobj;
+	int ret;
+
+	pobj = kzalloc(sizeof(*pobj), GFP_KERNEL);
+	if (!pobj)
+		return ERR_PTR(-ENOMEM);
+
+	pobj->id = id;
+	pobj->fme_dev = fme_dev;
+	pobj->attr_groups = groups;
+	INIT_LIST_HEAD(&pobj->node);
+	INIT_LIST_HEAD(&pobj->children);
+
+	if (id != PERF_OBJ_ROOT_ID)
+		ret = kobject_init_and_add(&pobj->kobj, &perf_obj_ktype,
+					   parent, "%s%d", name, id);
+	else
+		ret = kobject_init_and_add(&pobj->kobj, &perf_obj_ktype,
+					   parent, "%s", name);
+	if (ret)
+		goto put_exit;
+
+	if (pobj->attr_groups) {
+		ret = sysfs_create_groups(&pobj->kobj, pobj->attr_groups);
+		if (ret)
+			goto put_exit;
+	}
+
+	return pobj;
+
+put_exit:
+	kobject_put(&pobj->kobj);
+	return ERR_PTR(ret);
+}
+
+static void destroy_perf_obj(struct perf_object *pobj)
+{
+	struct perf_object *obj, *obj_tmp;
+
+	list_for_each_entry_safe(obj, obj_tmp, &pobj->children, node)
+		destroy_perf_obj(obj);
+
+	list_del(&pobj->node);
+	if (pobj->attr_groups)
+		sysfs_remove_groups(&pobj->kobj, pobj->attr_groups);
+	kobject_put(&pobj->kobj);
+}
+
+#define PERF_MAX_PORT_NUM	1
+
+static int create_perf_iommu_obj(struct perf_object *perf_dev)
+{
+	struct perf_object *pobj;
+	struct feature_fme_header *fme_hdr;
+	struct feature_fme_capability fme_capability;
+	int i;
+
+	fme_hdr = get_feature_ioaddr_by_index(perf_dev->fme_dev,
+					      FME_FEATURE_ID_HEADER);
+
+	/* check if iommu is not supported on this device. */
+	fme_capability.csr = readq(&fme_hdr->capability);
+	if (!fme_capability.iommu_support)
+		return 0;
+
+	pobj = create_perf_obj(perf_dev->fme_dev, &perf_dev->kobj,
+			       PERF_OBJ_ROOT_ID, iommu_top_attr_groups,
+			       "iommu");
+	if (IS_ERR(pobj))
+		return PTR_ERR(pobj);
+
+	list_add(&pobj->node, &perf_dev->children);
+
+	for (i = 0; i < PERF_MAX_PORT_NUM; i++) {
+		struct perf_object *obj;
+
+		obj = create_perf_obj(perf_dev->fme_dev, &pobj->kobj, i,
+				      iommu_attr_groups, "afu");
+		if (IS_ERR(obj))
+			return PTR_ERR(obj);
+
+		list_add(&obj->node, &pobj->children);
+	}
+
+	return 0;
+}
+
+static int create_perf_fabric_obj(struct perf_object *perf_dev)
+{
+	struct perf_object *pobj;
+	int i;
+
+	pobj = create_perf_obj(perf_dev->fme_dev, &perf_dev->kobj,
+			       PERF_OBJ_ROOT_ID, fabric_top_attr_groups,
+			       "fabric");
+	if (IS_ERR(pobj))
+		return PTR_ERR(pobj);
+
+	list_add(&pobj->node, &perf_dev->children);
+
+	for (i = 0; i < PERF_MAX_PORT_NUM; i++) {
+		struct perf_object *obj;
+
+		obj = create_perf_obj(perf_dev->fme_dev, &pobj->kobj, i,
+				      fabric_attr_groups, "port");
+		if (IS_ERR(obj))
+			return PTR_ERR(obj);
+
+		list_add(&obj->node, &pobj->children);
+	}
+
+	return 0;
+}
+
+static struct perf_object *create_perf_dev(struct platform_device *pdev)
+{
+	return create_perf_obj(&pdev->dev, &pdev->dev.kobj,
+			   PERF_OBJ_ROOT_ID, perf_dev_attr_groups, "iperf");
+}
+
+static int fme_iperf_init(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+	struct perf_object *perf_dev;
+	int ret;
+
+	perf_dev = create_perf_dev(pdev);
+	if (IS_ERR(perf_dev))
+		return PTR_ERR(perf_dev);
+
+	ret = create_perf_iommu_obj(perf_dev);
+	if (ret) {
+		destroy_perf_obj(perf_dev);
+		return ret;
+	}
+
+	ret = create_perf_fabric_obj(perf_dev);
+	if (ret) {
+		destroy_perf_obj(perf_dev);
+		return ret;
+	}
+
+	fme = fpga_pdata_get_private(pdata);
+	fme->iperf_dev = perf_dev;
+	return 0;
+}
+
+static void
+fme_iperf_uinit(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+
+	fme = fpga_pdata_get_private(pdata);
+	destroy_perf_obj(fme->iperf_dev);
+	fme->iperf_dev = NULL;
+}
+
+struct feature_ops global_iperf_ops = {
+	.init = fme_iperf_init,
+	.uinit = fme_iperf_uinit,
+};
diff --git a/drivers/fpga/intel/fme-main.c b/drivers/fpga/intel/fme-main.c
new file mode 100644
index 000000000000..d329d70e1bac
--- /dev/null
+++ b/drivers/fpga/intel/fme-main.c
@@ -0,0 +1,1124 @@
+/*
+ * Driver for FPGA Management Engine which implements all FPGA platform
+ * level management features.
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/stddef.h>
+#include <linux/errno.h>
+#include <linux/delay.h>
+#include <linux/uaccess.h>
+#include <linux/intel-fpga.h>
+
+#include <linux/fpga/fpga-mgr.h>
+
+#include "altera-asmip2.h"
+#include "feature-dev.h"
+#include "fme.h"
+
+#define PWR_THRESHOLD_MAX       0x7F
+
+#define FME_DEV_ATTR(_name, _filename, _mode, _show, _store)	\
+struct device_attribute dev_attr_##_name =			\
+	__ATTR(_filename, _mode, _show, _store)
+
+static ssize_t revision_show(struct device *dev, struct device_attribute *attr,
+			     char *buf)
+{
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	struct feature_header header;
+
+	header.csr = readq(&fme_hdr->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+
+static DEVICE_ATTR_RO(revision);
+
+static ssize_t ports_num_show(struct device *dev, struct device_attribute *attr,
+			char *buf)
+{
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	struct feature_fme_capability fme_capability;
+
+	fme_capability.csr = readq(&fme_hdr->capability);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", fme_capability.num_ports);
+}
+
+static DEVICE_ATTR_RO(ports_num);
+
+static ssize_t cache_size_show(struct device *dev,
+			struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	struct feature_fme_capability fme_capability;
+
+	fme_capability.csr = readq(&fme_hdr->capability);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", fme_capability.cache_size);
+}
+
+static DEVICE_ATTR_RO(cache_size);
+
+static ssize_t version_show(struct device *dev, struct device_attribute *attr,
+			char *buf)
+{
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	struct feature_fme_capability fme_capability;
+
+	fme_capability.csr = readq(&fme_hdr->capability);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", fme_capability.fabric_verid);
+}
+
+static DEVICE_ATTR_RO(version);
+
+static ssize_t socket_id_show(struct device *dev,
+			struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	struct feature_fme_capability fme_capability;
+
+	fme_capability.csr = readq(&fme_hdr->capability);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", fme_capability.socket_id);
+}
+
+static DEVICE_ATTR_RO(socket_id);
+
+static ssize_t bitstream_id_show(struct device *dev,
+			struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	u64 bitstream_id = readq(&fme_hdr->bitstream_id);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", bitstream_id);
+}
+
+static DEVICE_ATTR_RO(bitstream_id);
+
+static ssize_t bitstream_metadata_show(struct device *dev,
+			struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	u64 bitstream_md = readq(&fme_hdr->bitstream_md);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", bitstream_md);
+}
+
+static DEVICE_ATTR_RO(bitstream_metadata);
+
+static const struct attribute *fme_hdr_attrs[] = {
+	&dev_attr_revision.attr,
+	&dev_attr_ports_num.attr,
+	&dev_attr_cache_size.attr,
+	&dev_attr_version.attr,
+	&dev_attr_socket_id.attr,
+	&dev_attr_bitstream_id.attr,
+	&dev_attr_bitstream_metadata.attr,
+	NULL,
+};
+
+static int fme_hdr_init(struct platform_device *pdev, struct feature *feature)
+{
+	int ret;
+	struct feature_fme_header *fme_hdr = feature->ioaddr;
+
+	dev_dbg(&pdev->dev, "FME HDR Init.\n");
+	dev_dbg(&pdev->dev, "FME cap %llx.\n", fme_hdr->capability.csr);
+
+	ret = sysfs_create_files(&pdev->dev.kobj, fme_hdr_attrs);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static void fme_hdr_uinit(struct platform_device *pdev, struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "FME HDR UInit.\n");
+	sysfs_remove_files(&pdev->dev.kobj, fme_hdr_attrs);
+}
+
+struct feature_ops fme_hdr_ops = {
+	.init = fme_hdr_init,
+	.uinit = fme_hdr_uinit,
+};
+
+static ssize_t thermal_revision_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_header header;
+
+	header.csr = readq(&fme_thermal->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+
+static FME_DEV_ATTR(thermal_revision, revision, 0444,
+		    thermal_revision_show, NULL);
+
+static ssize_t thermal_threshold1_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_tmp_threshold tmp_threshold;
+
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", tmp_threshold.tmp_thshold1);
+}
+
+static ssize_t thermal_threshold1_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	struct feature_fme_tmp_threshold tmp_threshold;
+	struct feature_fme_capability fme_capability;
+	int err;
+	u8 tmp_threshold1;
+
+	mutex_lock(&pdata->lock);
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	err = kstrtou8(buf, 0, &tmp_threshold1);
+	if (err) {
+		mutex_unlock(&pdata->lock);
+		return err;
+	}
+
+	fme_capability.csr = readq(&fme_hdr->capability);
+
+	if (fme_capability.lock_bit == 1) {
+		mutex_unlock(&pdata->lock);
+		return -EBUSY;
+	} else if (tmp_threshold1 > 100) {
+		mutex_unlock(&pdata->lock);
+		return -EINVAL;
+	} else if (tmp_threshold1 == 0) {
+		tmp_threshold.tmp_thshold1_enable = 0;
+		tmp_threshold.tmp_thshold1 = tmp_threshold1;
+	} else {
+		tmp_threshold.tmp_thshold1_enable = 1;
+		tmp_threshold.tmp_thshold1 = tmp_threshold1;
+	}
+
+	writeq(tmp_threshold.csr, &fme_thermal->threshold);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static DEVICE_ATTR(threshold1, 0644,
+	thermal_threshold1_show, thermal_threshold1_store);
+
+static ssize_t thermal_threshold2_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_tmp_threshold tmp_threshold;
+
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", tmp_threshold.tmp_thshold2);
+}
+
+static ssize_t thermal_threshold2_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	struct feature_fme_tmp_threshold tmp_threshold;
+	struct feature_fme_capability fme_capability;
+	int err;
+	u8 tmp_threshold2;
+
+	mutex_lock(&pdata->lock);
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	err = kstrtou8(buf, 0, &tmp_threshold2);
+	if (err) {
+		mutex_unlock(&pdata->lock);
+		return err;
+	}
+
+	fme_capability.csr = readq(&fme_hdr->capability);
+
+	if (fme_capability.lock_bit == 1) {
+		mutex_unlock(&pdata->lock);
+		return -EBUSY;
+	} else if (tmp_threshold2 > 100) {
+		mutex_unlock(&pdata->lock);
+		return -EINVAL;
+	} else if (tmp_threshold2 == 0) {
+		tmp_threshold.tmp_thshold2_enable = 0;
+		tmp_threshold.tmp_thshold2 = tmp_threshold2;
+	} else {
+		tmp_threshold.tmp_thshold2_enable = 1;
+		tmp_threshold.tmp_thshold2 = tmp_threshold2;
+	}
+
+	writeq(tmp_threshold.csr, &fme_thermal->threshold);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static DEVICE_ATTR(threshold2, 0644,
+	thermal_threshold2_show, thermal_threshold2_store);
+
+static ssize_t thermal_threshold_trip_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_tmp_threshold tmp_threshold;
+
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+				tmp_threshold.therm_trip_thshold);
+}
+
+static DEVICE_ATTR(threshold_trip, 0444, thermal_threshold_trip_show, NULL);
+
+static ssize_t thermal_threshold1_reached_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_tmp_threshold tmp_threshold;
+
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+				tmp_threshold.thshold1_status);
+}
+
+static DEVICE_ATTR(threshold1_reached, 0444,
+	thermal_threshold1_reached_show, NULL);
+
+static ssize_t thermal_threshold2_reached_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_tmp_threshold tmp_threshold;
+
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+				tmp_threshold.thshold2_status);
+}
+
+static DEVICE_ATTR(threshold2_reached, 0444,
+	thermal_threshold2_reached_show, NULL);
+
+static ssize_t thermal_threshold1_policy_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_tmp_threshold tmp_threshold;
+
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+				tmp_threshold.thshold_policy);
+}
+
+static ssize_t thermal_threshold1_policy_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_tmp_threshold tmp_threshold;
+	int err;
+	u8 thshold_policy;
+
+	mutex_lock(&pdata->lock);
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	err = kstrtou8(buf, 0, &thshold_policy);
+	if (err) {
+		mutex_unlock(&pdata->lock);
+		return err;
+	}
+
+	if (thshold_policy == 0)
+		tmp_threshold.thshold_policy = 0;
+	else if (thshold_policy == 1)
+		tmp_threshold.thshold_policy = 1;
+	else {
+		mutex_unlock(&pdata->lock);
+		return -EINVAL;
+	}
+
+	writeq(tmp_threshold.csr, &fme_thermal->threshold);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static DEVICE_ATTR(threshold1_policy, 0644,
+	thermal_threshold1_policy_show, thermal_threshold1_policy_store);
+
+static ssize_t thermal_temperature_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_temp_rdsensor_fmt1 temp_rdsensor_fmt1;
+
+	temp_rdsensor_fmt1.csr = readq(&fme_thermal->rdsensor_fm1);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+				temp_rdsensor_fmt1.fpga_temp);
+}
+
+static DEVICE_ATTR(temperature, 0444, thermal_temperature_show, NULL);
+
+static struct attribute *thermal_mgmt_attrs[] = {
+	&dev_attr_thermal_revision.attr,
+	&dev_attr_threshold1.attr,
+	&dev_attr_threshold2.attr,
+	&dev_attr_threshold_trip.attr,
+	&dev_attr_threshold1_reached.attr,
+	&dev_attr_threshold2_reached.attr,
+	&dev_attr_threshold1_policy.attr,
+	&dev_attr_temperature.attr,
+	NULL,
+};
+
+static struct attribute_group thermal_mgmt_attr_group = {
+	.attrs	= thermal_mgmt_attrs,
+	.name	= "thermal_mgmt",
+};
+
+static int thermal_mgmt_init(struct platform_device *pdev,
+				struct feature *feature)
+{
+	int ret;
+
+	ret = sysfs_create_group(&pdev->dev.kobj, &thermal_mgmt_attr_group);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static void thermal_mgmt_uinit(struct platform_device *pdev,
+				struct feature *feature)
+{
+	sysfs_remove_group(&pdev->dev.kobj, &thermal_mgmt_attr_group);
+}
+
+struct feature_ops thermal_mgmt_ops = {
+	.init = thermal_mgmt_init,
+	.uinit = thermal_mgmt_uinit,
+};
+
+static ssize_t pwr_revision_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_header header;
+
+	header.csr = readq(&fme_power->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+
+static FME_DEV_ATTR(pwr_revision, revision, 0444, pwr_revision_show, NULL);
+
+static ssize_t consumed_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_status pm_status;
+
+	pm_status.csr = readq(&fme_power->status);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%x\n", pm_status.pwr_consumed);
+}
+
+static DEVICE_ATTR_RO(consumed);
+
+static ssize_t pwr_threshold1_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_ap_threshold pm_ap_threshold;
+
+	pm_ap_threshold.csr = readq(&fme_power->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%x\n", pm_ap_threshold.threshold1);
+}
+
+static ssize_t pwr_threshold1_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_ap_threshold pm_ap_threshold;
+	u8 threshold;
+	int err;
+
+	mutex_lock(&pdata->lock);
+	pm_ap_threshold.csr = readq(&fme_power->threshold);
+
+	err = kstrtou8(buf, 0, &threshold);
+	if (err) {
+		mutex_unlock(&pdata->lock);
+		return err;
+	}
+
+	if (threshold <= PWR_THRESHOLD_MAX)
+		pm_ap_threshold.threshold1 = threshold;
+	else {
+		mutex_unlock(&pdata->lock);
+		return -EINVAL;
+	}
+
+	writeq(pm_ap_threshold.csr, &fme_power->threshold);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static FME_DEV_ATTR(pwr_threshold1, threshold1, 0644, pwr_threshold1_show,
+		    pwr_threshold1_store);
+
+static ssize_t pwr_threshold2_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_ap_threshold pm_ap_threshold;
+
+	pm_ap_threshold.csr = readq(&fme_power->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%x\n",
+				pm_ap_threshold.threshold2);
+}
+
+static ssize_t pwr_threshold2_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_ap_threshold pm_ap_threshold;
+	u8 threshold;
+	int err;
+
+	mutex_lock(&pdata->lock);
+	pm_ap_threshold.csr = readq(&fme_power->threshold);
+
+	err = kstrtou8(buf, 0, &threshold);
+	if (err) {
+		mutex_unlock(&pdata->lock);
+		return err;
+	}
+
+	if (threshold <= PWR_THRESHOLD_MAX)
+		pm_ap_threshold.threshold2 = threshold;
+	else {
+		mutex_unlock(&pdata->lock);
+		return -EINVAL;
+	}
+
+	writeq(pm_ap_threshold.csr, &fme_power->threshold);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static FME_DEV_ATTR(pwr_threshold2, threshold2, 0644, pwr_threshold2_show,
+		    pwr_threshold2_store);
+
+static ssize_t threshold1_status_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_ap_threshold pm_ap_threshold;
+
+	pm_ap_threshold.csr = readq(&fme_power->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%u\n",
+				pm_ap_threshold.threshold1_status);
+}
+
+static DEVICE_ATTR_RO(threshold1_status);
+
+static ssize_t threshold2_status_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_ap_threshold pm_ap_threshold;
+
+	pm_ap_threshold.csr = readq(&fme_power->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%u\n",
+				pm_ap_threshold.threshold2_status);
+}
+static DEVICE_ATTR_RO(threshold2_status);
+
+static ssize_t rtl_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_status pm_status;
+
+	pm_status.csr = readq(&fme_power->status);
+
+	return scnprintf(buf, PAGE_SIZE, "%u\n",
+				pm_status.fpga_latency_report);
+}
+
+static DEVICE_ATTR_RO(rtl);
+
+static ssize_t xeon_limit_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_xeon_limit xeon_limit;
+
+	xeon_limit.csr = readq(&fme_power->xeon_limit);
+
+	if (!xeon_limit.enable)
+		xeon_limit.pwr_limit = 0;
+
+	return scnprintf(buf, PAGE_SIZE, "%u\n", xeon_limit.pwr_limit);
+}
+static DEVICE_ATTR_RO(xeon_limit);
+
+static ssize_t fpga_limit_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_fpga_limit fpga_limit;
+
+	fpga_limit.csr = readq(&fme_power->fpga_limit);
+
+	if (!fpga_limit.enable)
+		fpga_limit.pwr_limit = 0;
+
+	return scnprintf(buf, PAGE_SIZE, "%u\n", fpga_limit.pwr_limit);
+}
+static DEVICE_ATTR_RO(fpga_limit);
+
+static struct attribute *power_mgmt_attrs[] = {
+	&dev_attr_pwr_revision.attr,
+	&dev_attr_consumed.attr,
+	&dev_attr_pwr_threshold1.attr,
+	&dev_attr_pwr_threshold2.attr,
+	&dev_attr_threshold1_status.attr,
+	&dev_attr_threshold2_status.attr,
+	&dev_attr_xeon_limit.attr,
+	&dev_attr_fpga_limit.attr,
+	&dev_attr_rtl.attr,
+	NULL,
+};
+
+static struct attribute_group power_mgmt_attr_group = {
+	.attrs	= power_mgmt_attrs,
+	.name	= "power_mgmt",
+};
+
+static int power_mgmt_init(struct platform_device *pdev,
+				struct feature *feature)
+{
+	int ret;
+
+	ret = sysfs_create_group(&pdev->dev.kobj, &power_mgmt_attr_group);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static void power_mgmt_uinit(struct platform_device *pdev,
+				struct feature *feature)
+{
+	sysfs_remove_group(&pdev->dev.kobj, &power_mgmt_attr_group);
+}
+
+struct feature_ops power_mgmt_ops = {
+	.init = power_mgmt_init,
+	.uinit = power_mgmt_uinit,
+};
+
+static int hssi_mgmt_init(struct platform_device *pdev, struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "FME HSSI Init.\n");
+	return 0;
+}
+
+static void hssi_mgmt_uinit(struct platform_device *pdev,
+				struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "FME HSSI UInit.\n");
+}
+
+struct feature_ops hssi_mgmt_ops = {
+	.init = hssi_mgmt_init,
+	.uinit = hssi_mgmt_uinit,
+};
+
+#define FLASH_CAPABILITY_OFT 8
+
+static int qspi_flash_init(struct platform_device *pdev,
+			   struct feature *feature)
+{
+	u64 reg;
+	struct altera_asmip2_plat_data qdata;
+	struct platform_device *cdev;
+	int ret = 0;
+
+	reg = readq(feature->ioaddr + FLASH_CAPABILITY_OFT);
+	dev_info(&pdev->dev, "%s %s %d 0x%llx 0x%x 0x%x\n",
+		 __func__, ALTERA_ASMIP2_DRV_NAME, feature->resource_index,
+		 reg, readl(feature->ioaddr + FLASH_CAPABILITY_OFT),
+		 readl(feature->ioaddr + FLASH_CAPABILITY_OFT + 4));
+
+	cdev = platform_device_alloc(ALTERA_ASMIP2_DRV_NAME,
+				     PLATFORM_DEVID_AUTO);
+
+	if (!cdev) {
+		dev_err(&pdev->dev, "platform_device_alloc failed in %s\n",
+			__func__);
+		return -ENOMEM;
+	}
+
+	cdev->dev.parent = &pdev->dev;
+
+	memset(&qdata, 0, sizeof(qdata));
+	qdata.csr_base = feature->ioaddr + FLASH_CAPABILITY_OFT;
+	qdata.num_chip_sel = 1;
+
+	ret = platform_device_add_data(cdev, &qdata, sizeof(qdata));
+	if (ret) {
+		dev_err(&pdev->dev, "platform_device_add_data in %s\n",
+			__func__);
+		goto error;
+	}
+
+	ret = platform_device_add(cdev);
+	if (ret) {
+		dev_err(&pdev->dev, "platform_device_add failed with %d\n",
+			ret);
+		goto error;
+	}
+
+	return ret;
+
+error:
+	platform_device_put(cdev);
+	return ret;
+}
+
+struct feature_platform_search {
+	const char *drv_name;
+	int name_len;
+	struct feature *feature;
+};
+
+static int qspi_match(struct device *dev, void *data)
+{
+	struct feature_platform_search *src =
+		(struct feature_platform_search *)data;
+	struct altera_asmip2_plat_data *qdata;
+
+	if (strncmp(dev_name(dev), src->drv_name, src->name_len))
+		return 0;
+
+	qdata = dev_get_platdata(dev);
+
+	if (qdata &&
+	    (qdata->csr_base == (src->feature->ioaddr + FLASH_CAPABILITY_OFT)))
+		return 1;
+	else
+		return 0;
+}
+
+static void qspi_flash_uinit(struct platform_device *pdev,
+			     struct feature *feature)
+{
+	struct device *parent = &pdev->dev;
+	struct feature_platform_search src;
+	struct device *dev;
+	struct platform_device *cdev;
+
+	src.drv_name = ALTERA_ASMIP2_DRV_NAME;
+	src.name_len = strlen(ALTERA_ASMIP2_DRV_NAME);
+	src.feature = feature;
+
+	dev = device_find_child(parent, &src, qspi_match);
+
+	if (!dev) {
+		dev_err(&pdev->dev, "%s NOT found\n", ALTERA_ASMIP2_DRV_NAME);
+		return;
+	}
+
+	dev_info(&pdev->dev, "%s found %s\n", __func__, ALTERA_ASMIP2_DRV_NAME);
+
+	cdev = to_platform_device(dev);
+
+	if (!cdev) {
+		dev_err(&pdev->dev, "no platform container\n");
+		return;
+	}
+
+	platform_device_unregister(cdev);
+}
+
+struct feature_ops qspi_flash_ops = {
+	.init = qspi_flash_init,
+	.uinit = qspi_flash_uinit,
+};
+
+static struct feature_driver fme_feature_drvs[] = {
+	{
+		.name = FME_FEATURE_HEADER,
+		.ops = &fme_hdr_ops,
+	},
+	{
+		.name = FME_FEATURE_THERMAL_MGMT,
+		.ops = &thermal_mgmt_ops,
+	},
+	{
+		.name = FME_FEATURE_POWER_MGMT,
+		.ops = &power_mgmt_ops,
+	},
+	{
+		.name = FME_FEATURE_GLOBAL_ERR,
+		.ops = &global_error_ops,
+	},
+	{
+		.name = FME_FEATURE_PR_MGMT,
+		.ops = &pr_mgmt_ops,
+	},
+	{
+		.name = FME_FEATURE_GLOBAL_IPERF,
+		.ops = &global_iperf_ops,
+	},
+	{
+		.name = FME_FEATURE_HSSI_ETH,
+		.ops = &hssi_mgmt_ops,
+	},
+	{
+		.name = FME_FEATURE_GLOBAL_DPERF,
+		.ops = &global_dperf_ops,
+	},
+	{
+		.name = FME_FEATURE_QSPI_FLASH,
+		.ops = &qspi_flash_ops,
+	},
+	{
+		.ops = NULL,
+	},
+};
+
+static long fme_ioctl_check_extension(struct feature_platform_data *pdata,
+				     unsigned long arg)
+{
+	/* No extension support for now */
+	return 0;
+}
+
+static long
+fme_ioctl_get_info(struct feature_platform_data *pdata, void __user *arg)
+{
+	struct fpga_fme_info info;
+	struct fpga_fme *fme;
+	unsigned long minsz;
+
+	minsz = offsetofend(struct fpga_fme_info, capability);
+
+	if (copy_from_user(&info, arg, minsz))
+		return -EFAULT;
+
+	if (info.argsz < minsz)
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	fme = fpga_pdata_get_private(pdata);
+	info.flags = 0;
+	info.capability = fme->capability;
+	mutex_unlock(&pdata->lock);
+
+	if (copy_to_user(arg, &info, sizeof(info)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static int fme_ioctl_config_port(struct feature_platform_data *pdata,
+				 u32 port_id, u32 flags, bool is_release)
+{
+	struct platform_device *fme_pdev = pdata->dev;
+	struct feature_fme_header *fme_hdr;
+	struct feature_fme_capability capability;
+
+	if (flags)
+		return -EINVAL;
+
+	fme_hdr = get_feature_ioaddr_by_index(
+		&fme_pdev->dev, FME_FEATURE_ID_HEADER);
+	capability.csr = readq(&fme_hdr->capability);
+
+	if (port_id >= capability.num_ports)
+		return -EINVAL;
+
+	return pdata->config_port(fme_pdev, port_id, is_release);
+}
+
+static long fme_ioctl_release_port(struct feature_platform_data *pdata,
+				   void __user *arg)
+{
+	struct fpga_fme_port_release release;
+	unsigned long minsz;
+
+	minsz = offsetofend(struct fpga_fme_port_release, port_id);
+
+	if (copy_from_user(&release, arg, minsz))
+		return -EFAULT;
+
+	if (release.argsz < minsz)
+		return -EINVAL;
+
+	return fme_ioctl_config_port(pdata, release.port_id,
+				     release.flags, true);
+}
+
+static long fme_ioctl_assign_port(struct feature_platform_data *pdata,
+				  void __user *arg)
+{
+	struct fpga_fme_port_assign assign;
+	unsigned long minsz;
+
+	minsz = offsetofend(struct fpga_fme_port_assign, port_id);
+
+	if (copy_from_user(&assign, arg, minsz))
+		return -EFAULT;
+
+	if (assign.argsz < minsz)
+		return -EINVAL;
+
+	return fme_ioctl_config_port(pdata, assign.port_id,
+				     assign.flags, false);
+}
+
+static int fme_open(struct inode *inode, struct file *filp)
+{
+	struct platform_device *fdev = fpga_inode_to_feature_dev(inode);
+	struct feature_platform_data *pdata = dev_get_platdata(&fdev->dev);
+	int ret;
+
+	if (WARN_ON(!pdata))
+		return -ENODEV;
+
+	if (filp->f_flags & O_EXCL)
+		ret = feature_dev_use_excl_begin(pdata);
+	else
+		ret = feature_dev_use_begin(pdata);
+
+	if (ret)
+		return ret;
+
+	dev_dbg(&fdev->dev, "Device File Opened %d Times\n", pdata->open_count);
+	filp->private_data = pdata;
+	return 0;
+}
+
+static int fme_release(struct inode *inode, struct file *filp)
+{
+	struct feature_platform_data *pdata = filp->private_data;
+	struct platform_device *pdev = pdata->dev;
+
+	dev_dbg(&pdev->dev, "Device File Release\n");
+	mutex_lock(&pdata->lock);
+	__feature_dev_use_end(pdata);
+
+	if (!pdata->open_count)
+		fpga_msix_set_block(&pdata->features[FME_FEATURE_ID_GLOBAL_ERR],
+			0, pdata->features[FME_FEATURE_ID_GLOBAL_ERR].ctx_num,
+			NULL);
+	mutex_unlock(&pdata->lock);
+
+	return 0;
+}
+
+static long fme_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	struct feature_platform_data *pdata = filp->private_data;
+	struct platform_device *pdev = pdata->dev;
+	struct feature *f;
+	long ret;
+
+	dev_dbg(&pdev->dev, "%s cmd 0x%x\n", __func__, cmd);
+
+	switch (cmd) {
+	case FPGA_GET_API_VERSION:
+		return FPGA_API_VERSION;
+	case FPGA_CHECK_EXTENSION:
+		return fme_ioctl_check_extension(pdata, arg);
+	case FPGA_FME_GET_INFO:
+		return fme_ioctl_get_info(pdata, (void __user *)arg);
+	case FPGA_FME_PORT_RELEASE:
+		return fme_ioctl_release_port(pdata, (void __user *)arg);
+	case FPGA_FME_PORT_ASSIGN:
+		return fme_ioctl_assign_port(pdata, (void __user *)arg);
+	default:
+		/*
+		 * Let sub-feature's ioctl function to handle the cmd
+		 * Sub-feature's ioctl returns -ENODEV when cmd is not
+		 * handled in this sub feature, and returns 0 and other
+		 * error code if cmd is handled.
+		 */
+		fpga_dev_for_each_feature(pdata, f) {
+			if (f->ops && f->ops->ioctl) {
+				ret = f->ops->ioctl(pdev, f, cmd, arg);
+				if (ret == -ENODEV)
+					continue;
+				else
+					return ret;
+			}
+		}
+	}
+
+	return -EINVAL;
+}
+
+static const struct file_operations fme_fops = {
+	.owner		= THIS_MODULE,
+	.open		= fme_open,
+	.release	= fme_release,
+	.unlocked_ioctl = fme_ioctl,
+};
+
+static int fme_dev_init(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+
+	fme = devm_kzalloc(&pdev->dev, sizeof(*fme), GFP_KERNEL);
+	if (!fme)
+		return -ENOMEM;
+
+	fme->pdata = pdata;
+
+	mutex_lock(&pdata->lock);
+	fpga_pdata_set_private(pdata, fme);
+	mutex_unlock(&pdata->lock);
+	return 0;
+}
+
+static void fme_dev_destroy(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+
+	mutex_lock(&pdata->lock);
+	fme = fpga_pdata_get_private(pdata);
+	fpga_pdata_set_private(pdata, NULL);
+	mutex_unlock(&pdata->lock);
+
+	devm_kfree(&pdev->dev, fme);
+}
+
+static int fme_probe(struct platform_device *pdev)
+{
+	int ret;
+
+	ret = fme_dev_init(pdev);
+	if (ret)
+		goto exit;
+
+	ret = fpga_dev_feature_init(pdev, fme_feature_drvs);
+	if (ret)
+		goto dev_destroy;
+
+	ret = fpga_register_dev_ops(pdev, &fme_fops, THIS_MODULE);
+	if (ret)
+		goto feature_uinit;
+
+	return 0;
+
+feature_uinit:
+	fpga_dev_feature_uinit(pdev);
+dev_destroy:
+	fme_dev_destroy(pdev);
+exit:
+	return ret;
+}
+
+static int fme_remove(struct platform_device *pdev)
+{
+	fpga_dev_feature_uinit(pdev);
+	fpga_unregister_dev_ops(pdev);
+	fme_dev_destroy(pdev);
+	return 0;
+}
+
+static struct platform_driver fme_driver = {
+	.driver	= {
+		.name    = FPGA_FEATURE_DEV_FME,
+	},
+	.probe   = fme_probe,
+	.remove  = fme_remove,
+};
+
+module_platform_driver(fme_driver);
+
+MODULE_DESCRIPTION("FPGA Management Engine driver");
+MODULE_AUTHOR("Intel Corporation");
+MODULE_LICENSE("GPL v2");
+MODULE_ALIAS("platform:intel-fpga-fme");
diff --git a/drivers/fpga/intel/fme-perf.c b/drivers/fpga/intel/fme-perf.c
new file mode 100644
index 000000000000..e139e3f8db97
--- /dev/null
+++ b/drivers/fpga/intel/fme-perf.c
@@ -0,0 +1,715 @@
+/*
+ * Driver for FPGA Global Performance
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include "feature-dev.h"
+#include "fme.h"
+
+struct perf_obj_attributte {
+	struct attribute attr;
+	ssize_t (*show)(struct perf_object *pobj, char *buf);
+	ssize_t (*store)(struct perf_object *pobj,
+			 const char *buf, size_t n);
+};
+
+#define to_perf_obj_attr(_attr)					\
+		container_of(_attr, struct perf_obj_attributte, attr)
+#define to_perf_obj(_kobj)					\
+		container_of(_kobj, struct perf_object, kobj)
+
+static ssize_t perf_obj_attr_show(struct kobject *kobj,
+				     struct attribute *__attr, char *buf)
+{
+	struct perf_obj_attributte *attr = to_perf_obj_attr(__attr);
+	struct perf_object *pobj = to_perf_obj(kobj);
+	ssize_t ret = -EIO;
+
+	if (attr->show)
+		ret = attr->show(pobj, buf);
+	return ret;
+}
+
+static ssize_t perf_obj_attr_store(struct kobject *kobj,
+				      struct attribute *__attr,
+				      const char *buf, size_t n)
+{
+	struct perf_obj_attributte *attr = to_perf_obj_attr(__attr);
+	struct perf_object *pobj = to_perf_obj(kobj);
+	ssize_t ret = -EIO;
+
+	if (attr->store)
+		ret = attr->store(pobj, buf, n);
+	return ret;
+}
+
+static const struct sysfs_ops perf_obj_sysfs_ops = {
+	.show = perf_obj_attr_show,
+	.store = perf_obj_attr_store,
+};
+
+static void perf_obj_release(struct kobject *kobj)
+{
+	kfree(to_perf_obj(kobj));
+}
+
+static struct kobj_type perf_obj_ktype = {
+	.sysfs_ops = &perf_obj_sysfs_ops,
+	.release = perf_obj_release,
+};
+
+#define PERF_OBJ_ATTR(_name, _filename, _mode, _show, _store)	\
+struct perf_obj_attributte perf_obj_attr_##_name =		\
+	__ATTR(_filename, _mode, _show, _store)
+#define PERF_OBJ_ATTR_RW(_name)					\
+	struct perf_obj_attributte perf_obj_attr_##_name = __ATTR_RW(_name)
+#define PERF_OBJ_ATTR_RO(_name)					\
+	struct perf_obj_attributte perf_obj_attr_##_name = __ATTR_RO(_name)
+#define PERF_OBJ_ATTR_WO(_name)					\
+	struct perf_obj_attributte perf_obj_attr_##_name = __ATTR_WO(_name)
+
+static ssize_t revision_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_gperf *gperf;
+	struct feature_header header;
+
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	header.csr = readq(&gperf->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+static PERF_OBJ_ATTR_RO(revision);
+
+static ssize_t clock_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_gperf *gperf;
+	struct feature_fme_fpmon_clk_ctr clk;
+
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	clk.afu_interf_clock = readq(&gperf->clk);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", clk.afu_interf_clock);
+}
+static PERF_OBJ_ATTR_RO(clock);
+
+static struct attribute *clock_attrs[] = {
+	&perf_obj_attr_revision.attr,
+	&perf_obj_attr_clock.attr,
+	NULL,
+};
+
+static struct attribute_group clock_attr_group = {
+	.attrs = clock_attrs,
+};
+
+static ssize_t freeze_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_gperf *gperf;
+	struct feature_fme_fpmon_ch_ctl ctl;
+
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	ctl.csr = readq(&gperf->ch_ctl);
+	return scnprintf(buf, PAGE_SIZE, "%d\n", ctl.freeze);
+}
+
+static ssize_t freeze_store(struct perf_object *pobj,
+		 const char *buf, size_t n)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(pobj->fme_dev);
+	struct feature_fme_gperf *gperf;
+	struct feature_fme_fpmon_ch_ctl ctl;
+	bool state;
+
+	if (strtobool(buf, &state))
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	ctl.csr = readq(&gperf->ch_ctl);
+	ctl.freeze = state;
+	writeq(ctl.csr, &gperf->ch_ctl);
+	mutex_unlock(&pdata->lock);
+
+	return n;
+}
+static PERF_OBJ_ATTR_RW(freeze);
+
+#define GPERF_TIMEOUT	30
+
+static ssize_t read_cache_counter(struct perf_object *pobj, char *buf,
+				  u8 channel, enum gperf_cache_events event)
+{
+	struct device *fme_dev = pobj->fme_dev;
+	struct feature_platform_data *pdata = dev_get_platdata(fme_dev);
+	struct feature_fme_gperf *gperf;
+	struct feature_fme_fpmon_ch_ctl ctl;
+	struct feature_fme_fpmon_ch_ctr ctr0, ctr1;
+	u64 counter;
+
+	mutex_lock(&pdata->lock);
+	gperf = get_feature_ioaddr_by_index(fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+
+	/* set channel access type and cache event code. */
+	ctl.csr = readq(&gperf->ch_ctl);
+	ctl.cci_chsel = channel;
+	ctl.cache_event = event;
+	writeq(ctl.csr, &gperf->ch_ctl);
+
+	/* check the event type in the counter registers */
+	ctr0.event_code = event;
+
+	if (fpga_wait_register_field(event_code, ctr0,
+				     &gperf->ch_ctr0, GPERF_TIMEOUT, 1)) {
+		dev_err(fme_dev,
+		"timeout, unmatched cache event type in counter registers.\n");
+		mutex_unlock(&pdata->lock);
+		return -ETIMEDOUT;
+	}
+
+	ctr0.csr = readq(&gperf->ch_ctr0);
+	ctr1.csr = readq(&gperf->ch_ctr1);
+	counter = ctr0.cache_counter + ctr1.cache_counter;
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", counter);
+}
+
+#define CACHE_SHOW(name, type, event)					\
+static ssize_t name##_show(struct perf_object *pobj, char *buf)		\
+{									\
+	return read_cache_counter(pobj, buf, type, event);		\
+}									\
+static PERF_OBJ_ATTR_RO(name)
+
+CACHE_SHOW(read_hit, CACHE_CHANNEL_RD, CACHE_RD_HIT);
+CACHE_SHOW(read_miss, CACHE_CHANNEL_RD, CACHE_RD_MISS);
+CACHE_SHOW(write_hit, CACHE_CHANNEL_WR, CACHE_WR_HIT);
+CACHE_SHOW(write_miss, CACHE_CHANNEL_WR, CACHE_WR_MISS);
+CACHE_SHOW(hold_request, CACHE_CHANNEL_RD, CACHE_HOLD_REQ);
+CACHE_SHOW(tx_req_stall, CACHE_CHANNEL_RD, CACHE_TX_REQ_STALL);
+CACHE_SHOW(rx_req_stall, CACHE_CHANNEL_RD, CACHE_RX_REQ_STALL);
+CACHE_SHOW(rx_eviction, CACHE_CHANNEL_RD, CACHE_EVICTIONS);
+CACHE_SHOW(data_write_port_contention, CACHE_CHANNEL_WR,
+	   CACHE_DATA_WR_PORT_CONTEN);
+CACHE_SHOW(tag_write_port_contention, CACHE_CHANNEL_WR,
+	   CACHE_TAG_WR_PORT_CONTEN);
+
+static struct attribute *cache_attrs[] = {
+	&perf_obj_attr_read_hit.attr,
+	&perf_obj_attr_read_miss.attr,
+	&perf_obj_attr_write_hit.attr,
+	&perf_obj_attr_write_miss.attr,
+	&perf_obj_attr_hold_request.attr,
+	&perf_obj_attr_data_write_port_contention.attr,
+	&perf_obj_attr_tag_write_port_contention.attr,
+	&perf_obj_attr_tx_req_stall.attr,
+	&perf_obj_attr_rx_req_stall.attr,
+	&perf_obj_attr_rx_eviction.attr,
+	&perf_obj_attr_freeze.attr,
+	NULL,
+};
+
+static struct attribute_group cache_attr_group = {
+	.name = "cache",
+	.attrs = cache_attrs,
+};
+
+static const struct attribute_group *perf_dev_attr_groups[] = {
+	&clock_attr_group,
+	&cache_attr_group,
+	NULL,
+};
+
+ssize_t vtd_freeze_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_fpmon_vtd_ctl ctl;
+	struct feature_fme_gperf *gperf;
+
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					FME_FEATURE_ID_GLOBAL_PERF);
+	ctl.csr = readq(&gperf->vtd_ctl);
+	return scnprintf(buf, PAGE_SIZE, "%d\n", ctl.freeze);
+}
+
+ssize_t vtd_freeze_store(struct perf_object *pobj,
+		 const char *buf, size_t n)
+{
+	struct feature_platform_data *pdata;
+	struct feature_fme_fpmon_vtd_ctl ctl;
+	struct feature_fme_gperf *gperf;
+	bool state;
+
+	if (strtobool(buf, &state))
+		return -EINVAL;
+
+	pdata = dev_get_platdata(pobj->fme_dev);
+	mutex_lock(&pdata->lock);
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	ctl.csr = readq(&gperf->vtd_ctl);
+	ctl.freeze = state;
+	writeq(ctl.csr, &gperf->vtd_ctl);
+	mutex_unlock(&pdata->lock);
+
+	return n;
+}
+
+static PERF_OBJ_ATTR(vtd_freeze, freeze, 0644, vtd_freeze_show,
+		     vtd_freeze_store);
+static struct attribute *iommu_top_attrs[] = {
+	&perf_obj_attr_vtd_freeze.attr,
+	NULL,
+};
+
+static struct attribute_group iommu_top_attr_group = {
+	.attrs = iommu_top_attrs,
+};
+
+static const struct attribute_group *iommu_top_attr_groups[] = {
+	&iommu_top_attr_group,
+	NULL,
+};
+
+static ssize_t read_iommu_counter(struct perf_object *pobj,
+				  enum gperf_vtd_events base_event, char *buf)
+{
+	struct feature_platform_data *pdata;
+	struct feature_fme_fpmon_vtd_ctl ctl;
+	struct feature_fme_fpmon_vtd_ctr ctr;
+	struct feature_fme_gperf *gperf;
+	enum gperf_vtd_events event = base_event + pobj->id;
+	u64 counter;
+
+	pdata = dev_get_platdata(pobj->fme_dev);
+	mutex_lock(&pdata->lock);
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	ctl.csr = readq(&gperf->vtd_ctl);
+	ctl.vtd_evtcode = event;
+	writeq(ctl.csr, &gperf->vtd_ctl);
+
+	ctr.event_code = event;
+
+	if (fpga_wait_register_field(event_code, ctr,
+				     &gperf->vtd_ctr, GPERF_TIMEOUT, 1)) {
+		dev_err(pobj->fme_dev,
+		"timeout, unmatched VTd event type in counter registers.\n");
+		mutex_unlock(&pdata->lock);
+		return -ETIMEDOUT;
+	}
+
+	ctr.csr = readq(&gperf->vtd_ctr);
+	counter = ctr.vtd_counter;
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", counter);
+}
+
+#define VTD_SHOW(name, base_event)					\
+static ssize_t name##_show(struct perf_object *pobj, char *buf)		\
+{									\
+	return read_iommu_counter(pobj, base_event, buf);		\
+}									\
+static PERF_OBJ_ATTR_RO(name)
+
+VTD_SHOW(read_transaction, VTD_AFU0_MEM_RD_TRANS);
+VTD_SHOW(write_transaction, VTD_AFU0_MEM_WR_TRANS);
+VTD_SHOW(tlb_read_hit, VTD_AFU0_TLB_RD_HIT);
+VTD_SHOW(tlb_write_hit, VTD_AFU0_TLB_WR_HIT);
+
+static struct attribute *iommu_attrs[] = {
+	&perf_obj_attr_read_transaction.attr,
+	&perf_obj_attr_write_transaction.attr,
+	&perf_obj_attr_tlb_read_hit.attr,
+	&perf_obj_attr_tlb_write_hit.attr,
+	NULL,
+};
+
+static struct attribute_group iommu_attr_group = {
+	.attrs = iommu_attrs,
+};
+
+static const struct attribute_group *iommu_attr_groups[] = {
+	&iommu_attr_group,
+	NULL,
+};
+
+static bool fabric_pobj_is_enabled(struct perf_object *pobj,
+		struct feature_fme_gperf *gperf)
+{
+	struct feature_fme_fpmon_fab_ctl ctl;
+
+	ctl.csr = readq(&gperf->fab_ctl);
+
+	if (ctl.port_filter == FAB_DISABLE_FILTER)
+		return pobj->id == PERF_OBJ_ROOT_ID;
+
+	return pobj->id == ctl.port_id;
+}
+
+static ssize_t read_fabric_counter(struct perf_object *pobj,
+				  enum gperf_fab_events fab_event, char *buf)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(pobj->fme_dev);
+	struct feature_fme_fpmon_fab_ctl ctl;
+	struct feature_fme_fpmon_fab_ctr ctr;
+	struct feature_fme_gperf *gperf;
+	u64 counter = 0;
+
+	mutex_lock(&pdata->lock);
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+
+	/* if it is disabled, force the counter to return zero. */
+	if (!fabric_pobj_is_enabled(pobj, gperf))
+		goto exit;
+
+	ctl.csr = readq(&gperf->fab_ctl);
+	ctl.fab_evtcode = fab_event;
+	writeq(ctl.csr, &gperf->fab_ctl);
+
+	ctr.event_code = fab_event;
+
+	if (fpga_wait_register_field(event_code, ctr,
+				     &gperf->fab_ctr, GPERF_TIMEOUT, 1)) {
+		dev_err(pobj->fme_dev,
+		"timeout, unmatched VTd event type in counter registers.\n");
+		mutex_unlock(&pdata->lock);
+		return -ETIMEDOUT;
+	}
+
+	ctr.csr = readq(&gperf->fab_ctr);
+	counter = ctr.fab_cnt;
+exit:
+	mutex_unlock(&pdata->lock);
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", counter);
+}
+
+#define FAB_SHOW(name, event)						\
+static ssize_t name##_show(struct perf_object *pobj, char *buf)		\
+{									\
+	return read_fabric_counter(pobj, event, buf);			\
+}									\
+static PERF_OBJ_ATTR_RO(name)
+
+FAB_SHOW(pcie0_read, FAB_PCIE0_RD);
+FAB_SHOW(pcie0_write, FAB_PCIE0_WR);
+FAB_SHOW(pcie1_read, FAB_PCIE1_RD);
+FAB_SHOW(pcie1_write, FAB_PCIE1_WR);
+FAB_SHOW(upi_read, FAB_UPI_RD);
+FAB_SHOW(upi_write, FAB_UPI_WR);
+FAB_SHOW(mmio_read, FAB_MMIO_RD);
+FAB_SHOW(mmio_write, FAB_MMIO_WR);
+
+static ssize_t fab_enable_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_gperf *gperf;
+	int status;
+
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+
+	status = fabric_pobj_is_enabled(pobj, gperf);
+	return scnprintf(buf, PAGE_SIZE, "%d\n", status);
+}
+
+/*
+ * If enable one port or all port event counter in fabric, other
+ * fabric event counter originally enabled will be disable automatically.
+ */
+static ssize_t fab_enable_store(struct perf_object *pobj,
+		 const char *buf, size_t n)
+{
+	struct feature_platform_data *pdata  = dev_get_platdata(pobj->fme_dev);
+	struct feature_fme_fpmon_fab_ctl ctl;
+	struct feature_fme_gperf *gperf;
+	bool state;
+
+	if (strtobool(buf, &state))
+		return -EINVAL;
+
+	if (!state)
+		return -EINVAL;
+
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+
+	/* if it is already enabled. */
+	if (fabric_pobj_is_enabled(pobj, gperf))
+		return n;
+
+	mutex_lock(&pdata->lock);
+	ctl.csr = readq(&gperf->fab_ctl);
+	if (pobj->id == PERF_OBJ_ROOT_ID)
+		ctl.port_filter = FAB_DISABLE_FILTER;
+	else {
+		ctl.port_filter = FAB_ENABLE_FILTER;
+		ctl.port_id = pobj->id;
+	}
+
+	writeq(ctl.csr, &gperf->fab_ctl);
+	mutex_unlock(&pdata->lock);
+
+	return n;
+}
+
+static PERF_OBJ_ATTR(fab_enable, enable, 0644, fab_enable_show,
+		     fab_enable_store);
+
+static struct attribute *fabric_attrs[] = {
+	&perf_obj_attr_pcie0_read.attr,
+	&perf_obj_attr_pcie0_write.attr,
+	&perf_obj_attr_pcie1_read.attr,
+	&perf_obj_attr_pcie1_write.attr,
+	&perf_obj_attr_upi_read.attr,
+	&perf_obj_attr_upi_write.attr,
+	&perf_obj_attr_mmio_read.attr,
+	&perf_obj_attr_mmio_write.attr,
+	&perf_obj_attr_fab_enable.attr,
+	NULL,
+};
+
+static struct attribute_group fabric_attr_group = {
+	.attrs = fabric_attrs,
+};
+
+static const struct attribute_group *fabric_attr_groups[] = {
+	&fabric_attr_group,
+	NULL,
+};
+
+static ssize_t fab_freeze_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_gperf *gperf;
+	struct feature_fme_fpmon_fab_ctl ctl;
+
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	ctl.csr = readq(&gperf->fab_ctl);
+	return scnprintf(buf, PAGE_SIZE, "%d\n", ctl.freeze);
+}
+
+static ssize_t fab_freeze_store(struct perf_object *pobj,
+		 const char *buf, size_t n)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(pobj->fme_dev);
+	struct feature_fme_gperf *gperf;
+	struct feature_fme_fpmon_fab_ctl ctl;
+	bool state;
+
+	if (strtobool(buf, &state))
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	ctl.csr = readq(&gperf->fab_ctl);
+	ctl.freeze = state;
+	writeq(ctl.csr, &gperf->fab_ctl);
+	mutex_unlock(&pdata->lock);
+
+	return n;
+}
+
+static PERF_OBJ_ATTR(fab_freeze, freeze, 0644, fab_freeze_show,
+		     fab_freeze_store);
+
+static struct attribute *fabric_top_attrs[] = {
+	&perf_obj_attr_fab_freeze.attr,
+	NULL,
+};
+
+static struct attribute_group fabric_top_attr_group = {
+	.attrs = fabric_top_attrs,
+};
+
+static const struct attribute_group *fabric_top_attr_groups[] = {
+	&fabric_attr_group,
+	&fabric_top_attr_group,
+	NULL,
+};
+
+static struct perf_object *
+create_perf_obj(struct device *fme_dev, struct kobject *parent, int id,
+		const struct attribute_group **groups, const char *name)
+{
+	struct perf_object *pobj;
+	int ret;
+
+	pobj = kzalloc(sizeof(*pobj), GFP_KERNEL);
+	if (!pobj)
+		return ERR_PTR(-ENOMEM);
+
+	pobj->id = id;
+	pobj->fme_dev = fme_dev;
+	pobj->attr_groups = groups;
+	INIT_LIST_HEAD(&pobj->node);
+	INIT_LIST_HEAD(&pobj->children);
+
+	if (id != PERF_OBJ_ROOT_ID)
+		ret = kobject_init_and_add(&pobj->kobj, &perf_obj_ktype,
+				   parent, "%s%d", name, id);
+	else
+		ret = kobject_init_and_add(&pobj->kobj, &perf_obj_ktype,
+				   parent, "%s", name);
+	if (ret)
+		goto put_exit;
+
+	if (pobj->attr_groups) {
+		ret = sysfs_create_groups(&pobj->kobj, pobj->attr_groups);
+		if (ret)
+			goto put_exit;
+	}
+
+	return pobj;
+
+put_exit:
+	kobject_put(&pobj->kobj);
+	return ERR_PTR(ret);
+}
+
+static void destroy_perf_obj(struct perf_object *pobj)
+{
+	struct perf_object *obj, *obj_tmp;
+
+	list_for_each_entry_safe(obj, obj_tmp, &pobj->children, node)
+		destroy_perf_obj(obj);
+
+	list_del(&pobj->node);
+	if (pobj->attr_groups)
+		sysfs_remove_groups(&pobj->kobj, pobj->attr_groups);
+	kobject_put(&pobj->kobj);
+}
+
+#define PERF_MAX_PORT_NUM	2
+
+static int create_perf_iommu_obj(struct perf_object *perf_dev)
+{
+	struct perf_object *pobj;
+	struct feature_fme_header *fme_hdr;
+	struct feature_fme_capability fme_capability;
+	int i;
+
+	fme_hdr = get_feature_ioaddr_by_index(perf_dev->fme_dev,
+					      FME_FEATURE_ID_HEADER);
+
+	/* check if iommu is not supported on this device. */
+	fme_capability.csr = readq(&fme_hdr->capability);
+	if (!fme_capability.iommu_support)
+		return 0;
+
+	pobj = create_perf_obj(perf_dev->fme_dev, &perf_dev->kobj,
+			   PERF_OBJ_ROOT_ID, iommu_top_attr_groups, "iommu");
+	if (IS_ERR(pobj))
+		return PTR_ERR(pobj);
+
+	list_add(&pobj->node, &perf_dev->children);
+
+	for (i = 0; i < PERF_MAX_PORT_NUM; i++) {
+		struct perf_object *obj;
+
+		obj = create_perf_obj(perf_dev->fme_dev, &pobj->kobj, i,
+				       iommu_attr_groups, "afu");
+		if (IS_ERR(obj))
+			return PTR_ERR(obj);
+
+		list_add(&obj->node, &pobj->children);
+	}
+
+	return 0;
+}
+
+static int create_perf_fabric_obj(struct perf_object *perf_dev)
+{
+	struct perf_object *pobj;
+	int i;
+
+	pobj = create_perf_obj(perf_dev->fme_dev, &perf_dev->kobj,
+			   PERF_OBJ_ROOT_ID, fabric_top_attr_groups, "fabric");
+	if (IS_ERR(pobj))
+		return PTR_ERR(pobj);
+
+	list_add(&pobj->node, &perf_dev->children);
+
+	for (i = 0; i < PERF_MAX_PORT_NUM; i++) {
+		struct perf_object *obj;
+
+		obj = create_perf_obj(perf_dev->fme_dev, &pobj->kobj, i,
+				       fabric_attr_groups, "port");
+		if (IS_ERR(obj))
+			return PTR_ERR(obj);
+
+		list_add(&obj->node, &pobj->children);
+	}
+
+	return 0;
+}
+
+static struct perf_object *create_perf_dev(struct platform_device *pdev)
+{
+	return create_perf_obj(&pdev->dev, &pdev->dev.kobj,
+			   PERF_OBJ_ROOT_ID, perf_dev_attr_groups, "perf");
+}
+
+static int fme_perf_init(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+	struct perf_object *perf_dev;
+	int ret;
+
+	perf_dev = create_perf_dev(pdev);
+	if (IS_ERR(perf_dev))
+		return PTR_ERR(perf_dev);
+
+	ret = create_perf_iommu_obj(perf_dev);
+	if (ret) {
+		destroy_perf_obj(perf_dev);
+		return ret;
+	}
+
+	ret = create_perf_fabric_obj(perf_dev);
+	if (ret) {
+		destroy_perf_obj(perf_dev);
+		return ret;
+	}
+
+	fme = fpga_pdata_get_private(pdata);
+	fme->perf_dev = perf_dev;
+	return 0;
+}
+
+static void
+fme_perf_uinit(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+
+	fme = fpga_pdata_get_private(pdata);
+	destroy_perf_obj(fme->perf_dev);
+	fme->perf_dev = NULL;
+}
+
+struct feature_ops global_perf_ops = {
+	.init = fme_perf_init,
+	.uinit = fme_perf_uinit,
+};
diff --git a/drivers/fpga/intel/fme-pr.c b/drivers/fpga/intel/fme-pr.c
new file mode 100644
index 000000000000..9ef62ab30ae4
--- /dev/null
+++ b/drivers/fpga/intel/fme-pr.c
@@ -0,0 +1,487 @@
+/*
+ * Driver for FPGA Partial Reconfiguration
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Christopher Rauer <christopher.rauer@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include <linux/types.h>
+#include <linux/device.h>
+#include <linux/delay.h>
+#include <linux/vmalloc.h>
+#include <linux/uaccess.h>
+#include <linux/stddef.h> /* offsetofend */
+#include <linux/vfio.h> /* offsetofend in pre-4.1.0 kernels */
+#include <linux/intel-fpga.h>
+#include <linux/fpga/fpga-mgr.h>
+
+#include "feature-dev.h"
+#include "fme.h"
+
+#define PR_WAIT_TIMEOUT		8000000
+
+#define PR_HOST_STATUS_IDLE	0
+
+DEFINE_FPGA_PR_ERR_MSG(pr_err_msg);
+
+#if defined(CONFIG_X86) && defined(CONFIG_AS_AVX512)
+
+#include <asm/fpu/api.h>
+
+static inline void copy512(void *src, void *dst)
+{
+	asm volatile("vmovdqu64 (%0), %%zmm0;"
+		     "vmovntdq %%zmm0, (%1);"
+		     :
+		     : "r"(src), "r"(dst));
+}
+#else
+static inline void kernel_fpu_begin(void)
+{
+}
+
+static inline void kernel_fpu_end(void)
+{
+}
+
+static inline void copy512(void *src, void *dst)
+{
+	WARN_ON_ONCE(1);
+}
+#endif
+
+static ssize_t revision_show(struct device *dev, struct device_attribute *attr,
+			     char *buf)
+{
+	struct feature_fme_pr *fme_pr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_PR_MGMT);
+	struct feature_header header;
+
+	header.csr = readq(&fme_pr->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+
+static DEVICE_ATTR_RO(revision);
+
+static ssize_t interface_id_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_pr_key intfc_id_l, intfc_id_h;
+	struct feature_fme_pr *fme_pr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_PR_MGMT);
+
+	intfc_id_l.key = readq(&fme_pr->fme_pr_intfc_id_l);
+	intfc_id_h.key = readq(&fme_pr->fme_pr_intfc_id_h);
+
+	return scnprintf(buf, PAGE_SIZE, "%016llx%016llx\n",
+			intfc_id_h.key, intfc_id_l.key);
+}
+
+static DEVICE_ATTR_RO(interface_id);
+
+static struct attribute *pr_mgmt_attrs[] = {
+	&dev_attr_revision.attr,
+	&dev_attr_interface_id.attr,
+	NULL,
+};
+
+struct attribute_group pr_mgmt_attr_group = {
+	.attrs	= pr_mgmt_attrs,
+	.name	= "pr",
+};
+
+static u64
+pr_err_handle(struct platform_device *pdev, struct feature_fme_pr *fme_pr)
+{
+	struct feature_fme_pr_status fme_pr_status;
+	unsigned long err_code;
+	u64 fme_pr_error;
+	int i = 0;
+
+	fme_pr_status.csr = readq(&fme_pr->ccip_fme_pr_status);
+	if (!fme_pr_status.pr_status)
+		return 0;
+
+	err_code = fme_pr_error = readq(&fme_pr->ccip_fme_pr_err);
+	for_each_set_bit(i, &err_code, PR_MAX_ERR_NUM)
+		dev_dbg(&pdev->dev, "%s\n", pr_err_msg[i]);
+	writeq(fme_pr_error, &fme_pr->ccip_fme_pr_err);
+	return fme_pr_error;
+}
+
+static int fme_pr_write_init(struct fpga_manager *mgr,
+		struct fpga_image_info *info, const char *buf, size_t count)
+{
+	struct fpga_fme *fme = mgr->priv;
+	struct platform_device *pdev;
+	struct feature_fme_pr *fme_pr;
+	struct feature_fme_pr_ctl fme_pr_ctl;
+	struct feature_fme_pr_status fme_pr_status;
+
+	pdev = fme->pdata->dev;
+	fme_pr = get_feature_ioaddr_by_index(&pdev->dev,
+				FME_FEATURE_ID_PR_MGMT);
+	if (!fme_pr)
+		return -EINVAL;
+
+	if (WARN_ON(info->flags != FPGA_MGR_PARTIAL_RECONFIG))
+		return -EINVAL;
+
+	dev_dbg(&pdev->dev, "resetting PR before initiated PR\n");
+
+	fme_pr_ctl.csr = readq(&fme_pr->ccip_fme_pr_control);
+	fme_pr_ctl.pr_reset = 1;
+	writeq(fme_pr_ctl.csr, &fme_pr->ccip_fme_pr_control);
+
+	fme_pr_ctl.pr_reset_ack = 1;
+
+	if (fpga_wait_register_field(pr_reset_ack, fme_pr_ctl,
+		&fme_pr->ccip_fme_pr_control, PR_WAIT_TIMEOUT, 1)) {
+		dev_err(&pdev->dev, "maximum PR timeout\n");
+		return -ETIMEDOUT;
+	}
+
+	fme_pr_ctl.csr = readq(&fme_pr->ccip_fme_pr_control);
+	fme_pr_ctl.pr_reset = 0;
+	writeq(fme_pr_ctl.csr, &fme_pr->ccip_fme_pr_control);
+
+	dev_dbg(&pdev->dev,
+		"waiting for PR resource in HW to be initialized and ready\n");
+
+	fme_pr_status.pr_host_status = PR_HOST_STATUS_IDLE;
+
+	if (fpga_wait_register_field(pr_host_status, fme_pr_status,
+		&fme_pr->ccip_fme_pr_status, PR_WAIT_TIMEOUT, 1)) {
+		dev_err(&pdev->dev, "maximum PR timeout\n");
+		return -ETIMEDOUT;
+	}
+
+	dev_dbg(&pdev->dev, "check if have any previous PR error\n");
+	pr_err_handle(pdev, fme_pr);
+	return 0;
+}
+
+static int fme_pr_write(struct fpga_manager *mgr,
+			const char *buf, size_t count)
+{
+	struct fpga_fme *fme = mgr->priv;
+	struct platform_device *pdev;
+	struct feature_fme_pr *fme_pr;
+	struct feature_fme_pr_ctl fme_pr_ctl;
+	struct feature_fme_pr_status fme_pr_status;
+	struct feature_fme_pr_data fme_pr_data;
+	int ret = 0, delay = 0, pr_credit;
+
+	pdev = fme->pdata->dev;
+	fme_pr = get_feature_ioaddr_by_index(&pdev->dev,
+				FME_FEATURE_ID_PR_MGMT);
+
+	dev_dbg(&pdev->dev, "set PR port ID and start request\n");
+
+	fme_pr_ctl.csr = readq(&fme_pr->ccip_fme_pr_control);
+	fme_pr_ctl.pr_regionid = fme->port_id;
+	fme_pr_ctl.pr_start_req = 1;
+	writeq(fme_pr_ctl.csr, &fme_pr->ccip_fme_pr_control);
+
+	dev_dbg(&pdev->dev, "pushing data from bitstream to HW\n");
+
+	fme_pr_status.csr = readq(&fme_pr->ccip_fme_pr_status);
+	pr_credit = fme_pr_status.pr_credit;
+
+	if (fme->pr_avx512)
+		kernel_fpu_begin();
+
+	while (count > 0) {
+		while (pr_credit <= 1) {
+			if (delay++ > PR_WAIT_TIMEOUT) {
+				dev_err(&pdev->dev, "maximum try\n");
+
+				fme->pr_err = pr_err_handle(pdev, fme_pr);
+				ret = fme->pr_err ? -EIO : -ETIMEDOUT;
+				goto done;
+			}
+			udelay(1);
+
+			fme_pr_status.csr = readq(&fme_pr->ccip_fme_pr_status);
+			pr_credit = fme_pr_status.pr_credit;
+		};
+
+		if (count >= fme->pr_bandwidth) {
+			switch (fme->pr_bandwidth) {
+			case 4:
+				fme_pr_data.rsvd = 0;
+				fme_pr_data.pr_data_raw = *((u32 *)buf);
+				writeq(fme_pr_data.csr,
+				       &fme_pr->ccip_fme_pr_data);
+				break;
+			case 64:
+				copy512((void *)buf, &fme_pr->fme_pr_data1);
+				break;
+			default:
+				ret = -EFAULT;
+				goto done;
+			}
+
+			buf += fme->pr_bandwidth;
+			count -= fme->pr_bandwidth;
+			pr_credit--;
+		} else {
+			WARN_ON(1);
+			ret = -EINVAL;
+			goto done;
+		}
+	}
+
+done:
+	if (fme->pr_avx512)
+		kernel_fpu_end();
+
+	return ret;
+}
+
+static int fme_pr_write_complete(struct fpga_manager *mgr,
+			struct fpga_image_info *info)
+{
+	struct fpga_fme *fme = mgr->priv;
+	struct platform_device *pdev;
+	struct feature_fme_pr *fme_pr;
+	struct feature_fme_pr_ctl fme_pr_ctl;
+
+	pdev = fme->pdata->dev;
+	fme_pr = get_feature_ioaddr_by_index(&pdev->dev,
+				FME_FEATURE_ID_PR_MGMT);
+
+	fme_pr_ctl.csr = readq(&fme_pr->ccip_fme_pr_control);
+	fme_pr_ctl.pr_push_complete = 1;
+	writeq(fme_pr_ctl.csr, &fme_pr->ccip_fme_pr_control);
+
+	dev_dbg(&pdev->dev, "green bitstream push complete\n");
+	dev_dbg(&pdev->dev, "waiting for HW to release PR resource\n");
+
+	fme_pr_ctl.pr_start_req = 0;
+
+	if (fpga_wait_register_field(pr_start_req, fme_pr_ctl,
+		&fme_pr->ccip_fme_pr_control, PR_WAIT_TIMEOUT, 1)) {
+		dev_err(&pdev->dev, "maximum try.\n");
+		return -ETIMEDOUT;
+	}
+
+	dev_dbg(&pdev->dev, "PR operation complete, checking status\n");
+	fme->pr_err = pr_err_handle(pdev, fme_pr);
+	if (fme->pr_err)
+		return -EIO;
+
+	dev_dbg(&pdev->dev, "PR done successfully\n");
+	return 0;
+}
+
+static enum fpga_mgr_states fme_pr_state(struct fpga_manager *mgr)
+{
+	return FPGA_MGR_STATE_UNKNOWN;
+}
+
+static const struct fpga_manager_ops fme_pr_ops = {
+	.write_init = fme_pr_write_init,
+	.write = fme_pr_write,
+	.write_complete = fme_pr_write_complete,
+	.state = fme_pr_state,
+};
+
+static int fme_pr(struct platform_device *pdev, unsigned long arg)
+{
+	void __user *argp = (void __user *)arg;
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+	struct fpga_manager *mgr;
+	struct feature_fme_header *fme_hdr;
+	struct feature_fme_capability fme_capability;
+	struct fpga_image_info info;
+	struct fpga_fme_port_pr port_pr;
+	struct platform_device *port;
+	unsigned long minsz;
+	void *buf = NULL;
+	size_t length;
+	int ret = 0;
+
+	minsz = offsetofend(struct fpga_fme_port_pr, status);
+
+	if (copy_from_user(&port_pr, argp, minsz))
+		return -EFAULT;
+
+	if (port_pr.argsz < minsz || port_pr.flags)
+		return -EINVAL;
+
+	/* get fme header region */
+	fme_hdr = get_feature_ioaddr_by_index(&pdev->dev,
+					FME_FEATURE_ID_HEADER);
+	if (WARN_ON(!fme_hdr))
+		return -EINVAL;
+
+	/* check port id */
+	fme_capability.csr = readq(&fme_hdr->capability);
+	if (port_pr.port_id >= fme_capability.num_ports) {
+		dev_dbg(&pdev->dev, "port number more than maximum\n");
+		return -EINVAL;
+	}
+
+	if (!access_ok(VERIFY_READ, port_pr.buffer_address,
+				    port_pr.buffer_size))
+		return -EFAULT;
+
+	mutex_lock(&pdata->lock);
+	fme = fpga_pdata_get_private(pdata);
+	/* fme device has been unregistered. */
+	if (!fme) {
+		ret = -EINVAL;
+		goto unlock_exit;
+	}
+
+	/*
+	 * Padding extra zeros to align PR buffer with PR bandwidth, HW will
+	 * ignore these zeros automatically.
+	 */
+	length = ALIGN(port_pr.buffer_size, fme->pr_bandwidth);
+
+	buf = vzalloc(length);
+	if (!buf) {
+		ret = -ENOMEM;
+		goto unlock_exit;
+	}
+
+	if (copy_from_user(buf, (void __user *)port_pr.buffer_address,
+					       port_pr.buffer_size)) {
+		ret = -EFAULT;
+		goto free_exit;
+	}
+
+	memset(&info, 0, sizeof(struct fpga_image_info));
+	info.flags = FPGA_MGR_PARTIAL_RECONFIG;
+
+	mgr = fpga_mgr_get(&pdev->dev);
+	if (IS_ERR(mgr)) {
+		ret = PTR_ERR(mgr);
+		goto free_exit;
+	}
+
+	fme->pr_err = 0;
+	fme->port_id = port_pr.port_id;
+
+	/* Find and get port device by index */
+	port = pdata->fpga_for_each_port(pdev, &fme->port_id,
+					 fpga_port_check_id);
+	WARN_ON(!port);
+
+	/* Disable Port before PR */
+	fpga_port_disable(port);
+
+	ret = fpga_mgr_buf_load(mgr, &info, buf, length);
+	port_pr.status = fme->pr_err;
+
+	/* Re-enable Port after PR finished */
+	fpga_port_enable(port);
+
+	put_device(&port->dev);
+
+	fpga_mgr_put(mgr);
+free_exit:
+	vfree(buf);
+unlock_exit:
+	mutex_unlock(&pdata->lock);
+	if (copy_to_user((void __user *)arg, &port_pr, minsz))
+		return -EFAULT;
+	return ret;
+}
+
+static int fpga_fme_pr_probe(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct feature_fme_pr *fme_pr;
+	struct feature_header fme_pr_header;
+	struct fpga_fme *priv;
+	int ret;
+
+	mutex_lock(&pdata->lock);
+	priv = fpga_pdata_get_private(pdata);
+
+	fme_pr = get_feature_ioaddr_by_index(&pdev->dev,
+				FME_FEATURE_ID_PR_MGMT);
+
+	fme_pr_header.csr = readq(&fme_pr->header);
+	if (fme_pr_header.revision == 2) {
+		dev_dbg(&pdev->dev, "using 512-bit PR\n");
+		priv->pr_bandwidth = 64;
+		priv->pr_avx512 = true;
+	} else {
+		dev_dbg(&pdev->dev, "using 32-bit PR\n");
+		priv->pr_bandwidth = 4;
+	}
+
+	ret = fpga_mgr_register(&pdata->dev->dev,
+		"Intel FPGA Manager", &fme_pr_ops, priv);
+	mutex_unlock(&pdata->lock);
+
+	return ret;
+}
+
+static int fpga_fme_pr_remove(struct platform_device *pdev)
+{
+	fpga_mgr_unregister(&pdev->dev);
+	return 0;
+}
+
+static int pr_mgmt_init(struct platform_device *pdev, struct feature *feature)
+{
+	int ret;
+
+	ret = fpga_fme_pr_probe(pdev);
+	if (ret)
+		return ret;
+
+	ret = sysfs_create_group(&pdev->dev.kobj, &pr_mgmt_attr_group);
+	if (ret)
+		fpga_fme_pr_remove(pdev);
+
+	return ret;
+}
+
+static void pr_mgmt_uinit(struct platform_device *pdev, struct feature *feature)
+{
+	sysfs_remove_group(&pdev->dev.kobj, &pr_mgmt_attr_group);
+	fpga_fme_pr_remove(pdev);
+}
+
+static long fme_pr_ioctl(struct platform_device *pdev, struct feature *feature,
+	unsigned int cmd, unsigned long arg)
+{
+	long ret;
+
+	switch (cmd) {
+	case FPGA_FME_PORT_PR:
+		ret = fme_pr(pdev, arg);
+		break;
+	default:
+		ret = -ENODEV;
+	}
+
+	return ret;
+}
+
+struct feature_ops pr_mgmt_ops = {
+	.init = pr_mgmt_init,
+	.uinit = pr_mgmt_uinit,
+	.ioctl = fme_pr_ioctl,
+};
diff --git a/drivers/fpga/intel/fme.h b/drivers/fpga/intel/fme.h
new file mode 100644
index 000000000000..dd988474912c
--- /dev/null
+++ b/drivers/fpga/intel/fme.h
@@ -0,0 +1,88 @@
+/*
+ * FPGA Management Engine Drier Header
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#ifndef __INTEL_FME_PR_H
+#define __INTEL_FME_PR_H
+
+#define PERF_OBJ_ROOT_ID	(~0)
+
+struct perf_object {
+	/*
+	 * instance id. PERF_OBJ_ROOT_ID indicates it is a parent
+	 * object which counts performance counters for all instances.
+	 */
+	int id;
+
+	/* the sysfs files are associated with this object. */
+	const struct attribute_group **attr_groups;
+
+	/* the fme feature device. */
+	struct device *fme_dev;
+
+	/*
+	 * they are used to construct parent-children hierarchy.
+	 *
+	 * 'node' is used to link itself to parent's children list.
+	 * 'children' is used to link its children objects together.
+	 */
+	struct list_head node;
+	struct list_head children;
+
+	struct kobject kobj;
+};
+
+struct fpga_fme {
+	u8  port_id;
+	u64 pr_err;
+	u32 capability;
+	int pr_bandwidth;
+	bool pr_avx512;
+	struct device *dev_err;
+	struct perf_object *iperf_dev;
+	struct perf_object *dperf_dev;
+	struct feature_platform_data *pdata;
+};
+
+struct perf_obj_attributte {
+	struct attribute attr;
+	ssize_t (*show)(struct perf_object *pobj, char *buf);
+	ssize_t (*store)(struct perf_object *pobj,
+			 const char *buf, size_t n);
+};
+
+#define to_perf_obj_attr(_attr)					\
+		container_of(_attr, struct perf_obj_attributte, attr)
+#define to_perf_obj(_kobj)					\
+		container_of(_kobj, struct perf_object, kobj)
+
+#define PERF_OBJ_ATTR(_name, _filename, _mode, _show, _store)	\
+struct perf_obj_attributte perf_obj_attr_##_name =		\
+	__ATTR(_filename, _mode, _show, _store)
+#define PERF_OBJ_ATTR_RW(_name)					\
+	struct perf_obj_attributte perf_obj_attr_##_name = __ATTR_RW(_name)
+#define PERF_OBJ_ATTR_RO(_name)					\
+	struct perf_obj_attributte perf_obj_attr_##_name = __ATTR_RO(_name)
+#define PERF_OBJ_ATTR_WO(_name)					\
+	struct perf_obj_attributte perf_obj_attr_##_name = __ATTR_WO(_name)
+
+extern struct feature_ops global_error_ops;
+extern struct feature_ops pr_mgmt_ops;
+extern struct feature_ops global_iperf_ops;
+extern struct feature_ops global_dperf_ops;
+#endif
diff --git a/drivers/fpga/intel/pcie.c b/drivers/fpga/intel/pcie.c
new file mode 100644
index 000000000000..231e6886f8d7
--- /dev/null
+++ b/drivers/fpga/intel/pcie.c
@@ -0,0 +1,1404 @@
+/*
+ * Driver for the PCIe device which locates between CPU and accelerated
+ * function units(AFUs) and allows them to communicate with each other.
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Zhang Yi <Yi.Z.Zhang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include <linux/pci.h>
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/stddef.h>
+#include <linux/errno.h>
+#include <linux/aer.h>
+#include <linux/uuid.h>
+#include <linux/kdev_t.h>
+#include <linux/mfd/core.h>
+
+#include "altera-asmip2.h"
+#include "feature-dev.h"
+
+#define DRV_VERSION	"EXPERIMENTAL VERSION"
+#define DRV_NAME	"intel-fpga-pci"
+
+static DEFINE_MUTEX(fpga_id_mutex);
+
+enum fpga_id_type {
+	/*
+	 * fpga parent device id allocation and mapping, parent device
+	 * is the container of fme device and port device
+	 */
+	PARENT_ID,
+	/* fme id allocation and mapping */
+	FME_ID,
+	/* port id allocation and mapping */
+	PORT_ID,
+	FPGA_ID_MAX,
+};
+
+/* it is protected by fpga_id_mutex */
+static struct idr fpga_ids[FPGA_ID_MAX];
+
+struct cci_drvdata {
+	int device_id;
+	struct device *fme_dev;
+
+	struct mutex lock;
+	struct list_head port_dev_list;
+	/* number of released ports which can be configured as VF  */
+	int released_port_num;
+
+	struct list_head regions;
+
+};
+
+struct cci_pci_region {
+	int bar;
+	void __iomem *ioaddr;
+
+	struct list_head node;
+};
+
+static void fpga_ids_init(void)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(fpga_ids); i++)
+		idr_init(fpga_ids + i);
+}
+
+static void fpga_ids_destroy(void)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(fpga_ids); i++)
+		idr_destroy(fpga_ids + i);
+}
+
+static int alloc_fpga_id(enum fpga_id_type type, struct device *dev)
+{
+	int id;
+
+	WARN_ON(type >= FPGA_ID_MAX);
+	mutex_lock(&fpga_id_mutex);
+	id = idr_alloc(fpga_ids + type, dev, 0, 0, GFP_KERNEL);
+	mutex_unlock(&fpga_id_mutex);
+	return id;
+}
+
+static void free_fpga_id(enum fpga_id_type type, int id)
+{
+	WARN_ON(type >= FPGA_ID_MAX);
+	mutex_lock(&fpga_id_mutex);
+	idr_remove(fpga_ids + type, id);
+	mutex_unlock(&fpga_id_mutex);
+}
+
+static void cci_pci_add_port_dev(struct pci_dev *pdev,
+				 struct platform_device *port_dev)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pdev->dev);
+	struct feature_platform_data *pdata = dev_get_platdata(&port_dev->dev);
+
+	mutex_lock(&drvdata->lock);
+	list_add(&pdata->node, &drvdata->port_dev_list);
+	get_device(&pdata->dev->dev);
+	mutex_unlock(&drvdata->lock);
+}
+
+static void cci_pci_remove_port_devs(struct pci_dev *pdev)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pdev->dev);
+	struct feature_platform_data *pdata, *ptmp;
+
+	mutex_lock(&drvdata->lock);
+	list_for_each_entry_safe(pdata, ptmp, &drvdata->port_dev_list, node) {
+		struct platform_device *port_dev = pdata->dev;
+
+		/* the port should be unregistered first. */
+		WARN_ON(device_is_registered(&port_dev->dev));
+		list_del(&pdata->node);
+		free_fpga_id(PORT_ID, port_dev->id);
+		put_device(&port_dev->dev);
+	}
+	mutex_unlock(&drvdata->lock);
+}
+
+static struct platform_device *cci_pci_lookup_port_by_id(struct pci_dev *pdev,
+							 int port_id)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pdev->dev);
+	struct feature_platform_data *pdata;
+
+	list_for_each_entry(pdata, &drvdata->port_dev_list, node)
+		if (fpga_port_check_id(pdata->dev, &port_id))
+			return pdata->dev;
+
+	return NULL;
+}
+
+/* info collection during feature dev build. */
+struct build_feature_devs_info {
+	struct pci_dev *pdev;
+
+	/*
+	 * PCI BAR mapping info. Parsing feature list starts from
+	 * BAR 0 and switch to different BARs to parse Port
+	 */
+	void __iomem *ioaddr;
+	void __iomem *ioend;
+	int current_bar;
+
+	/* points to FME header where the port offset is figured out. */
+	void __iomem *pfme_hdr;
+
+	/* the container device for all feature devices */
+	struct device *parent_dev;
+
+	/* current feature device */
+	struct platform_device *feature_dev;
+};
+
+static void cci_pci_release_regions(struct pci_dev *pdev)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pdev->dev);
+	struct cci_pci_region *tmp, *region;
+
+	list_for_each_entry_safe(region, tmp, &drvdata->regions, node) {
+		list_del(&region->node);
+		if (region->ioaddr)
+			pci_iounmap(pdev, region->ioaddr);
+		devm_kfree(&pdev->dev, region);
+	}
+}
+
+static void __iomem *cci_pci_ioremap_bar(struct pci_dev *pdev, int bar)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pdev->dev);
+	struct cci_pci_region *region;
+
+	list_for_each_entry(region, &drvdata->regions, node)
+		if (region->bar == bar) {
+			dev_dbg(&pdev->dev, "BAR %d region exists\n", bar);
+			return region->ioaddr;
+		}
+
+	region = devm_kzalloc(&pdev->dev, sizeof(*region), GFP_KERNEL);
+	if (!region)
+		return NULL;
+
+	region->bar = bar;
+	region->ioaddr = pci_ioremap_bar(pdev, bar);
+	if (!region->ioaddr) {
+		dev_err(&pdev->dev, "can't ioremap memory from BAR %d.\n", bar);
+		devm_kfree(&pdev->dev, region);
+		return NULL;
+	}
+
+	list_add(&region->node, &drvdata->regions);
+	return region->ioaddr;
+}
+
+static int parse_start_from(struct build_feature_devs_info *binfo, int bar)
+{
+	binfo->ioaddr = cci_pci_ioremap_bar(binfo->pdev, bar);
+	if (!binfo->ioaddr)
+		return -ENOMEM;
+
+	binfo->current_bar = bar;
+	binfo->ioend = binfo->ioaddr + pci_resource_len(binfo->pdev, bar);
+	return 0;
+}
+
+static int parse_start(struct build_feature_devs_info *binfo)
+{
+	/* fpga feature list starts from BAR 0 */
+	return parse_start_from(binfo, 0);
+}
+
+/* switch the memory mapping to BAR# @bar */
+static int parse_switch_to(struct build_feature_devs_info *binfo, int bar)
+{
+	return parse_start_from(binfo, bar);
+}
+
+static int attach_port_dev(struct platform_device *pdev, int port_id)
+{
+	struct feature_fme_header *fme_hdr;
+	struct feature_fme_port port;
+	struct device *pci_dev = fpga_feature_dev_to_pcidev(pdev);
+	struct cci_drvdata *drvdata = dev_get_drvdata(pci_dev);
+	struct platform_device *port_dev;
+	int ret;
+
+	fme_hdr = get_feature_ioaddr_by_index(&pdev->dev,
+					      FME_FEATURE_ID_HEADER);
+
+	mutex_lock(&drvdata->lock);
+	port.csr = readq(&fme_hdr->port[port_id]);
+	if (port.afu_access_control == FME_AFU_ACCESS_VF) {
+		dev_dbg(&pdev->dev, "port_%d has already been turned to VF.\n",
+			port_id);
+		mutex_unlock(&drvdata->lock);
+		return -EBUSY;
+	}
+
+	port_dev = cci_pci_lookup_port_by_id(to_pci_dev(pci_dev), port_id);
+	if (!port_dev) {
+		dev_err(&pdev->dev, "port_%d is not detected.\n", port_id);
+		ret = -EINVAL;
+		goto exit;
+	}
+
+	if (device_is_registered(&port_dev->dev)) {
+		dev_dbg(pci_dev, "port_%d is not released.\n", port_id);
+		ret = -EBUSY;
+		goto exit;
+	}
+
+	dev_dbg(pci_dev, "now re-assign port_%d:%s\n", port_id, port_dev->name);
+
+	ret = platform_device_add(port_dev);
+	if (ret)
+		goto exit;
+
+	get_device(&port_dev->dev);
+	feature_dev_use_end(dev_get_platdata(&port_dev->dev));
+	drvdata->released_port_num--;
+exit:
+	mutex_unlock(&drvdata->lock);
+	return ret;
+}
+
+static int detach_port_dev(struct platform_device *pdev, int port_id)
+{
+	struct device *dev = fpga_feature_dev_to_pcidev(pdev);
+	struct cci_drvdata *drvdata = dev_get_drvdata(dev);
+	struct platform_device *port_dev;
+	int ret;
+
+	mutex_lock(&drvdata->lock);
+	port_dev = cci_pci_lookup_port_by_id(to_pci_dev(dev), port_id);
+	if (!port_dev) {
+		dev_err(&pdev->dev, "port_%d is not detected.\n", port_id);
+		ret = -EINVAL;
+		goto exit;
+	}
+
+	if (!device_is_registered(&port_dev->dev)) {
+		dev_dbg(&pdev->dev,
+		   "port_%d is released or already assigned a VF.\n", port_id);
+		ret = -EBUSY;
+		goto exit;
+	}
+
+	ret = feature_dev_use_excl_begin(dev_get_platdata(&port_dev->dev));
+	if (ret)
+		goto exit;
+
+	platform_device_del(port_dev);
+	put_device(&port_dev->dev);
+	drvdata->released_port_num++;
+exit:
+	mutex_unlock(&drvdata->lock);
+	return ret;
+}
+
+static int
+config_port(struct platform_device *pdev, u32 port_id, bool release)
+{
+	/* Todo: some potential check */
+	if (release)
+		return detach_port_dev(pdev, port_id);
+
+	return attach_port_dev(pdev, port_id);
+}
+
+static struct platform_device *fpga_for_each_port(struct platform_device *pdev,
+		     void *data, int (*match)(struct platform_device *, void *))
+{
+	struct device *pci_dev = fpga_feature_dev_to_pcidev(pdev);
+	struct cci_drvdata *drvdata = dev_get_drvdata(pci_dev);
+	struct feature_platform_data *pdata;
+	struct platform_device *port_dev;
+
+	mutex_lock(&drvdata->lock);
+	list_for_each_entry(pdata, &drvdata->port_dev_list, node) {
+		port_dev = pdata->dev;
+
+		if (match(port_dev, data) && get_device(&port_dev->dev))
+			goto exit;
+	}
+	port_dev = NULL;
+exit:
+	mutex_unlock(&drvdata->lock);
+	return port_dev;
+}
+
+static struct build_feature_devs_info *
+build_info_alloc_and_init(struct pci_dev *pdev)
+{
+	struct build_feature_devs_info *binfo;
+
+	binfo = devm_kzalloc(&pdev->dev, sizeof(*binfo), GFP_KERNEL);
+	if (binfo)
+		binfo->pdev = pdev;
+
+	return binfo;
+}
+
+static enum fpga_id_type feature_dev_id_type(struct platform_device *pdev)
+{
+	if (!strcmp(pdev->name, FPGA_FEATURE_DEV_FME))
+		return FME_ID;
+
+	if (!strcmp(pdev->name, FPGA_FEATURE_DEV_PORT))
+		return PORT_ID;
+
+	WARN_ON(1);
+	return FPGA_ID_MAX;
+}
+
+/*
+ * register current feature device, it is called when we need to switch to
+ * another feature parsing or we have parsed all features
+ */
+static int build_info_commit_dev(struct build_feature_devs_info *binfo)
+{
+	int ret;
+
+	if (!binfo->feature_dev)
+		return 0;
+
+	ret = platform_device_add(binfo->feature_dev);
+	if (!ret) {
+		struct cci_drvdata *drvdata;
+
+		drvdata = dev_get_drvdata(&binfo->pdev->dev);
+		if (feature_dev_id_type(binfo->feature_dev) == PORT_ID)
+			cci_pci_add_port_dev(binfo->pdev, binfo->feature_dev);
+		else
+			drvdata->fme_dev = get_device(&binfo->feature_dev->dev);
+
+		/*
+		 * reset it to avoid build_info_free() freeing their resource.
+		 *
+		 * The resource of successfully registered feature devices
+		 * will be freed by platform_device_unregister(). See the
+		 * comments in build_info_create_dev().
+		 */
+		binfo->feature_dev = NULL;
+	}
+
+	return ret;
+}
+
+static int
+build_info_create_dev(struct build_feature_devs_info *binfo,
+		      enum fpga_id_type type, int feature_nr, const char *name)
+{
+	struct platform_device *fdev;
+	struct resource *res;
+	struct feature_platform_data *pdata;
+	enum fpga_devt_type devt_type = FPGA_DEVT_FME;
+	int ret;
+
+	if (type == PORT_ID)
+		devt_type = FPGA_DEVT_PORT;
+
+	/* we will create a new device, commit current device first */
+	ret = build_info_commit_dev(binfo);
+	if (ret)
+		return ret;
+
+	/*
+	 * we use -ENODEV as the initialization indicator which indicates
+	 * whether the id need to be reclaimed
+	 */
+	fdev = binfo->feature_dev = platform_device_alloc(name, -ENODEV);
+	if (!fdev)
+		return -ENOMEM;
+
+	fdev->id = alloc_fpga_id(type, &fdev->dev);
+	if (fdev->id < 0)
+		return fdev->id;
+
+	fdev->dev.parent = binfo->parent_dev;
+	fdev->dev.devt = fpga_get_devt(devt_type, fdev->id);
+
+	/*
+	 * we need not care the memory which is associated with the
+	 * platform device. After call platform_device_unregister(),
+	 * it will be automatically freed by device's
+	 * release() callback, platform_device_release().
+	 */
+	pdata = feature_platform_data_alloc_and_init(fdev, feature_nr);
+	if (!pdata)
+		return -ENOMEM;
+
+	if (type == FME_ID) {
+		pdata->config_port = config_port;
+		pdata->fpga_for_each_port = fpga_for_each_port;
+	}
+
+	/*
+	 * the count should be initialized to 0 to make sure
+	 *__fpga_port_enable() following __fpga_port_disable()
+	 * works properly.
+	 */
+	WARN_ON(pdata->disable_count);
+
+	fdev->dev.platform_data = pdata;
+	fdev->num_resources = feature_nr;
+	fdev->resource = kcalloc(feature_nr, sizeof(*res), GFP_KERNEL);
+	if (!fdev->resource)
+		return -ENOMEM;
+
+	return 0;
+}
+
+static int remove_feature_dev(struct device *dev, void *data)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+
+	platform_device_unregister(pdev);
+	return 0;
+}
+
+static int remove_parent_dev(struct device *dev, void *data)
+{
+	/* remove platform devices attached in the parent device */
+	device_for_each_child(dev, NULL, remove_feature_dev);
+	device_unregister(dev);
+	return 0;
+}
+
+static void remove_all_devs(struct pci_dev *pdev)
+{
+	/* remove parent device and all its children. */
+	device_for_each_child(&pdev->dev, NULL, remove_parent_dev);
+}
+
+static void build_info_free(struct build_feature_devs_info *binfo)
+{
+	if (!IS_ERR_OR_NULL(binfo->parent_dev))
+		remove_all_devs(binfo->pdev);
+
+	/*
+	 * it is a valid id, free it. See comments in
+	 * build_info_create_dev()
+	 */
+	if (binfo->feature_dev && binfo->feature_dev->id >= 0)
+		free_fpga_id(feature_dev_id_type(binfo->feature_dev),
+			     binfo->feature_dev->id);
+
+	platform_device_put(binfo->feature_dev);
+
+	devm_kfree(&binfo->pdev->dev, binfo);
+}
+
+/*
+ * UAFU GUID is dynamic as it can be changed after FME downloads different
+ * Green Bitstream to the port, so we treat the unknown GUIDs which are
+ * attached on port's feature list as UAFU.
+ */
+static bool feature_is_UAFU(struct build_feature_devs_info *binfo)
+{
+	if (!binfo->feature_dev ||
+	      feature_dev_id_type(binfo->feature_dev) != PORT_ID)
+		return false;
+
+	return true;
+}
+
+static int
+build_info_add_sub_feature(struct build_feature_devs_info *binfo,
+			   int feature_id, const char *feature_name,
+			   resource_size_t resource_size, void __iomem *start,
+			   unsigned int vec_start, unsigned int vec_cnt)
+{
+	struct platform_device *fdev = binfo->feature_dev;
+	struct feature_platform_data *pdata = dev_get_platdata(&fdev->dev);
+	struct resource *res = &fdev->resource[feature_id];
+	struct feature_irq_ctx *ctx = NULL;
+	int i;
+
+	res->start = pci_resource_start(binfo->pdev, binfo->current_bar) +
+		start - binfo->ioaddr;
+	res->end = res->start + resource_size - 1;
+	res->flags = IORESOURCE_MEM;
+	res->name = feature_name;
+
+	/*
+	 * Add interrupt information for the feature which support interrupt.
+	 */
+	if (vec_cnt) {
+		if (vec_start + vec_cnt > pci_msix_vec_count(binfo->pdev))
+			return -EINVAL;
+
+		ctx = devm_kcalloc(&binfo->pdev->dev, vec_cnt,
+						sizeof(*ctx), GFP_KERNEL);
+		if (!ctx)
+			return -ENOMEM;
+
+		for (i = 0; i < vec_cnt; i++)
+			ctx[i].irq = pci_irq_vector(binfo->pdev, vec_start + i);
+	}
+
+	feature_platform_data_add(pdata, feature_id, feature_name, feature_id,
+				  start, ctx, vec_cnt);
+
+	return 0;
+}
+
+struct feature_info {
+	const char *name;
+	resource_size_t resource_size;
+	int feature_index;
+	int revision_id;
+	unsigned int vec_start;
+	unsigned int vec_cnt;
+};
+
+/* indexed by fme feature IDs which are defined in 'enum fme_feature_id'. */
+static struct feature_info fme_features[] = {
+	{
+		.name = FME_FEATURE_HEADER,
+		.resource_size = sizeof(struct feature_fme_header),
+		.feature_index = FME_FEATURE_ID_HEADER,
+		.revision_id = FME_HEADER_REVISION
+	},
+	{
+		.name = FME_FEATURE_THERMAL_MGMT,
+		.resource_size = sizeof(struct feature_fme_thermal),
+		.feature_index = FME_FEATURE_ID_THERMAL_MGMT,
+		.revision_id = FME_THERMAL_MGMT_REVISION
+	},
+	{
+		.name = FME_FEATURE_POWER_MGMT,
+		.resource_size = sizeof(struct feature_fme_power),
+		.feature_index = FME_FEATURE_ID_POWER_MGMT,
+		.revision_id = FME_POWER_MGMT_REVISION
+	},
+	{
+		.name = FME_FEATURE_GLOBAL_IPERF,
+		.resource_size = sizeof(struct feature_fme_iperf),
+		.feature_index = FME_FEATURE_ID_GLOBAL_IPERF,
+		.revision_id = FME_GLOBAL_IPERF_REVISION
+	},
+	{
+		.name = FME_FEATURE_GLOBAL_ERR,
+		.resource_size = sizeof(struct feature_fme_err),
+		.feature_index = FME_FEATURE_ID_GLOBAL_ERR,
+		.revision_id = FME_GLOBAL_ERR_REVISION
+	},
+	{
+		.name = FME_FEATURE_PR_MGMT,
+		.resource_size = sizeof(struct feature_fme_pr),
+		.feature_index = FME_FEATURE_ID_PR_MGMT,
+		.revision_id = FME_PR_MGMT_REVISION
+	},
+	{
+		.name = FME_FEATURE_HSSI_ETH,
+		.resource_size = sizeof(struct feature_fme_hssi),
+		.feature_index = FME_FEATURE_ID_HSSI_ETH,
+		.revision_id = FME_HSSI_ETH_REVISION
+	},
+	{
+		.name = FME_FEATURE_GLOBAL_DPERF,
+		.resource_size = sizeof(struct feature_fme_dperf),
+		.feature_index = FME_FEATURE_ID_GLOBAL_DPERF,
+		.revision_id = FME_GLOBAL_DPERF_REVISION
+	},
+	{
+		.name = FME_FEATURE_QSPI_FLASH,
+		.resource_size = ALTERA_ASMIP2_RESOURCE_SIZE,
+		.feature_index = FME_FEATURE_ID_QSPI_FLASH,
+		.revision_id = FME_QSPI_REVISION
+	}
+};
+
+static struct feature_info port_features[] = {
+	{
+		.name = PORT_FEATURE_HEADER,
+		.resource_size = sizeof(struct feature_port_header),
+		.feature_index = PORT_FEATURE_ID_HEADER,
+		.revision_id = PORT_HEADER_REVISION
+	},
+	{
+		.name = PORT_FEATURE_ERR,
+		.resource_size = sizeof(struct feature_port_error),
+		.feature_index = PORT_FEATURE_ID_ERROR,
+		.revision_id = PORT_ERR_REVISION
+	},
+	{
+		.name = PORT_FEATURE_UMSG,
+		.resource_size = sizeof(struct feature_port_umsg),
+		.feature_index = PORT_FEATURE_ID_UMSG,
+		.revision_id = PORT_UMSG_REVISION
+	},
+	{
+		.name = PORT_FEATURE_UINT,
+		.resource_size = sizeof(struct feature_port_uint),
+		.feature_index = PORT_FEATURE_ID_UINT,
+		.revision_id = PORT_UINT_REVISION
+	},
+	{
+		.name = PORT_FEATURE_STP,
+		.resource_size = PORT_FEATURE_STP_REGION_SIZE,
+		.feature_index = PORT_FEATURE_ID_STP,
+		.revision_id = PORT_STP_REVISION
+	},
+	{
+		.name = PORT_FEATURE_UAFU,
+		/* UAFU feature size should be read from PORT_CAP.MMIOSIZE.
+		 * Will set uafu feature size while parse port device.
+		 */
+		.resource_size = 0,
+		.feature_index = PORT_FEATURE_ID_UAFU,
+		.revision_id = PORT_UAFU_REVISION
+	},
+};
+
+static int
+create_feature_instance(struct build_feature_devs_info *binfo,
+			void __iomem *start, struct feature_info *finfo)
+{
+	struct feature_header *hdr = start;
+
+	if (binfo->ioend - start < finfo->resource_size)
+		return -EINVAL;
+
+	if (finfo->revision_id != SKIP_REVISION_CHECK
+		&& hdr->revision > finfo->revision_id) {
+		dev_dbg(&binfo->pdev->dev,
+		"feature %s revision :default:%x, now at:%x, mis-match.\n",
+		finfo->name, finfo->revision_id, hdr->revision);
+	}
+
+	return build_info_add_sub_feature(binfo, finfo->feature_index,
+			finfo->name, finfo->resource_size, start,
+			finfo->vec_start, finfo->vec_cnt);
+}
+
+static int parse_feature_fme(struct build_feature_devs_info *binfo,
+			     void __iomem *start)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&binfo->pdev->dev);
+	int ret;
+
+	ret = build_info_create_dev(binfo, FME_ID, fme_feature_num(),
+					FPGA_FEATURE_DEV_FME);
+	if (ret)
+		return ret;
+
+	if (drvdata->fme_dev) {
+		dev_err(&binfo->pdev->dev, "Multiple FMEs are detected.\n");
+		return -EINVAL;
+	}
+
+	return create_feature_instance(binfo, start,
+				       &fme_features[FME_FEATURE_ID_HEADER]);
+}
+
+static void parse_feature_irqs(struct build_feature_devs_info *binfo,
+			       void __iomem *start, struct feature_info *finfo)
+{
+	finfo->vec_start = 0;
+	finfo->vec_cnt = 0;
+
+	if (!strcmp(finfo->name, PORT_FEATURE_UINT)) {
+		struct feature_port_uint *port_uint = start;
+		struct feature_port_uint_cap uint_cap;
+
+		uint_cap.csr = readq(&port_uint->capability);
+		if (uint_cap.intr_num) {
+			finfo->vec_start = uint_cap.first_vec_num;
+			finfo->vec_cnt = uint_cap.intr_num;
+		} else
+			dev_dbg(&binfo->pdev->dev, "UAFU doesn't support interrupt\n");
+
+	} else if (!strcmp(finfo->name, PORT_FEATURE_ERR)) {
+		struct feature_port_error *port_err = start;
+		struct feature_port_err_capability port_err_cap;
+
+		port_err_cap.csr = readq(&port_err->error_capability);
+		if (port_err_cap.support_intr) {
+			finfo->vec_start = port_err_cap.intr_vector_num;
+			finfo->vec_cnt = 1;
+		} else
+			dev_dbg(&binfo->pdev->dev, "Port error doesn't support interrupt\n");
+
+	} else if (!strcmp(finfo->name, FME_FEATURE_GLOBAL_ERR)) {
+		struct feature_fme_err *fme_err = start;
+		struct feature_fme_error_capability fme_err_cap;
+
+		fme_err_cap.csr = readq(&fme_err->fme_err_capability);
+		if (fme_err_cap.support_intr) {
+			finfo->vec_start = fme_err_cap.intr_vector_num;
+			finfo->vec_cnt = 1;
+		} else
+			dev_dbg(&binfo->pdev->dev, "FME error doesn't support interrupt\n");
+	}
+}
+
+static int parse_feature_fme_private(struct build_feature_devs_info *binfo,
+				     struct feature_header *hdr)
+{
+	struct feature_header header;
+
+	header.csr = readq(hdr);
+
+	if (header.id >= ARRAY_SIZE(fme_features)) {
+		dev_info(&binfo->pdev->dev, "FME feature id %x is not supported yet.\n",
+			 header.id);
+		return 0;
+	}
+
+	parse_feature_irqs(binfo, hdr, &fme_features[header.id]);
+
+	check_features_header(binfo->pdev, hdr, FPGA_DEVT_FME, header.id);
+
+	return create_feature_instance(binfo, hdr, &fme_features[header.id]);
+}
+
+static int parse_feature_port(struct build_feature_devs_info *binfo,
+			     void __iomem *start)
+{
+	int ret;
+
+	ret = build_info_create_dev(binfo, PORT_ID, port_feature_num(),
+					FPGA_FEATURE_DEV_PORT);
+	if (ret)
+		return ret;
+
+	return create_feature_instance(binfo, start,
+				       &port_features[PORT_FEATURE_ID_HEADER]);
+}
+
+static void enable_port_uafu(struct build_feature_devs_info *binfo,
+			     void __iomem *start)
+{
+	enum port_feature_id id = PORT_FEATURE_ID_UAFU;
+	struct feature_port_header *port_hdr;
+	struct feature_port_capability capability;
+	struct feature_port_control control;
+
+	port_hdr = (struct feature_port_header *)start;
+	capability.csr = readq(&port_hdr->capability);
+	control.csr = readq(&port_hdr->control);
+	port_features[id].resource_size = capability.mmio_size << 10;
+
+	/*
+	 * From SAS spec, to Enable UAFU, we should reset related port,
+	 * or the whole mmio space in this UAFU will be invalid
+	 */
+	if (port_features[id].resource_size)
+		fpga_port_reset(binfo->feature_dev);
+}
+
+static int parse_feature_port_private(struct build_feature_devs_info *binfo,
+				      struct feature_header *hdr)
+{
+	struct feature_header header;
+	enum port_feature_id id;
+
+	header.csr = readq(hdr);
+	/*
+	 * the region of port feature id is [0x10, 0x13], + 1 to reserve 0
+	 * which is dedicated for port-hdr.
+	 */
+	id = (header.id & 0x000f) + 1;
+
+	if (id >= ARRAY_SIZE(port_features)) {
+		dev_info(&binfo->pdev->dev, "Port feature id %x is not supported yet.\n",
+			 header.id);
+		return 0;
+	}
+
+	parse_feature_irqs(binfo, hdr, &port_features[id]);
+
+	check_features_header(binfo->pdev, hdr, FPGA_DEVT_PORT, id);
+
+	return create_feature_instance(binfo, hdr, &port_features[id]);
+}
+
+static int parse_feature_port_uafu(struct build_feature_devs_info *binfo,
+				 struct feature_header *hdr)
+{
+	enum port_feature_id id = PORT_FEATURE_ID_UAFU;
+	int ret;
+
+	if (port_features[id].resource_size) {
+		ret = create_feature_instance(binfo, hdr, &port_features[id]);
+		port_features[id].resource_size = 0;
+	} else {
+		dev_err(&binfo->pdev->dev, "the uafu feature header is mis-configured.\n");
+		ret = -EINVAL;
+	}
+
+	return ret;
+}
+
+static int parse_feature_afus(struct build_feature_devs_info *binfo,
+			      struct feature_header *hdr)
+{
+	int ret;
+	struct feature_afu_header *afu_hdr, header;
+	void __iomem *start;
+	void __iomem *end = binfo->ioend;
+
+	start = hdr;
+	for (; start < end; start += header.next_afu) {
+		if (end - start < (sizeof(*afu_hdr) + sizeof(*hdr)))
+			return -EINVAL;
+
+		hdr = start;
+		afu_hdr = (struct feature_afu_header *) (hdr + 1);
+		header.csr = readq(&afu_hdr->csr);
+
+		if (feature_is_UAFU(binfo)) {
+			ret = parse_feature_port_uafu(binfo, hdr);
+			if (ret)
+				return ret;
+		}
+
+		if (!header.next_afu)
+			break;
+	}
+
+	return 0;
+}
+
+static int parse_feature_fiu(struct build_feature_devs_info *binfo,
+			     struct feature_header *hdr)
+{
+	struct feature_header header;
+	struct feature_fiu_header *fiu_hdr, fiu_header;
+	void __iomem *start = hdr;
+	int ret;
+
+	header.csr = readq(hdr);
+
+	switch (header.id) {
+	case FEATURE_FIU_ID_FME:
+		ret = parse_feature_fme(binfo, hdr);
+		check_features_header(binfo->pdev, hdr, FPGA_DEVT_FME, 0);
+		binfo->pfme_hdr = hdr;
+		if (ret)
+			return ret;
+		break;
+	case FEATURE_FIU_ID_PORT:
+		ret = parse_feature_port(binfo, hdr);
+		check_features_header(binfo->pdev, hdr, FPGA_DEVT_PORT, 0);
+		enable_port_uafu(binfo, hdr);
+		if (ret)
+			return ret;
+
+		/* Check Port FIU's next_afu pointer to User AFU DFH */
+		fiu_hdr = (struct feature_fiu_header *) (hdr + 1);
+		fiu_header.csr = readq(&fiu_hdr->csr);
+
+		if (fiu_header.next_afu) {
+			start += fiu_header.next_afu;
+			ret = parse_feature_afus(binfo, start);
+			if (ret)
+				return ret;
+		} else
+			dev_dbg(&binfo->pdev->dev, "No AFUs detected on Port\n");
+		break;
+	default:
+		dev_info(&binfo->pdev->dev, "FIU TYPE %d is not supported yet.\n",
+			 header.id);
+	}
+
+	return 0;
+}
+
+static int parse_feature_private(struct build_feature_devs_info *binfo,
+				 struct feature_header *hdr)
+{
+	struct feature_header header;
+
+	header.csr = readq(hdr);
+
+	if (!binfo->feature_dev) {
+		dev_err(&binfo->pdev->dev, "the private feature %x does not belong to any AFU.\n",
+			header.id);
+		return -EINVAL;
+	}
+
+	switch (feature_dev_id_type(binfo->feature_dev)) {
+	case FME_ID:
+		return parse_feature_fme_private(binfo, hdr);
+	case PORT_ID:
+		return parse_feature_port_private(binfo, hdr);
+	default:
+		dev_info(&binfo->pdev->dev, "private feature %x belonging to AFU %s is not supported yet.\n",
+			 header.id, binfo->feature_dev->name);
+	}
+	return 0;
+}
+
+static int parse_feature(struct build_feature_devs_info *binfo,
+			 struct feature_header *hdr)
+{
+	struct feature_header header;
+	int ret = 0;
+
+	header.csr = readq(hdr);
+
+	switch (header.type) {
+	case FEATURE_TYPE_AFU:
+		ret = parse_feature_afus(binfo, hdr);
+		break;
+	case FEATURE_TYPE_PRIVATE:
+		ret = parse_feature_private(binfo, hdr);
+		break;
+	case FEATURE_TYPE_FIU:
+		ret = parse_feature_fiu(binfo, hdr);
+		break;
+	default:
+		dev_info(&binfo->pdev->dev,
+			 "Feature Type %x is not supported.\n", hdr->type);
+	};
+
+	return ret;
+}
+
+static int
+parse_feature_list(struct build_feature_devs_info *binfo, void __iomem *start)
+{
+	struct feature_header *hdr, header;
+	void __iomem *end = binfo->ioend;
+	int ret = 0;
+
+	for (; start < end; start += header.next_header_offset) {
+		if (end - start < sizeof(*hdr)) {
+			dev_err(&binfo->pdev->dev, "The region is too small to contain a feature.\n");
+			ret =  -EINVAL;
+			break;
+		}
+
+		hdr = (struct feature_header *)start;
+		ret = parse_feature(binfo, hdr);
+		if (ret)
+			break;
+
+		header.csr = readq(hdr);
+		if (header.eol || !header.next_header_offset)
+			break;
+	}
+
+	return ret;
+}
+
+static int parse_ports_from_fme(struct build_feature_devs_info *binfo)
+{
+	struct feature_fme_header *fme_hdr;
+	struct feature_fme_port port;
+	int i = 0, ret = 0;
+
+	if (binfo->pfme_hdr == NULL) {
+		dev_dbg(&binfo->pdev->dev, "VF is detected.\n");
+		return ret;
+	}
+
+	fme_hdr = binfo->pfme_hdr;
+
+	do {
+		port.csr = readq(&fme_hdr->port[i]);
+		if (!port.port_implemented)
+			break;
+
+		ret = parse_switch_to(binfo, port.port_bar);
+		if (ret)
+			break;
+
+		ret = parse_feature_list(binfo,
+				binfo->ioaddr + port.port_offset);
+		if (ret)
+			break;
+	} while (++i < MAX_FPGA_PORT_NUM);
+
+	return ret;
+}
+
+static int create_init_drvdata(struct pci_dev *pdev)
+{
+	struct cci_drvdata *drvdata;
+
+	drvdata = devm_kzalloc(&pdev->dev, sizeof(*drvdata), GFP_KERNEL);
+	if (!drvdata)
+		return -ENOMEM;
+
+	drvdata->device_id = alloc_fpga_id(PARENT_ID, NULL);
+	if (drvdata->device_id < 0) {
+		int ret = drvdata->device_id;
+
+		devm_kfree(&pdev->dev, drvdata);
+		return ret;
+	}
+
+	mutex_init(&drvdata->lock);
+	INIT_LIST_HEAD(&drvdata->port_dev_list);
+	INIT_LIST_HEAD(&drvdata->regions);
+
+	dev_set_drvdata(&pdev->dev, drvdata);
+	return 0;
+}
+
+static void destroy_drvdata(struct pci_dev *pdev)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pdev->dev);
+
+	if (drvdata->fme_dev) {
+		/* fme device should be unregistered first. */
+		WARN_ON(device_is_registered(drvdata->fme_dev));
+		free_fpga_id(FME_ID, to_platform_device(drvdata->fme_dev)->id);
+		put_device(drvdata->fme_dev);
+	}
+
+	cci_pci_remove_port_devs(pdev);
+	cci_pci_release_regions(pdev);
+	dev_set_drvdata(&pdev->dev, NULL);
+	free_fpga_id(PARENT_ID, drvdata->device_id);
+	devm_kfree(&pdev->dev, drvdata);
+}
+
+static struct class *fpga_class;
+
+struct device *fpga_create_parent_dev(struct pci_dev *pdev, int id)
+{
+	struct device *dev;
+
+	dev = device_create(fpga_class, &pdev->dev, MKDEV(0, 0), NULL,
+			    "intel-fpga-dev.%d", id);
+	if (IS_ERR(dev)) {
+		dev_err(&pdev->dev, "create parent device failed %ld.\n",
+			PTR_ERR(dev));
+		return dev;
+	}
+
+	/*
+	 * it is safe to modify some device fields here as:
+	 *   a) the device is not attached to any bus, i.e, no driver
+	 *      will match with this device;
+	 *   b) it is a single device, i.e, no child will access its
+	 *      resource.
+	 */
+	dev->dma_mask = pdev->dev.dma_mask;
+	dev->dma_parms = pdev->dev.dma_parms;
+	dev->coherent_dma_mask = pdev->dev.coherent_dma_mask;
+	return dev;
+}
+
+static int cci_pci_create_feature_devs(struct pci_dev *pdev)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pdev->dev);
+	struct build_feature_devs_info *binfo;
+	int ret;
+
+	binfo = build_info_alloc_and_init(pdev);
+	if (!binfo)
+		return -ENOMEM;
+
+	binfo->parent_dev = fpga_create_parent_dev(pdev, drvdata->device_id);
+	if (IS_ERR(binfo->parent_dev)) {
+		ret = PTR_ERR(binfo->parent_dev);
+		goto free_binfo_exit;
+	}
+
+	ret = parse_start(binfo);
+	if (ret)
+		goto free_binfo_exit;
+
+	ret = parse_feature_list(binfo, binfo->ioaddr);
+	if (ret)
+		goto free_binfo_exit;
+
+	ret = parse_ports_from_fme(binfo);
+	if (ret)
+		goto free_binfo_exit;
+
+	ret = build_info_commit_dev(binfo);
+	if (ret)
+		goto free_binfo_exit;
+
+	/*
+	 * everything is okay, reset ->parent_dev to stop it being
+	 * freed by build_info_free()
+	 */
+	binfo->parent_dev = NULL;
+
+free_binfo_exit:
+	build_info_free(binfo);
+	return ret;
+}
+
+/* PCI Device ID */
+#define PCIe_DEVICE_ID_RCiEP0_MCP    0xBCBD
+#define PCIe_DEVICE_ID_RCiEP0_SKX_P  0xBCC0
+#define PCIe_DEVICE_ID_RCiEP0_DCP    0x09C4
+/* VF Device */
+#define PCIe_DEVICE_ID_VF_MCP        0xBCBF
+#define PCIe_DEVICE_ID_VF_SKX_P      0xBCC1
+#define PCIe_DEVICE_ID_VF_DCP        0x09C5
+
+static struct pci_device_id cci_pcie_id_tbl[] = {
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCIe_DEVICE_ID_RCiEP0_MCP),},
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCIe_DEVICE_ID_VF_MCP),},
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCIe_DEVICE_ID_RCiEP0_SKX_P),},
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCIe_DEVICE_ID_VF_SKX_P),},
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCIe_DEVICE_ID_RCiEP0_DCP),},
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCIe_DEVICE_ID_VF_DCP),},
+	{0,}
+};
+MODULE_DEVICE_TABLE(pci, cci_pcie_id_tbl);
+
+static void port_config_vf(struct device *fme_dev, int port_id, bool is_vf)
+{
+	struct feature_fme_header *fme_hdr;
+	struct feature_fme_port port;
+	int type = is_vf ? FME_AFU_ACCESS_VF : FME_AFU_ACCESS_PF;
+
+	fme_hdr = get_feature_ioaddr_by_index(fme_dev,
+		FME_FEATURE_ID_HEADER);
+
+	WARN_ON(!fme_hdr);
+
+	port.csr = readq(&fme_hdr->port[port_id]);
+	WARN_ON(!port.port_implemented);
+
+	port.afu_access_control = type;
+	writeq(port.csr, &fme_hdr->port[port_id]);
+}
+
+static int cci_pci_sriov_configure(struct pci_dev *pcidev, int num_vfs)
+{
+	int ret;
+	int vf_ports = 0;
+	struct device *fme_dev;
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pcidev->dev);
+	struct list_head *port_list = &drvdata->port_dev_list;
+	struct feature_platform_data *pdata;
+
+	mutex_lock(&drvdata->lock);
+
+	fme_dev = drvdata->fme_dev;
+	WARN_ON(!fme_dev);
+
+	if (drvdata->released_port_num < num_vfs) {
+		ret = -EBUSY;
+		goto unlock_exit;
+	}
+
+	if (!num_vfs)
+		pci_disable_sriov(pcidev);
+
+	list_for_each_entry(pdata, port_list, node) {
+		int id = fpga_port_id(pdata->dev);
+
+		if (device_is_registered(&pdata->dev->dev))
+			continue;
+
+		if (!num_vfs) {
+			port_config_vf(fme_dev, id, false);
+			dev_dbg(&pcidev->dev, "port_%d is turned to PF.\n", id);
+			continue;
+		}
+
+		port_config_vf(fme_dev, id, true);
+		dev_dbg(&pcidev->dev, "port_%d is turned to VF.\n", id);
+		if (++vf_ports == num_vfs)
+			break;
+	}
+
+	if (num_vfs) {
+		ret = pci_enable_sriov(pcidev, num_vfs);
+		if (ret) {
+			list_for_each_entry(pdata, port_list, node) {
+				int id = fpga_port_id(pdata->dev);
+
+				if (device_is_registered(&pdata->dev->dev))
+					continue;
+
+				port_config_vf(fme_dev, id, false);
+			}
+
+			goto unlock_exit;
+		}
+	}
+
+	ret = num_vfs;
+unlock_exit:
+	mutex_unlock(&drvdata->lock);
+	return ret;
+}
+
+static int cci_pci_alloc_irq(struct pci_dev *pcidev)
+{
+	int nvec = pci_msix_vec_count(pcidev);
+	int ret = 0;
+
+	if (nvec <= 0) {
+		dev_dbg(&pcidev->dev, "fpga interrupt not supported\n");
+		return 0;
+	}
+
+	ret = pci_alloc_irq_vectors(pcidev, nvec, nvec, PCI_IRQ_MSIX);
+	if (ret < 0)
+		return ret;
+
+	return 0;
+}
+
+static void cci_pci_free_irq(struct pci_dev *pcidev)
+{
+	pci_free_irq_vectors(pcidev);
+}
+
+static
+int cci_pci_probe(struct pci_dev *pcidev, const struct pci_device_id *pcidevid)
+{
+	int ret;
+
+	ret = pci_enable_device(pcidev);
+	if (ret < 0) {
+		dev_err(&pcidev->dev, "Failed to enable device %d.\n", ret);
+		goto exit;
+	}
+
+	ret = pci_enable_pcie_error_reporting(pcidev);
+	if (ret && ret != -EINVAL)
+		dev_info(&pcidev->dev, "PCIE AER unavailable %d.\n", ret);
+
+	ret = pci_request_regions(pcidev, DRV_NAME);
+	if (ret) {
+		dev_err(&pcidev->dev, "Failed to request regions.\n");
+		goto disable_error_report_exit;
+	}
+
+	pci_set_master(pcidev);
+	pci_save_state(pcidev);
+
+	if (!dma_set_mask(&pcidev->dev, DMA_BIT_MASK(64))) {
+		dma_set_coherent_mask(&pcidev->dev, DMA_BIT_MASK(64));
+	} else if (!dma_set_mask(&pcidev->dev, DMA_BIT_MASK(32))) {
+		dma_set_coherent_mask(&pcidev->dev, DMA_BIT_MASK(32));
+	} else {
+		ret = -EIO;
+		dev_err(&pcidev->dev, "No suitable DMA support available.\n");
+		goto release_region_exit;
+	}
+
+	ret = create_init_drvdata(pcidev);
+	if (ret)
+		goto release_region_exit;
+
+	ret = cci_pci_alloc_irq(pcidev);
+	if (ret)
+		goto destroy_drvdata_exit;
+
+	ret = cci_pci_create_feature_devs(pcidev);
+	if (ret)
+		goto free_irq_exit;
+
+	return 0;
+
+free_irq_exit:
+	cci_pci_free_irq(pcidev);
+destroy_drvdata_exit:
+	destroy_drvdata(pcidev);
+release_region_exit:
+	pci_release_regions(pcidev);
+disable_error_report_exit:
+	pci_disable_pcie_error_reporting(pcidev);
+	pci_disable_device(pcidev);
+exit:
+	return ret;
+}
+
+static
+void cci_pci_remove(struct pci_dev *pcidev)
+{
+	/* disable sriov. */
+	if (dev_is_pf(&pcidev->dev))
+		cci_pci_sriov_configure(pcidev, 0);
+
+	remove_all_devs(pcidev);
+
+	pci_disable_pcie_error_reporting(pcidev);
+
+	cci_pci_free_irq(pcidev);
+	destroy_drvdata(pcidev);
+	pci_release_regions(pcidev);
+	pci_disable_device(pcidev);
+}
+
+static struct pci_driver cci_pci_driver = {
+	.name = DRV_NAME,
+	.id_table = cci_pcie_id_tbl,
+	.probe = cci_pci_probe,
+	.remove = cci_pci_remove,
+	.sriov_configure = cci_pci_sriov_configure
+};
+
+static int __init ccidrv_init(void)
+{
+	int ret;
+
+	pr_info("Intel(R) FPGA PCIe Driver: Version %s\n", DRV_VERSION);
+
+	fpga_ids_init();
+
+	ret = fpga_chardev_init();
+	if (ret)
+		goto exit_ids;
+
+	fpga_class = class_create(THIS_MODULE, "fpga");
+	if (IS_ERR(fpga_class)) {
+		ret = PTR_ERR(fpga_class);
+		goto exit_chardev;
+	}
+
+	ret = pci_register_driver(&cci_pci_driver);
+	if (ret) {
+		class_destroy(fpga_class);
+exit_chardev:
+		fpga_chardev_uinit();
+exit_ids:
+		fpga_ids_destroy();
+	}
+
+	return ret;
+}
+
+static void __exit ccidrv_exit(void)
+{
+	pci_unregister_driver(&cci_pci_driver);
+	class_destroy(fpga_class);
+	fpga_chardev_uinit();
+	fpga_ids_destroy();
+}
+
+module_init(ccidrv_init);
+module_exit(ccidrv_exit);
+
+MODULE_DESCRIPTION("FPGA PCIe Device Drive");
+MODULE_AUTHOR("Intel Corporation");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/fpga/intel/pcie_check.c b/drivers/fpga/intel/pcie_check.c
new file mode 100644
index 000000000000..48d4f51e4755
--- /dev/null
+++ b/drivers/fpga/intel/pcie_check.c
@@ -0,0 +1,141 @@
+/*
+ * check the pcie parsed header with the default value in SAS spec
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Zhang Yi <Yi.Z.Zhang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ */
+
+#include <linux/pci.h>
+#include <linux/kdev_t.h>
+#include <linux/stddef.h>
+#include "feature-dev.h"
+
+#define DFH_CCI_VERSION				0x1
+#define DFH_TYPE_PRIVATE			0x3
+#define DFH_TYPE_AFU				0x1
+#define DFH_TYPE_FIU				0x4
+#define DFH_TYPE_FIU_ID_FME			0x0
+#define DFH_TYPE_FIU_ID_PORT			0x1
+
+#define FME_FEATURE_HEADER_TYPE			DFH_TYPE_FIU
+#define FME_FEATURE_HEADER_NEXT_OFFSET		0x1000
+#define FME_FEATURE_HEADER_ID			DFH_TYPE_FIU_ID_FME
+#define FME_FEATURE_HEADER_VERSION		0x1
+
+#define FME_FEATURE_THERMAL_MGMT_TYPE		DFH_TYPE_PRIVATE
+#define FME_FEATURE_THERMAL_MGMT_NEXT_OFFSET	0x1000
+#define FME_FEATURE_THERMAL_MGMT_ID		0x1
+#define FME_FEATURE_THERMAL_MGMT_VERSION	0x0
+
+#define FME_FEATURE_POWER_MGMT_TYPE		DFH_TYPE_PRIVATE
+#define FME_FEATURE_POWER_MGMT_NEXT_OFFSET	0x1000
+#define FME_FEATURE_POWER_MGMT_ID		0x2
+#define FME_FEATURE_POWER_MGMT_VERSION		0x1
+
+#define FME_FEATURE_GLOBAL_IPERF_TYPE		DFH_TYPE_PRIVATE
+#define FME_FEATURE_GLOBAL_IPERF_NEXT_OFFSET	0x1000
+#define FME_FEATURE_GLOBAL_IPERF_ID		0x3
+#define FME_FEATURE_GLOBAL_IPERF_VERSION	0x1
+
+#define FME_FEATURE_GLOBAL_ERR_TYPE		DFH_TYPE_PRIVATE
+#define FME_FEATURE_GLOBAL_ERR_NEXT_OFFSET	0x1000
+#define FME_FEATURE_GLOBAL_ERR_ID		0x4
+#define FME_FEATURE_GLOBAL_ERR_VERSION		0x1
+
+#define FME_FEATURE_PR_MGMT_TYPE		DFH_TYPE_PRIVATE
+#define FME_FEATURE_PR_MGMT_NEXT_OFFSET		0x1000
+#define FME_FEATURE_PR_MGMT_ID			0x5
+#define FME_FEATURE_PR_MGMT_VERSION		0x2
+
+#define FME_FEATURE_HSSI_ETH_TYPE		DFH_TYPE_PRIVATE
+#define FME_FEATURE_HSSI_ETH_NEXT_OFFSET	0x1000
+#define FME_FEATURE_HSSI_ETH_ID			0x6
+#define FME_FEATURE_HSSI_ETH_VERSION		0x0
+
+#define FME_FEATURE_GLOBAL_DPERF_TYPE		DFH_TYPE_PRIVATE
+#define FME_FEATURE_GLOBAL_DPERF_NEXT_OFFSET	0x0
+#define FME_FEATURE_GLOBAL_DPERF_ID		0x7
+#define FME_FEATURE_GLOBAL_DPERF_VERSION	0x0
+
+#define FME_FEATURE_QSPI_FLASH_TYPE		DFH_TYPE_PRIVATE
+#define FME_FEATURE_QSPI_FLASH_NEXT_OFFSET	0x2000
+#define FME_FEATURE_QSPI_FLASH_ID		FME_FEATURE_ID_QSPI_FLASH
+#define FME_FEATURE_QSPI_FLASH_VERSION		FME_QSPI_REVISION
+
+#define PORT_FEATURE_HEADER_TYPE		DFH_TYPE_FIU
+#define PORT_FEATURE_HEADER_NEXT_OFFSET		0x1000
+#define PORT_FEATURE_HEADER_ID			DFH_TYPE_FIU_ID_PORT
+#define PORT_FEATURE_HEADER_VERSION		0x0
+
+#define PORT_FEATURE_ERR_TYPE			DFH_TYPE_PRIVATE
+#define PORT_FEATURE_ERR_NEXT_OFFSET		0x1000
+#define PORT_FEATURE_ERR_ID			0x10
+#define PORT_FEATURE_ERR_VERSION		0x1
+
+#define PORT_FEATURE_UMSG_TYPE			DFH_TYPE_PRIVATE
+#define PORT_FEATURE_UMSG_NEXT_OFFSET		0x2000
+#define PORT_FEATURE_UMSG_ID			0x11
+#define PORT_FEATURE_UMSG_VERSION		0x0
+
+#define PORT_FEATURE_STP_TYPE			DFH_TYPE_PRIVATE
+#define PORT_FEATURE_STP_NEXT_OFFSET		0x0
+#define PORT_FEATURE_STP_ID			0x13
+#define PORT_FEATURE_STP_VERSION		0x1
+
+#define DEFAULT_REG(name)	{.id = name##_ID, .revision = name##_VERSION,\
+				.next_header_offset = name##_NEXT_OFFSET,\
+				.type = name##_TYPE,}
+
+static struct feature_header default_port_feature_hdr[] = {
+	DEFAULT_REG(PORT_FEATURE_HEADER),
+	DEFAULT_REG(PORT_FEATURE_ERR),
+	DEFAULT_REG(PORT_FEATURE_UMSG),
+	{.csr = 0,},
+	DEFAULT_REG(PORT_FEATURE_STP),
+	{.csr = 0,},
+};
+
+static struct feature_header default_fme_feature_hdr[] = {
+	DEFAULT_REG(FME_FEATURE_HEADER),
+	DEFAULT_REG(FME_FEATURE_THERMAL_MGMT),
+	DEFAULT_REG(FME_FEATURE_POWER_MGMT),
+	DEFAULT_REG(FME_FEATURE_GLOBAL_IPERF),
+	DEFAULT_REG(FME_FEATURE_GLOBAL_ERR),
+	DEFAULT_REG(FME_FEATURE_PR_MGMT),
+	DEFAULT_REG(FME_FEATURE_HSSI_ETH),
+	DEFAULT_REG(FME_FEATURE_GLOBAL_DPERF),
+	DEFAULT_REG(FME_FEATURE_QSPI_FLASH),
+};
+
+void check_features_header(struct pci_dev *pdev, struct feature_header *hdr,
+			   enum fpga_devt_type type, int id)
+{
+	struct feature_header *default_header, header;
+
+	if (type == FPGA_DEVT_FME) {
+		default_header = default_fme_feature_hdr;
+		WARN_ON(id >= ARRAY_SIZE(default_fme_feature_hdr));
+	} else if (type == FPGA_DEVT_PORT) {
+		default_header = default_port_feature_hdr;
+		WARN_ON(id >= ARRAY_SIZE(default_port_feature_hdr));
+	} else {
+		WARN_ON(1);
+		return;
+	}
+
+	header.csr = readq(hdr);
+
+	if (memcmp(&header, default_header + id, sizeof(header)))
+		dev_dbg(&pdev->dev,
+			"check header failed. current hdr:%llx - default_value:%llx.\n",
+			header.csr, *(u64 *)(default_header + id));
+	else
+		dev_dbg(&pdev->dev,
+			"check header pass.\n");
+}
diff --git a/drivers/fpga/intel/region.c b/drivers/fpga/intel/region.c
new file mode 100644
index 000000000000..8819561f1392
--- /dev/null
+++ b/drivers/fpga/intel/region.c
@@ -0,0 +1,130 @@
+/*
+ * Driver for FPGA Accelerated Function Unit (AFU) Region Management
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Wu Hao <hao.wu@linux.intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include "afu.h"
+
+void afu_region_init(struct feature_platform_data *pdata)
+{
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+
+	INIT_LIST_HEAD(&afu->regions);
+}
+
+#define for_each_region(region, afu)	\
+	list_for_each_entry((region), &(afu)->regions, node)
+static struct fpga_afu_region *get_region_by_index(struct fpga_afu *afu,
+						   u32 region_index)
+{
+	struct fpga_afu_region *region;
+
+	for_each_region(region, afu)
+		if (region->index == region_index)
+			return region;
+
+	return NULL;
+}
+
+int afu_region_add(struct feature_platform_data *pdata, u32 region_index,
+		   u64 region_size, u64 phys, u32 flags)
+{
+	struct fpga_afu_region *region;
+	struct fpga_afu *afu;
+	int ret = 0;
+
+	region = devm_kzalloc(&pdata->dev->dev, sizeof(*region), GFP_KERNEL);
+	if (!region)
+		return -ENOMEM;
+
+	region->index = region_index;
+	region->size = region_size;
+	region->phys = phys;
+	region->flags = flags;
+
+	mutex_lock(&pdata->lock);
+
+	afu = fpga_pdata_get_private(pdata);
+
+	/* check if @index already exists */
+	if (get_region_by_index(afu, region_index)) {
+		mutex_unlock(&pdata->lock);
+		ret = -EEXIST;
+		goto exit;
+	}
+
+	region_size = PAGE_ALIGN(region_size);
+	region->offset = afu->region_cur_offset;
+	list_add(&region->node, &afu->regions);
+
+	afu->region_cur_offset += region_size;
+	afu->num_regions++;
+	mutex_unlock(&pdata->lock);
+	return 0;
+
+exit:
+	devm_kfree(&pdata->dev->dev, region);
+	return ret;
+}
+
+void afu_region_destroy(struct feature_platform_data *pdata)
+{
+	struct fpga_afu_region *tmp, *region;
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+
+	list_for_each_entry_safe(region, tmp, &afu->regions, node) {
+		list_del(&region->node);
+		devm_kfree(&pdata->dev->dev, region);
+	}
+}
+
+int afu_get_region_by_index(struct feature_platform_data *pdata,
+			    u32 region_index, struct fpga_afu_region *pregion)
+{
+	struct fpga_afu_region *region;
+	struct fpga_afu *afu;
+	int ret = 0;
+
+	mutex_lock(&pdata->lock);
+	afu = fpga_pdata_get_private(pdata);
+	region = get_region_by_index(afu, region_index);
+	if (!region) {
+		ret = -EINVAL;
+		goto exit;
+	}
+	*pregion = *region;
+exit:
+	mutex_unlock(&pdata->lock);
+	return ret;
+}
+
+int afu_get_region_by_offset(struct feature_platform_data *pdata,
+			    u64 offset, u64 size,
+			    struct fpga_afu_region *pregion)
+{
+	struct fpga_afu_region *region;
+	struct fpga_afu *afu;
+	int ret = 0;
+
+	mutex_lock(&pdata->lock);
+	afu = fpga_pdata_get_private(pdata);
+	for_each_region(region, afu)
+		if (region->offset <= offset &&
+		   region->offset + region->size >= offset + size) {
+			*pregion = *region;
+			goto exit;
+		}
+	ret = -EINVAL;
+exit:
+	mutex_unlock(&pdata->lock);
+	return ret;
+}
diff --git a/include/uapi/linux/intel-fpga.h b/include/uapi/linux/intel-fpga.h
new file mode 100644
index 000000000000..5cf8d688f1a8
--- /dev/null
+++ b/include/uapi/linux/intel-fpga.h
@@ -0,0 +1,354 @@
+/*
+ * Header File for Intel FPGA User API
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Zhang Yi <yi.z.zhang@intel.com>
+ *   Wu Hao <hao.wu@linux.intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#ifndef _UAPI_INTEL_FPGA_H
+#define _UAPI_INTEL_FPGA_H
+
+#ifdef USER_LINUX_TYPES
+#include <intel-fpga/glist.h>
+#else
+#include <linux/types.h>
+#endif
+
+#define FPGA_API_VERSION 0
+
+#define INTEL_FPGA_DRIVER_VER_MAJOR  0
+#define INTEL_FPGA_DRIVER_VER_MINOR  13
+#define INTEL_FPGA_DRIVER_VER_REV    0
+#define INTEL_FPGA_DRIVER_VER_BUILD  
+#define INTEL_FPGA_DRIVER_VERSION    "0.13.0"
+
+/*
+ * The IOCTL interface for Intel FPGA is designed for extensibility by
+ * embedding the structure length (argsz) and flags into structures passed
+ * between kernel and userspace. This design referenced the VFIO IOCTL
+ * interface (include/uapi/linux/vfio.h).
+ */
+
+#define FPGA_MAGIC 0xB5
+
+#define FPGA_BASE 0
+#define PORT_BASE 0x40
+#define FME_BASE 0x80
+
+/* Common IOCTLs for both FME and AFU file descriptor */
+
+/**
+ * FPGA_GET_API_VERSION - _IO(FPGA_MAGIC, FPGA_BASE + 0)
+ *
+ * Report the version of the driver API.
+ * Return: Driver API Version.
+ */
+
+#define FPGA_GET_API_VERSION	_IO(FPGA_MAGIC, FPGA_BASE + 0)
+
+/**
+ * FPGA_CHECK_EXTENSION - _IO(FPGA_MAGIC, FPGA_BASE + 1)
+ *
+ * Check whether an extension is supported.
+ * Return: 0 if not supported, otherwise the extension is supported.
+ */
+
+#define FPGA_CHECK_EXTENSION	_IO(FPGA_MAGIC, FPGA_BASE + 1)
+
+/* IOCTLs for AFU file descriptor */
+
+/**
+ * FPGA_PORT_RESET - _IO(FPGA_MAGIC, PORT_BASE + 0)
+ *
+ * Reset the FPGA AFU Port. No parameters are supported.
+ * Return: 0 on success, -errno of failure
+ */
+
+#define FPGA_PORT_RESET		_IO(FPGA_MAGIC, PORT_BASE + 0)
+
+/**
+ * FPGA_PORT_GET_INFO - _IOR(FPGA_MAGIC, PORT_BASE + 1, struct fpga_port_info)
+ *
+ * Retrieve information about the fpga port.
+ * Driver fills the info in provided struct fpga_port_info.
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_port_info {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	/* Output */
+	__u32 flags;		/* Zero for now */
+	__u32 capability;	/* The capability of port device */
+#define FPGA_PORT_CAP_ERR_IRQ	(1 << 0) /* Support port error interrupt */
+#define FPGA_PORT_CAP_UAFU_IRQ	(1 << 1) /* Support uafu error interrupt */
+	__u32 num_regions;	/* The number of supported regions */
+	__u32 num_umsgs;	/* The number of allocated umsgs */
+	__u32 num_uafu_irqs;    /* The number of uafu interrupts */
+};
+
+#define FPGA_PORT_GET_INFO	_IO(FPGA_MAGIC, PORT_BASE + 1)
+
+/**
+ * FPGA_PORT_GET_REGION_INFO - _IOWR(FPGA_MAGIC, PORT_BASE + 2,
+ *						struct fpga_port_region_info)
+ *
+ * Retrieve information about a device region.
+ * Caller provides struct fpga_port_region_info with index value set.
+ * Driver returns the region info in other fields.
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_port_region_info {
+	/* input */
+	__u32 argsz;		/* Structure length */
+	/* Output */
+	__u32 flags;		/* Access permission */
+#define FPGA_REGION_READ	(1 << 0)	/* Region is readable */
+#define FPGA_REGION_WRITE	(1 << 1)	/* Region is writable */
+#define FPGA_REGION_MMAP	(1 << 2)	/* Can be mmaped to userspace */
+	/* Input */
+	__u32 index;		/* Region index */
+#define FPGA_PORT_INDEX_UAFU	0		/* User AFU */
+#define FPGA_PORT_INDEX_STP	1		/* Signal Tap */
+	__u32 padding;
+	/* Output */
+	__u64 size;		/* Region size (bytes) */
+	__u64 offset;		/* Region offset from start of device fd */
+};
+
+#define FPGA_PORT_GET_REGION_INFO	_IO(FPGA_MAGIC, PORT_BASE + 2)
+
+/**
+ * FPGA_PORT_DMA_MAP - _IOWR(FPGA_MAGIC, PORT_BASE + 3,
+ *						struct fpga_port_dma_map)
+ *
+ * Map the dma memory per user_addr and length which are provided by caller.
+ * Driver fills the iova in provided struct afu_port_dma_map.
+ * This interface only accepts page-size aligned user memory for dma mapping.
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_port_dma_map {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__u64 user_addr;        /* Process virtual address */
+	__u64 length;           /* Length of mapping (bytes)*/
+	/* Output */
+	__u64 iova;             /* IO virtual address */
+};
+
+#define FPGA_PORT_DMA_MAP	_IO(FPGA_MAGIC, PORT_BASE + 3)
+
+/**
+ * FPGA_PORT_DMA_UNMAP - _IOW(FPGA_MAGIC, PORT_BASE + 4,
+ *						struct fpga_port_dma_unmap)
+ *
+ * Unmap the dma memory per iova provided by caller.
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_port_dma_unmap {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__u64 iova;		/* IO virtual address */
+};
+
+#define FPGA_PORT_DMA_UNMAP	_IO(FPGA_MAGIC, PORT_BASE + 4)
+
+/**
+ * FPGA_PORT_UMSG_ENABLE - _IO(FPGA_MAGIC, PORT_BASE + 5)
+ * FPGA_PORT_UMSG_DISABLE - _IO(FPGA_MAGIC, PORT_BASE + 6)
+ *
+ * Interfaces to control UMSG function. No parameters are supported.
+ * Return: 0 on success, -errno on failure.
+ */
+
+#define FPGA_PORT_UMSG_ENABLE	_IO(FPGA_MAGIC, PORT_BASE + 5)
+#define FPGA_PORT_UMSG_DISABLE	_IO(FPGA_MAGIC, PORT_BASE + 6)
+
+/**
+ * FPGA_PORT_UMSG_SET_MODE - _IOW(FPGA_MAGIC, PORT_BASE + 7,
+ *						struct fpga_port_umsg_cfg)
+ *
+ * Set Hint Mode per bitmap provided by caller. One bit for each page
+ * in hint_bitmap. 0 - Disable and 1 - Enable Hint Mode.
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_port_umsg_cfg {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__u32 hint_bitmap;	/* UMSG Hint Mode Bitmap */
+};
+
+#define FPGA_PORT_UMSG_SET_MODE		_IO(FPGA_MAGIC, PORT_BASE + 7)
+
+/**
+ * FPGA_PORT_UMSG_SET_BASE_ADDR - _IOW(FPGA_MAGIC, PORT_BASE + 8,
+ *						struct afu_port_umsg_base_addr)
+ *
+ * Set UMSG base address per iova provided by caller. Driver configures the
+ * UMSG base address with the iova, but only accept iova which get from the
+ * DMA_MAP IOCTL interface and the DMA region is big enough for all UMSGs
+ * (num_umsg * PAGE_SIZE)
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_port_umsg_base_addr {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__u64 iova;		/* IO virtual address */
+};
+
+#define FPGA_PORT_UMSG_SET_BASE_ADDR	_IO(FPGA_MAGIC, PORT_BASE + 8)
+
+/**
+ * FPGA_PORT_ERR_SET_IRQ - _IOW(FPGA_MAGIC, PORT_BASE + 9,
+ *                                             struct fpga_port_err_irq_set)
+ *
+ * Set fpga port global error interrupt eventfd
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_port_err_irq_set {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__s32 evtfd;		/* Eventfd handler */
+};
+
+#define FPGA_PORT_ERR_SET_IRQ		_IO(FPGA_MAGIC, PORT_BASE + 9)
+
+/**
+ * FPGA_PORT_UAFU_SET_IRQ - _IOW(FPGA_MAGIC, PORT_BASE + 10,
+ *                                             struct fpga_port_uafu_irq_set)
+ *
+ * Set fpga UAFU interrupt eventfd
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_port_uafu_irq_set {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__u32 start;		/* First irq number */
+	__u32 count;		/* The number of eventfd handler */
+	__s32 evtfd[];		/* Eventfd handler */
+};
+
+#define FPGA_PORT_UAFU_SET_IRQ		_IO(FPGA_MAGIC, PORT_BASE + 10)
+
+/* IOCTLs for FME file descriptor */
+
+/**
+ * FPGA_FME_PORT_PR - _IOWR(FPGA_MAGIC, FME_BASE + 0, struct fpga_fme_port_pr)
+ *
+ * Driver does Partial Reconfiguration based on Port ID and Buffer (Image)
+ * provided by caller.
+ * Return: 0 on success, -errno on failure.
+ * If FPGA_FME_PORT_PR returns -EIO, that indicates the HW has detected
+ * some errors during PR, under this case, the user can fetch HW error code
+ * from fpga_fme_port_pr.status. Each bit on the error code is used as the
+ * index for the array created by DEFINE_FPGA_PR_ERR_MSG().
+ * Otherwise, it is always zero.
+ */
+
+#define DEFINE_FPGA_PR_ERR_MSG(_name_)			\
+static const char * const _name_[] = {			\
+	"PR operation error detected",			\
+	"PR CRC error detected",			\
+	"PR incompatiable bitstream error detected",	\
+	"PR IP protocol error detected",		\
+	"PR FIFO overflow error detected",		\
+	"PR timeout error detected",			\
+	"PR secure load error detected",		\
+}
+
+#define PR_MAX_ERR_NUM	7
+
+struct fpga_fme_port_pr {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__u32 port_id;
+	__u32 buffer_size;
+	__u64 buffer_address;	/* Userspace address to the buffer for PR */
+	/* Output */
+	__u64 status;		/* HW error code if ioctl returns -EIO */
+};
+
+#define FPGA_FME_PORT_PR	_IO(FPGA_MAGIC, FME_BASE + 0)
+
+/**
+ * FPGA_FME_PORT_RELEASE - _IOW(FPGA_MAGIC, FME_BASE + 1,
+ *						struct fpga_fme_port_release)
+ *
+ * Driver releases the port per Port ID provided by caller.
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_fme_port_release {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__u32 port_id;
+};
+
+#define FPGA_FME_PORT_RELEASE	_IO(FPGA_MAGIC, FME_BASE + 1)
+
+/**
+ * FPGA_FME_PORT_ASSIGN - _IOW(FPGA_MAGIC, FME_BASE + 2,
+ *						struct fpga_fme_port_assign)
+ *
+ * Driver assigns the port per Port ID provided by caller.
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_fme_port_assign {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__u32 port_id;
+};
+
+#define FPGA_FME_PORT_ASSIGN	_IO(FPGA_MAGIC, FME_BASE + 2)
+
+/**
+ * FPGA_FME_GET_INFO - _IOR(FPGA_MAGIC, FME_BASE + 3, struct fpga_fme_info)
+ *
+ * Retrieve information about the fpga fme.
+ * Driver fills the info in provided struct fpga_fme_info.
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_fme_info {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	/* Output */
+	__u32 flags;		/* Zero for now */
+	__u32 capability;	/* The capability of FME device */
+#define FPGA_FME_CAP_ERR_IRQ	(1 << 0) /* Support fme error interrupt */
+};
+
+#define FPGA_FME_GET_INFO      _IO(FPGA_MAGIC, FME_BASE + 3)
+
+/**
+ * FPGA_FME_ERR_SET_IRQ - _IOW(FPGA_MAGIC, FME_BASE + 4,
+ *                                             struct fpga_fme_err_irq_set)
+ *
+ * Set fpga fme global error interrupt eventfd
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_fme_err_irq_set {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__s32 evtfd;		/* Eventfd handler */
+};
+
+#define FPGA_FME_ERR_SET_IRQ	_IO(FPGA_MAGIC, FME_BASE + 4)
+
+#endif /* _UAPI_INTEL_FPGA_H */
-- 
2.16.1

