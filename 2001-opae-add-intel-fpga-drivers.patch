From c9304ab8898f463c4519832900e8f00d78a462d2 Mon Sep 17 00:00:00 2001
From: Miguel Bernal Marin <miguel.bernal.marin@linux.intel.com>
Date: Fri, 15 Dec 2017 16:02:17 -0600
Subject: [PATCH 2001/2002] opae: add intel fpga drivers

Signed-off-by: Miguel Bernal Marin <miguel.bernal.marin@linux.intel.com>
---
 drivers/fpga/intel/Kconfig       |   32 +
 drivers/fpga/intel/Makefile      |   22 +
 drivers/fpga/intel/afu-check.c   |  149 ++++
 drivers/fpga/intel/afu-error.c   |  226 +++++++
 drivers/fpga/intel/afu.c         |  965 ++++++++++++++++++++++++++
 drivers/fpga/intel/afu.h         |   78 +++
 drivers/fpga/intel/dma-region.c  |  372 ++++++++++
 drivers/fpga/intel/feature-dev.c |  288 ++++++++
 drivers/fpga/intel/feature-dev.h | 1390 ++++++++++++++++++++++++++++++++++++++
 drivers/fpga/intel/fme-error.c   |  448 ++++++++++++
 drivers/fpga/intel/fme-main.c    |  911 +++++++++++++++++++++++++
 drivers/fpga/intel/fme-perf.c    |  715 ++++++++++++++++++++
 drivers/fpga/intel/fme-pr.c      |  469 +++++++++++++
 drivers/fpga/intel/fme.h         |   61 ++
 drivers/fpga/intel/pcie.c        | 1272 ++++++++++++++++++++++++++++++++++
 drivers/fpga/intel/pcie_check.c  |  121 ++++
 drivers/fpga/intel/region.c      |  130 ++++
 include/uapi/linux/intel-fpga.h  |  282 ++++++++
 18 files changed, 7931 insertions(+)
 create mode 100644 drivers/fpga/intel/Kconfig
 create mode 100644 drivers/fpga/intel/Makefile
 create mode 100644 drivers/fpga/intel/afu-check.c
 create mode 100644 drivers/fpga/intel/afu-error.c
 create mode 100644 drivers/fpga/intel/afu.c
 create mode 100644 drivers/fpga/intel/afu.h
 create mode 100644 drivers/fpga/intel/dma-region.c
 create mode 100644 drivers/fpga/intel/feature-dev.c
 create mode 100644 drivers/fpga/intel/feature-dev.h
 create mode 100644 drivers/fpga/intel/fme-error.c
 create mode 100644 drivers/fpga/intel/fme-main.c
 create mode 100644 drivers/fpga/intel/fme-perf.c
 create mode 100644 drivers/fpga/intel/fme-pr.c
 create mode 100644 drivers/fpga/intel/fme.h
 create mode 100644 drivers/fpga/intel/pcie.c
 create mode 100644 drivers/fpga/intel/pcie_check.c
 create mode 100644 drivers/fpga/intel/region.c
 create mode 100644 include/uapi/linux/intel-fpga.h

diff --git a/drivers/fpga/intel/Kconfig b/drivers/fpga/intel/Kconfig
new file mode 100644
index 000000000000..2c458513cee7
--- /dev/null
+++ b/drivers/fpga/intel/Kconfig
@@ -0,0 +1,32 @@
+#
+# Open Programmable Acceleration Engine (OPAE) kernel driver
+#
+
+config FPGA_INTEL_OPAE
+	bool "Enabling Open Programmable Acceleration Engine (OPAE) "
+	help
+	  Enable the Open Programmable Acceleration Engine (OPAE) driver.
+	  More info at https://01.org/OPAE
+
+
+if FPGA_INTEL_OPAE
+
+config FPGA_INTEL_PCI
+	tristate "OPAE PCI"
+	default m
+	help
+	  Enable OPAE PCI
+
+config FPGA_INTEL_FME
+	tristate "OPAE FME"
+	default m
+	help
+	  Enable OPAE FME
+
+config FPGA_INTEL_AFU
+	tristate "OPAE AFU"
+	default m
+	help
+	  Enable OPAE AFU
+
+endif # FPGA_INTEL_OPAE
diff --git a/drivers/fpga/intel/Makefile b/drivers/fpga/intel/Makefile
new file mode 100644
index 000000000000..66af0a73f162
--- /dev/null
+++ b/drivers/fpga/intel/Makefile
@@ -0,0 +1,22 @@
+
+ccflags-y +=  -Wno-unused-value -Wno-unused-label
+ccflags-y += -DCONFIG_AS_AVX512
+
+obj-$(CONFIG_FPGA_INTEL_PCI) += intel-fpga-pci.o
+obj-$(CONFIG_FPGA_INTEL_FME) += intel-fpga-fme.o
+obj-$(CONFIG_FPGA_INTEL_AFU) += intel-fpga-afu.o
+
+intel-fpga-pci-y := pcie.o       \
+		    pcie_check.o \
+		    feature-dev.o
+
+intel-fpga-fme-y := fme-pr.o    \
+		    fme-perf.o  \
+		    fme-error.o \
+		    fme-main.o
+
+intel-fpga-afu-y := afu.o        \
+		    region.o     \
+		    dma-region.o \
+		    afu-error.o  \
+		    afu-check.o
diff --git a/drivers/fpga/intel/afu-check.c b/drivers/fpga/intel/afu-check.c
new file mode 100644
index 000000000000..95201c680f5b
--- /dev/null
+++ b/drivers/fpga/intel/afu-check.c
@@ -0,0 +1,149 @@
+#include "afu.h"
+
+static void port_check_reg(struct device *dev, void __iomem *addr,
+				const char *reg_name, u64 dflt)
+{
+	u64 value = readq(addr);
+
+	if (value != dflt)
+		dev_err(dev, "%s: incorrect value 0x%llx vs defautl 0x%llx\n",
+				reg_name, (unsigned long long)value,
+				(unsigned long long)dflt);
+}
+
+struct feature_port_header hdr_dflt = {
+	.rsvd1			= 0x0000000000000000,
+	.scratchpad		= 0x0000000000000000,
+	.capability = {
+		.csr		= 0x0000000100010000,
+	},
+	.control = {
+		/* Port Reset Bit is cleared in PCIe driver */
+		.csr		= 0x0000000000000004,
+	},
+	.status = {
+		.csr		= 0x0000000000000000,
+	},
+	.rsvd2			= 0x0000000000000000,
+	.user_clk_freq_cmd0	= 0x0000000000000000,
+	.user_clk_freq_cmd1	= 0x0000000000000000,
+	.user_clk_freq_sts0	= 0x0000000000000000,
+	.user_clk_freq_sts1	= 0x0000000000000000,
+};
+
+int port_hdr_test(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_port_header *port_hdr = feature->ioaddr;
+
+	/* Check if default value of hardware registers matches with spec */
+	port_check_reg(&pdev->dev, &port_hdr->rsvd1,
+			"hdr:rsvd1", hdr_dflt.rsvd1);
+	port_check_reg(&pdev->dev, &port_hdr->scratchpad,
+			"hdr:scratchpad", hdr_dflt.scratchpad);
+	port_check_reg(&pdev->dev, &port_hdr->capability,
+			"hdr:capability", hdr_dflt.capability.csr);
+	port_check_reg(&pdev->dev, &port_hdr->control,
+			"hdr:control", hdr_dflt.control.csr);
+	port_check_reg(&pdev->dev, &port_hdr->status,
+			"hdr:status", hdr_dflt.status.csr);
+	port_check_reg(&pdev->dev, &port_hdr->rsvd2,
+			"hdr:rsvd2", hdr_dflt.rsvd2);
+	port_check_reg(&pdev->dev, &port_hdr->user_clk_freq_cmd0,
+			"hdr:user_clk_cmd0", hdr_dflt.user_clk_freq_cmd0);
+	port_check_reg(&pdev->dev, &port_hdr->user_clk_freq_cmd1,
+			"hdr:user_clk_cmd1", hdr_dflt.user_clk_freq_cmd1);
+	port_check_reg(&pdev->dev, &port_hdr->user_clk_freq_sts0,
+			"hdr:user_clk_sts0", hdr_dflt.user_clk_freq_sts0);
+	port_check_reg(&pdev->dev, &port_hdr->user_clk_freq_sts1,
+			"hdr:user_clk_sts1", hdr_dflt.user_clk_freq_sts1);
+
+	dev_dbg(&pdev->dev, "%s finished\n", __func__);
+
+	return 0;
+}
+
+struct feature_port_error err_dflt = {
+	.error_mask = {
+		.csr		= 0x0000000000000000,
+	},
+	.port_error = {
+		.csr		= 0x0000000000000000,
+	},
+	.port_first_error = {
+		.csr		= 0x0000000000000000,
+	},
+	.malreq0 = {
+		.header_lsb	= 0x0000000000000000,
+	},
+	.malreq1 = {
+		.header_msb	= 0x0000000000000000,
+	},
+	.port_debug = {
+		.port_debug	= 0x0000000000000000,
+	},
+};
+
+int port_err_test(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_port_error *port_err = feature->ioaddr;
+
+	port_check_reg(&pdev->dev, &port_err->error_mask,
+			"err:error_mask", err_dflt.error_mask.csr);
+	port_check_reg(&pdev->dev, &port_err->port_error,
+			"err:port_error", err_dflt.port_error.csr);
+	port_check_reg(&pdev->dev, &port_err->port_first_error,
+			"err:port_first_err", err_dflt.port_first_error.csr);
+	port_check_reg(&pdev->dev, &port_err->malreq0,
+			"err:malreq0", err_dflt.malreq0.header_lsb);
+	port_check_reg(&pdev->dev, &port_err->malreq1,
+			"err:malreq1", err_dflt.malreq1.header_msb);
+	port_check_reg(&pdev->dev, &port_err->port_debug,
+			"err:port_debug", err_dflt.port_debug.port_debug);
+
+	dev_dbg(&pdev->dev, "%s finished\n", __func__);
+	return 0;
+}
+
+struct feature_port_umsg umsg_dflt = {
+	.capability = {
+		.csr		= 0x0000000000000008,
+	},
+	.baseaddr = {
+		.csr		= 0x0000000000000000,
+	},
+	.mode = {
+		.csr		= 0x0000000000000000,
+	},
+};
+
+int port_umsg_test(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_port_umsg *port_umsg = feature->ioaddr;
+
+	port_check_reg(&pdev->dev, &port_umsg->capability,
+				"umsg:capaiblity", umsg_dflt.capability.csr);
+	port_check_reg(&pdev->dev, &port_umsg->baseaddr,
+				"umsg:baseaddr", umsg_dflt.baseaddr.csr);
+	port_check_reg(&pdev->dev, &port_umsg->mode,
+				"umsg:mode", umsg_dflt.mode.csr);
+
+	dev_dbg(&pdev->dev, "%s finished\n", __func__);
+	return 0;
+}
+
+struct feature_port_stp stp_dflt = {
+	.stp_status = {
+		.csr		= 0x0000000000000000,
+	},
+};
+
+int port_stp_test(struct platform_device *pdev,	struct feature *feature)
+{
+	struct feature_port_stp *port_stp = feature->ioaddr;
+
+	port_check_reg(&pdev->dev, &port_stp->stp_status,
+				"stp:stp_csr", stp_dflt.stp_status.csr);
+
+	dev_dbg(&pdev->dev, "%s finished\n", __func__);
+	return 0;
+}
diff --git a/drivers/fpga/intel/afu-error.c b/drivers/fpga/intel/afu-error.c
new file mode 100644
index 000000000000..7b5cbc5711cf
--- /dev/null
+++ b/drivers/fpga/intel/afu-error.c
@@ -0,0 +1,226 @@
+/*
+ * Driver for FPGA Accelerated Function Unit (AFU) Error Handling
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Wu Hao <hao.wu@linux.intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include "afu.h"
+
+/* Mask / Unmask Port Errors by the Error Mask register. */
+static void port_err_mask(struct device *dev, bool mask)
+{
+	struct feature_port_error *port_err;
+	struct feature_port_err_key err_mask;
+
+	port_err = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_ERROR);
+
+	if (mask)
+		err_mask.csr = PORT_ERR_MASK;
+	else
+		err_mask.csr = 0;
+
+	writeq(err_mask.csr, &port_err->error_mask);
+}
+
+/* Clear All Port Errors. */
+static int port_err_clear(struct device *dev, u64 err)
+{
+	struct feature_port_header *port_hdr;
+	struct feature_port_error *port_err;
+	struct feature_port_err_key error, mask;
+	struct feature_port_first_err_key first;
+	struct feature_port_status status;
+	int ret = 0;
+
+	port_err = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_ERROR);
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	/*
+	 * Clear All Port Errors
+	 *
+	 * - Check for AP6 State
+	 * - Halt Port by keeping Port in reset
+	 * - Set PORT Error mask to all 1 to mask errors
+	 * - Clear all errors
+	 * - Set Port mask to all 0 to enable errors
+	 * - All errors start capturing new errors
+	 * - Enable Port by pulling the port out of reset
+	 */
+
+	/* If device is still in AP6 state, can not clear any error.*/
+	status.csr = readq(&port_hdr->status);
+	if (status.power_state == PORT_POWER_STATE_AP6) {
+		dev_err(dev, "Could not clear errors, device in AP6 state.\n");
+		return -EBUSY;
+	}
+
+	/* Halt Port if AP6 event detected */
+	error.csr = readq(&port_err->port_error);
+	if (error.ap6_event) {
+		ret = __fpga_port_disable(to_platform_device(dev));
+		if (ret)
+			return ret;
+	}
+
+	/* Mask all errors */
+	port_err_mask(dev, true);
+
+	/* Clear errors if err input matches with current port errors.*/
+	mask.csr = readq(&port_err->port_error);
+
+	if (mask.csr == err) {
+		writeq(mask.csr, &port_err->port_error);
+
+		first.csr = readq(&port_err->port_first_error);
+		writeq(first.csr, &port_err->port_first_error);
+	} else
+		ret = -EBUSY;
+
+	/* Clear mask */
+	port_err_mask(dev, false);
+
+	/* Enable the Port by clear the reset */
+	if (error.ap6_event)
+		__fpga_port_enable(to_platform_device(dev));
+
+	return ret;
+}
+
+static ssize_t
+revision_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct feature_port_error *port_err;
+	struct feature_header header;
+
+	port_err = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_ERROR);
+
+	header.csr = readq(&port_err->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+static DEVICE_ATTR_RO(revision);
+
+static ssize_t
+errors_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct feature_port_error *port_err;
+	struct feature_port_err_key error;
+
+	port_err = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_ERROR);
+
+	error.csr = readq(&port_err->port_error);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n",
+				(unsigned long long)error.csr);
+}
+static DEVICE_ATTR_RO(errors);
+
+static ssize_t
+first_error_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct feature_port_error *port_err;
+	struct feature_port_first_err_key first_error;
+
+	port_err = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_ERROR);
+
+	first_error.csr = readq(&port_err->port_first_error);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n",
+				(unsigned long long)first_error.csr);
+}
+static DEVICE_ATTR_RO(first_error);
+
+static ssize_t first_malformed_req_show(struct device *dev,
+				struct device_attribute *attr, char *buf)
+{
+	struct feature_port_error *port_err;
+	struct feature_port_malformed_req0 malreq0;
+	struct feature_port_malformed_req1 malreq1;
+
+	port_err = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_ERROR);
+
+	malreq0.header_lsb = readq(&port_err->malreq0);
+	malreq1.header_msb = readq(&port_err->malreq1);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%016llx%016llx\n",
+				(unsigned long long)malreq1.header_msb,
+				(unsigned long long)malreq0.header_lsb);
+}
+static DEVICE_ATTR_RO(first_malformed_req);
+
+static ssize_t clear_store(struct device *dev,
+		struct device_attribute *attr, const char *buff, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	int ret;
+	u64 value;
+
+	if (kstrtou64(buff, 0, &value))
+		return -EINVAL;
+
+	WARN_ON(!is_feature_present(dev, PORT_FEATURE_ID_HEADER));
+
+	mutex_lock(&pdata->lock);
+	ret = port_err_clear(dev, value);
+	mutex_unlock(&pdata->lock);
+
+	if (ret)
+		return ret;
+	else
+		return count;
+}
+static DEVICE_ATTR_WO(clear);
+
+static struct attribute *port_err_attrs[] = {
+	&dev_attr_revision.attr,
+	&dev_attr_errors.attr,
+	&dev_attr_first_error.attr,
+	&dev_attr_first_malformed_req.attr,
+	&dev_attr_clear.attr,
+	NULL,
+};
+
+static struct attribute_group port_err_attr_group = {
+	.attrs = port_err_attrs,
+	.name = "errors",
+};
+
+static int port_err_init(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+
+	dev_dbg(&pdev->dev, "PORT ERR Init.\n");
+
+	mutex_lock(&pdata->lock);
+	port_err_mask(&pdev->dev, false);
+	mutex_unlock(&pdata->lock);
+
+	return sysfs_create_group(&pdev->dev.kobj, &port_err_attr_group);
+}
+
+static void port_err_uinit(struct platform_device *pdev,
+					struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "PORT ERR UInit.\n");
+
+	sysfs_remove_group(&pdev->dev.kobj, &port_err_attr_group);
+}
+
+struct feature_ops port_err_ops = {
+	.init = port_err_init,
+	.uinit = port_err_uinit,
+	.test = port_err_test,
+};
diff --git a/drivers/fpga/intel/afu.c b/drivers/fpga/intel/afu.c
new file mode 100644
index 000000000000..bf2aed8b4cba
--- /dev/null
+++ b/drivers/fpga/intel/afu.c
@@ -0,0 +1,965 @@
+/*
+ * Driver for FPGA Accelerated Function Unit (AFU)
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Wu Hao <hao.wu@linux.intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/uaccess.h>
+#include <linux/stddef.h>
+#include <linux/errno.h>
+#include <linux/delay.h>
+#include <linux/fs.h>
+#include <linux/dma-mapping.h>
+#include <linux/intel-fpga.h>
+
+#include "afu.h"
+
+/* sysfs attributes for port_hdr feature */
+static ssize_t
+revision_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct feature_port_header *port_hdr
+		= get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+	struct feature_header header;
+
+	header.csr = readq(&port_hdr->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+
+static DEVICE_ATTR_RO(revision);
+
+static ssize_t
+id_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	int id = fpga_port_id(to_platform_device(dev));
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", id);
+}
+static DEVICE_ATTR_RO(id);
+
+static ssize_t
+ltr_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct feature_port_header *port_hdr;
+	struct feature_port_control control;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	control.csr = readq(&port_hdr->control);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", control.latency_tolerance);
+}
+static DEVICE_ATTR_RO(ltr);
+
+static ssize_t
+userclk_freqcmd_show(struct device *dev, struct device_attribute *attr,
+		     char *buf)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_port_header *port_hdr;
+	u64 userclk_freq_cmd;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	mutex_lock(&pdata->lock);
+	userclk_freq_cmd = readq(&port_hdr->user_clk_freq_cmd0);
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", userclk_freq_cmd);
+}
+
+static ssize_t
+userclk_freqcmd_store(struct device *dev, struct device_attribute *attr,
+		      const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_port_header *port_hdr;
+	u64 userclk_freq_cmd;
+	int err;
+
+	err = kstrtou64(buf, 0, &userclk_freq_cmd);
+	if (err)
+		return err;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	mutex_lock(&pdata->lock);
+	writeq(userclk_freq_cmd, &port_hdr->user_clk_freq_cmd0);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+static DEVICE_ATTR_RW(userclk_freqcmd);
+
+static ssize_t
+userclk_freqcntrcmd_show(struct device *dev, struct device_attribute *attr,
+			 char *buf)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_port_header *port_hdr;
+	u64 userclk_freqcntr_cmd;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	mutex_lock(&pdata->lock);
+	userclk_freqcntr_cmd = readq(&port_hdr->user_clk_freq_cmd1);
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", userclk_freqcntr_cmd);
+}
+
+static ssize_t
+userclk_freqcntrcmd_store(struct device *dev, struct device_attribute *attr,
+			  const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_port_header *port_hdr;
+	u64 userclk_freqcntr_cmd;
+	int err;
+
+	err = kstrtou64(buf, 0, &userclk_freqcntr_cmd);
+	if (err)
+		return err;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	mutex_lock(&pdata->lock);
+	writeq(userclk_freqcntr_cmd, &port_hdr->user_clk_freq_cmd1);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+static DEVICE_ATTR_RW(userclk_freqcntrcmd);
+
+static ssize_t
+userclk_freqsts_show(struct device *dev, struct device_attribute *attr,
+		     char *buf)
+{
+	struct feature_port_header *port_hdr;
+	u64 userclk_freq_sts;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	userclk_freq_sts = readq(&port_hdr->user_clk_freq_sts0);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", userclk_freq_sts);
+}
+static DEVICE_ATTR_RO(userclk_freqsts);
+
+static ssize_t
+userclk_freqcntrsts_show(struct device *dev, struct device_attribute *attr,
+			 char *buf)
+{
+	struct feature_port_header *port_hdr;
+	u64 userclk_freqcntr_sts;
+
+	port_hdr = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_HEADER);
+
+	userclk_freqcntr_sts = readq(&port_hdr->user_clk_freq_sts1);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", userclk_freqcntr_sts);
+}
+static DEVICE_ATTR_RO(userclk_freqcntrsts);
+
+static const struct attribute *port_hdr_attrs[] = {
+	&dev_attr_revision.attr,
+	&dev_attr_id.attr,
+	&dev_attr_ltr.attr,
+	&dev_attr_userclk_freqcmd.attr,
+	&dev_attr_userclk_freqcntrcmd.attr,
+	&dev_attr_userclk_freqsts.attr,
+	&dev_attr_userclk_freqcntrsts.attr,
+	NULL,
+};
+
+static int port_hdr_init(struct platform_device *pdev, struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "PORT HDR Init.\n");
+
+	fpga_port_reset(pdev);
+
+	return sysfs_create_files(&pdev->dev.kobj, port_hdr_attrs);
+}
+
+static void port_hdr_uinit(struct platform_device *pdev,
+					struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "PORT HDR UInit.\n");
+
+	sysfs_remove_files(&pdev->dev.kobj, port_hdr_attrs);
+}
+
+static long
+port_hdr_ioctl(struct platform_device *pdev, struct feature *feature,
+					unsigned int cmd, unsigned long arg)
+{
+	long ret;
+
+	switch (cmd) {
+	case FPGA_PORT_RESET:
+		if (!arg)
+			ret = fpga_port_reset(pdev);
+		else
+			ret = -EINVAL;
+		break;
+	default:
+		dev_dbg(&pdev->dev, "%x cmd not handled", cmd);
+		ret = -ENODEV;
+	}
+
+	return ret;
+}
+
+struct feature_ops port_hdr_ops = {
+	.init = port_hdr_init,
+	.uinit = port_hdr_uinit,
+	.ioctl = port_hdr_ioctl,
+	.test = port_hdr_test,
+};
+
+/* sysfs attributes for port_uafu feature */
+static ssize_t
+afu_id_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_port_header *port_hdr =
+			get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_UAFU);
+	u64 guidl;
+	u64 guidh;
+
+	mutex_lock(&pdata->lock);
+	guidl = readq(&port_hdr->afu_header.guid.b[0]);
+	guidh = readq(&port_hdr->afu_header.guid.b[8]);
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "%016llx%016llx\n", guidh, guidl);
+}
+static DEVICE_ATTR_RO(afu_id);
+
+static const struct attribute *port_uafu_attrs[] = {
+	&dev_attr_afu_id.attr,
+	NULL
+};
+
+static int port_afu_init(struct platform_device *pdev, struct feature *feature)
+{
+	struct resource *res = &pdev->resource[feature->resource_index];
+	u32 flags = FPGA_REGION_READ | FPGA_REGION_WRITE | FPGA_REGION_MMAP;
+	int ret;
+
+	dev_dbg(&pdev->dev, "PORT AFU Init.\n");
+
+	ret = afu_region_add(dev_get_platdata(&pdev->dev),
+			     FPGA_PORT_INDEX_UAFU, resource_size(res),
+			     res->start, flags);
+	if (ret)
+		return ret;
+
+	return sysfs_create_files(&pdev->dev.kobj, port_uafu_attrs);
+}
+
+static void port_afu_uinit(struct platform_device *pdev,
+					struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "PORT AFU UInit.\n");
+
+	sysfs_remove_files(&pdev->dev.kobj, port_uafu_attrs);
+}
+
+struct feature_ops port_afu_ops = {
+	.init = port_afu_init,
+	.uinit = port_afu_uinit,
+};
+
+static u8 port_umsg_get_num(struct device *dev)
+{
+	struct feature_port_umsg *port_umsg;
+	struct feature_port_umsg_cap capability;
+
+	port_umsg = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_UMSG);
+
+	capability.csr = readq(&port_umsg->capability);
+
+	return capability.umsg_allocated;
+}
+
+#define UMSG_EN_POLL_INVL 10 /* us */
+#define UMSG_EN_POLL_TIMEOUT 1000 /* us */
+
+static int port_umsg_enable(struct device *dev, bool enable)
+{
+	struct feature_port_umsg *port_umsg;
+	struct feature_port_umsg_cap capability;
+
+	port_umsg = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_UMSG);
+
+	capability.csr = readq(&port_umsg->capability);
+
+	/* Return directly if UMSG is already enabled/disabled */
+	if ((enable && capability.umsg_enable) ||
+			!(enable || capability.umsg_enable))
+		return 0;
+
+	capability.umsg_enable = enable;
+	writeq(capability.csr, &port_umsg->capability);
+
+	/*
+	 * Each time umsg engine enabled/disabled, driver polls the
+	 * init_complete bit for confirmation.
+	 */
+	capability.umsg_init_complete = !!enable;
+
+	if (fpga_wait_register_field(umsg_init_complete, capability,
+				     &port_umsg->capability,
+				     UMSG_EN_POLL_TIMEOUT, UMSG_EN_POLL_INVL)) {
+		dev_err(dev, "timeout, fail to %s umsg\n",
+					enable ? "enable" : "disable");
+		return -ETIMEDOUT;
+	}
+
+	return 0;
+}
+
+static bool port_umsg_is_enabled(struct device *dev)
+{
+	struct feature_port_umsg *port_umsg;
+	struct feature_port_umsg_cap capability;
+
+	port_umsg = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_UMSG);
+
+	capability.csr = readq(&port_umsg->capability);
+
+	return capability.umsg_enable;
+}
+
+static void port_umsg_set_mode(struct device *dev, u32 mode)
+{
+	struct feature_port_umsg *port_umsg;
+	struct feature_port_umsg_mode umode;
+
+	port_umsg = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_UMSG);
+
+	umode.csr = readq(&port_umsg->mode);
+	umode.umsg_hint_enable = mode;
+	writeq(umode.csr, &port_umsg->mode);
+}
+
+static u64 port_umsg_get_addr(struct device *dev)
+{
+	struct feature_port_umsg *port_umsg;
+	struct feature_port_umsg_baseaddr baseaddr;
+
+	port_umsg = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_UMSG);
+
+	baseaddr.csr = readq(&port_umsg->baseaddr);
+
+	return baseaddr.base_addr;
+}
+
+static void port_umsg_set_addr(struct device *dev, u64 iova)
+{
+	struct feature_port_umsg *port_umsg;
+	struct feature_port_umsg_baseaddr baseaddr;
+
+	port_umsg = get_feature_ioaddr_by_index(dev, PORT_FEATURE_ID_UMSG);
+
+	baseaddr.csr = readq(&port_umsg->baseaddr);
+	baseaddr.base_addr = iova;
+	writeq(baseaddr.csr, &port_umsg->baseaddr);
+}
+
+static int afu_port_umsg_enable(struct device *dev, bool enable)
+{
+	if (enable && !port_umsg_get_addr(dev)) {
+		dev_dbg(dev, "umsg base addr is not configured\n");
+		return -EIO;
+	}
+
+	return port_umsg_enable(dev, enable);
+}
+
+static int afu_port_umsg_set_addr(struct device *dev, u64 iova)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+	struct fpga_afu_dma_region *dma_region;
+	u64 size = afu->num_umsgs * PAGE_SIZE;
+
+	/* Make sure base addr is configured only when umsg is disabled */
+	if (port_umsg_is_enabled(dev)) {
+		dev_dbg(dev, "umsg is still enabled\n");
+		return -EIO;
+	}
+
+	if (iova) {
+		/* Check input, only accept page-aligned region for umsg */
+		if (!PAGE_ALIGNED(iova))
+			return -EINVAL;
+
+		/* Check overflow */
+		if (iova + size < iova)
+			return -EINVAL;
+
+		/* Check if any dma region matches with iova for umsg */
+		dma_region = afu_dma_region_find(pdata, iova, size);
+		if (!dma_region) {
+			dev_dbg(dev, "dma region not found for umsg\n");
+			return -EINVAL;
+		}
+
+		port_umsg_set_addr(dev, iova);
+
+		/* Mark the region to prevent it from unexpected unmapping */
+		dma_region->in_use = true;
+	} else {
+		/* Read current iova from hardware */
+		iova = port_umsg_get_addr(dev);
+		if (!iova)
+			return 0;
+
+		/* Check overflow */
+		if (WARN_ON(iova + size < iova))
+			return -EINVAL;
+
+		/* Check if any dma region matches with iova for umsg */
+		dma_region = afu_dma_region_find(pdata, iova, size);
+		if (WARN_ON(!dma_region))
+			return -ENODEV;
+
+		port_umsg_set_addr(dev, 0);
+
+		/* Unmark the region */
+		dma_region->in_use = false;
+	}
+
+	return 0;
+}
+
+static int afu_port_umsg_set_mode(struct device *dev, u32 mode)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+
+	if (mode >> afu->num_umsgs) {
+		dev_dbg(dev, "invaild UMsg config hint_bitmap\n");
+		return -EINVAL;
+	}
+
+	port_umsg_set_mode(dev, mode);
+
+	return 0;
+}
+
+static void afu_port_umsg_halt(struct device *dev)
+{
+	if (is_feature_present(dev, PORT_FEATURE_ID_UMSG)) {
+		afu_port_umsg_enable(dev, false);
+		afu_port_umsg_set_addr(dev, 0);
+		afu_port_umsg_set_mode(dev, 0);
+	}
+}
+
+static long afu_umsg_ioctl_enable(struct feature_platform_data *pdata,
+				  bool enable, unsigned long arg)
+{
+	long ret;
+
+	if (arg)
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	ret = afu_port_umsg_enable(&pdata->dev->dev, enable);
+	mutex_unlock(&pdata->lock);
+
+	return ret;
+}
+
+static long
+afu_umsg_ioctl_set_mode(struct feature_platform_data *pdata, void __user *arg)
+{
+	struct fpga_port_umsg_cfg uconfig;
+	unsigned long minsz;
+	long ret;
+
+	minsz = offsetofend(struct fpga_port_umsg_cfg, hint_bitmap);
+
+	if (copy_from_user(&uconfig, arg, minsz))
+		return -EFAULT;
+
+	if (uconfig.argsz < minsz || uconfig.flags)
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	ret = afu_port_umsg_set_mode(&pdata->dev->dev, uconfig.hint_bitmap);
+	mutex_unlock(&pdata->lock);
+
+	return ret;
+}
+
+static long afu_umsg_ioctl_set_base_addr(struct feature_platform_data *pdata,
+						void __user *arg)
+{
+	struct fpga_port_umsg_base_addr baseaddr;
+	unsigned long minsz;
+	long ret;
+
+	minsz = offsetofend(struct fpga_port_umsg_base_addr, iova);
+
+	if (copy_from_user(&baseaddr, arg, minsz))
+		return -EFAULT;
+
+	if (baseaddr.argsz < minsz || baseaddr.flags)
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	ret = afu_port_umsg_set_addr(&pdata->dev->dev, baseaddr.iova);
+	mutex_unlock(&pdata->lock);
+
+	return ret;
+}
+
+static int port_umsg_init(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_afu *afu;
+
+	dev_dbg(&pdev->dev, "PORT UMSG Init.\n");
+
+	mutex_lock(&pdata->lock);
+	afu = fpga_pdata_get_private(pdata);
+	afu->num_umsgs = port_umsg_get_num(&pdev->dev);
+	WARN_ON(!afu->num_umsgs || afu->num_umsgs > MAX_PORT_UMSG_NUM);
+	mutex_unlock(&pdata->lock);
+
+	return 0;
+}
+
+static void port_umsg_uinit(struct platform_device *pdev,
+					struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "PORT UMSG UInit.\n");
+}
+
+static long
+port_umsg_ioctl(struct platform_device *pdev, struct feature *feature,
+					unsigned int cmd, unsigned long arg)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	long ret;
+
+	switch (cmd) {
+	case FPGA_PORT_UMSG_ENABLE:
+		return afu_umsg_ioctl_enable(pdata, true, arg);
+	case FPGA_PORT_UMSG_DISABLE:
+		return afu_umsg_ioctl_enable(pdata, false, arg);
+	case FPGA_PORT_UMSG_SET_MODE:
+		return afu_umsg_ioctl_set_mode(pdata, (void __user *)arg);
+	case FPGA_PORT_UMSG_SET_BASE_ADDR:
+		return afu_umsg_ioctl_set_base_addr(pdata, (void __user *)arg);
+	default:
+		dev_dbg(&pdev->dev, "%x cmd not handled", cmd);
+		ret = -ENODEV;
+	}
+
+	return ret;
+}
+
+struct feature_ops port_umsg_ops = {
+	.init = port_umsg_init,
+	.uinit = port_umsg_uinit,
+	.test = port_umsg_test,
+	.ioctl = port_umsg_ioctl,
+};
+
+static int port_stp_init(struct platform_device *pdev, struct feature *feature)
+{
+	struct resource *res = &pdev->resource[feature->resource_index];
+	u32 flags = FPGA_REGION_READ | FPGA_REGION_WRITE | FPGA_REGION_MMAP;
+
+	dev_dbg(&pdev->dev, "PORT STP Init.\n");
+
+	return afu_region_add(dev_get_platdata(&pdev->dev),
+			      FPGA_PORT_INDEX_STP, resource_size(res),
+			      res->start, flags);
+}
+
+static void port_stp_uinit(struct platform_device *pdev,
+					struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "PORT STP UInit.\n");
+}
+
+struct feature_ops port_stp_ops = {
+	.init = port_stp_init,
+	.uinit = port_stp_uinit,
+	.test = port_stp_test,
+};
+
+static struct feature_driver port_feature_drvs[] = {
+	{
+		.name = PORT_FEATURE_HEADER,
+		.ops = &port_hdr_ops,
+	},
+	{
+		.name = PORT_FEATURE_UAFU,
+		.ops = &port_afu_ops,
+	},
+	{
+		.name = PORT_FEATURE_ERR,
+		.ops = &port_err_ops,
+	},
+	{
+		.name = PORT_FEATURE_UMSG,
+		.ops = &port_umsg_ops,
+	},
+	{
+		.name = PORT_FEATURE_STP,
+		.ops = &port_stp_ops,
+	},
+	{
+		.ops = NULL,
+	}
+};
+
+static int afu_open(struct inode *inode, struct file *filp)
+{
+	struct platform_device *fdev = fpga_inode_to_feature_dev(inode);
+	struct feature_platform_data *pdata;
+	int ret;
+
+	pdata = dev_get_platdata(&fdev->dev);
+	if (WARN_ON(!pdata))
+		return -ENODEV;
+
+	if (filp->f_flags & O_EXCL)
+		ret = feature_dev_use_excl_begin(pdata);
+	else
+		ret = feature_dev_use_begin(pdata);
+
+	if (ret)
+		return ret;
+
+	dev_dbg(&fdev->dev, "Device File Opened %d Times\n", pdata->open_count);
+	filp->private_data = fdev;
+
+	return 0;
+}
+
+static int afu_release(struct inode *inode, struct file *filp)
+{
+	struct platform_device *pdev = filp->private_data;
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+
+	dev_dbg(&pdev->dev, "Device File Release\n");
+	mutex_lock(&pdata->lock);
+	__feature_dev_use_end(pdata);
+
+	if (!pdata->open_count) {
+		afu_port_umsg_halt(&pdata->dev->dev);
+		__fpga_port_reset(pdev);
+		afu_dma_region_destroy(pdata);
+	}
+	mutex_unlock(&pdata->lock);
+	return 0;
+}
+
+static long afu_ioctl_check_extension(struct feature_platform_data *pdata,
+				     unsigned long arg)
+{
+	/* No extension support for now */
+	return 0;
+}
+
+static long
+afu_ioctl_get_info(struct feature_platform_data *pdata, void __user *arg)
+{
+	struct fpga_port_info info;
+	struct fpga_afu *afu;
+	unsigned long minsz;
+
+	minsz = offsetofend(struct fpga_port_info, num_umsgs);
+
+	if (copy_from_user(&info, arg, minsz))
+		return -EFAULT;
+
+	if (info.argsz < minsz)
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	afu = fpga_pdata_get_private(pdata);
+	info.flags = 0;
+	info.num_regions = afu->num_regions;
+	info.num_umsgs = afu->num_umsgs;
+	mutex_unlock(&pdata->lock);
+
+	if (copy_to_user(arg, &info, sizeof(info)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static long
+afu_ioctl_get_region_info(struct feature_platform_data *pdata, void __user *arg)
+{
+	struct fpga_port_region_info rinfo;
+	struct fpga_afu_region region;
+	unsigned long minsz;
+	long ret;
+
+	minsz = offsetofend(struct fpga_port_region_info, offset);
+
+	if (copy_from_user(&rinfo, arg, minsz))
+		return -EFAULT;
+
+	if (rinfo.argsz < minsz || rinfo.padding)
+		return -EINVAL;
+
+	ret = afu_get_region_by_index(pdata, rinfo.index, &region);
+	if (ret)
+		return ret;
+
+	rinfo.flags = region.flags;
+	rinfo.size = region.size;
+	rinfo.offset = region.offset;
+
+	if (copy_to_user(arg, &rinfo, sizeof(rinfo)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static long
+afu_ioctl_dma_map(struct feature_platform_data *pdata, void __user *arg)
+{
+	struct fpga_port_dma_map map;
+	unsigned long minsz;
+	long ret;
+
+	minsz = offsetofend(struct fpga_port_dma_map, iova);
+
+	if (copy_from_user(&map, arg, minsz))
+		return -EFAULT;
+
+	if (map.argsz < minsz || map.flags)
+		return -EINVAL;
+
+	ret = afu_dma_map_region(pdata, map.user_addr, map.length, &map.iova);
+	if (ret)
+		return ret;
+
+	if (copy_to_user(arg, &map, sizeof(map))) {
+		afu_dma_unmap_region(pdata, map.iova);
+		return -EFAULT;
+	}
+
+	dev_dbg(&pdata->dev->dev, "dma map: ua=%llx, len=%llx, iova=%llx\n",
+				(unsigned long long)map.user_addr,
+				(unsigned long long)map.length,
+				(unsigned long long)map.iova);
+
+	return 0;
+}
+
+static long
+afu_ioctl_dma_unmap(struct feature_platform_data *pdata, void __user *arg)
+{
+	struct fpga_port_dma_unmap unmap;
+	unsigned long minsz;
+
+	minsz = offsetofend(struct fpga_port_dma_unmap, iova);
+
+	if (copy_from_user(&unmap, arg, minsz))
+		return -EFAULT;
+
+	if (unmap.argsz < minsz || unmap.flags)
+		return -EINVAL;
+
+	return afu_dma_unmap_region(pdata, unmap.iova);
+}
+
+static long afu_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	struct platform_device *pdev = filp->private_data;
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct feature *f;
+	long ret;
+
+	dev_dbg(&pdev->dev, "%s cmd 0x%x\n", __func__, cmd);
+
+	switch (cmd) {
+	case FPGA_GET_API_VERSION:
+		return FPGA_API_VERSION;
+	case FPGA_CHECK_EXTENSION:
+		return afu_ioctl_check_extension(pdata, arg);
+	case FPGA_PORT_GET_INFO:
+		return afu_ioctl_get_info(pdata, (void __user *)arg);
+	case FPGA_PORT_GET_REGION_INFO:
+		return afu_ioctl_get_region_info(pdata, (void __user *)arg);
+	case FPGA_PORT_DMA_MAP:
+		return afu_ioctl_dma_map(pdata, (void __user *)arg);
+	case FPGA_PORT_DMA_UNMAP:
+		return afu_ioctl_dma_unmap(pdata, (void __user *)arg);
+	default:
+		/*
+		 * Let sub-feature's ioctl function to handle the cmd
+		 * Sub-feature's ioctl returns -ENODEV when cmd is not
+		 * handled in this sub feature, and returns 0 and other
+		 * error code if cmd is handled.
+		 */
+		fpga_dev_for_each_feature(pdata, f)
+			if (f->ops && f->ops->ioctl) {
+				ret = f->ops->ioctl(pdev, f, cmd, arg);
+				if (ret == -ENODEV)
+					continue;
+				else
+					return ret;
+			}
+	}
+
+	return -EINVAL;
+}
+
+static int afu_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	struct fpga_afu_region region;
+	struct platform_device *pdev = filp->private_data;
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	u64 size = vma->vm_end - vma->vm_start;
+	u64 offset;
+	int ret;
+
+	if (!(vma->vm_flags & VM_SHARED) && (vma->vm_flags & VM_WRITE))
+		return -EINVAL;
+
+	offset = vma->vm_pgoff << PAGE_SHIFT;
+	ret = afu_get_region_by_offset(pdata, offset, size, &region);
+	if (ret)
+		return ret;
+
+	if (!(region.flags & FPGA_REGION_MMAP))
+		return -EINVAL;
+
+	if ((vma->vm_flags & VM_READ) && !(region.flags & FPGA_REGION_READ))
+		return -EPERM;
+
+	if ((vma->vm_flags & VM_WRITE) && !(region.flags & FPGA_REGION_WRITE))
+		return -EPERM;
+
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+	return remap_pfn_range(vma, vma->vm_start,
+			(region.phys + (offset - region.offset)) >> PAGE_SHIFT,
+			size, vma->vm_page_prot);
+}
+
+static const struct file_operations afu_fops = {
+	.owner = THIS_MODULE,
+	.open = afu_open,
+	.release = afu_release,
+	.unlocked_ioctl = afu_ioctl,
+	.mmap = afu_mmap,
+};
+
+static int afu_dev_init(struct platform_device *pdev)
+{
+	struct fpga_afu *afu;
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+
+	afu = devm_kzalloc(&pdev->dev, sizeof(*afu), GFP_KERNEL);
+	if (!afu)
+		return -ENOMEM;
+
+	afu->pdata = pdata;
+
+	mutex_lock(&pdata->lock);
+	fpga_pdata_set_private(pdata, afu);
+	afu_region_init(pdata);
+	afu_dma_region_init(pdata);
+	mutex_unlock(&pdata->lock);
+	return 0;
+}
+
+static int afu_dev_destroy(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_afu *afu;
+
+	mutex_lock(&pdata->lock);
+	afu = fpga_pdata_get_private(pdata);
+	afu_region_destroy(pdata);
+	afu_dma_region_destroy(pdata);
+	fpga_pdata_set_private(pdata, NULL);
+	mutex_unlock(&pdata->lock);
+
+	devm_kfree(&pdev->dev, afu);
+	return 0;
+}
+
+static int afu_probe(struct platform_device *pdev)
+{
+	int ret;
+
+	dev_dbg(&pdev->dev, "%s\n", __func__);
+
+	ret = afu_dev_init(pdev);
+	if (ret)
+		goto exit;
+
+	ret = fpga_dev_feature_init(pdev, port_feature_drvs);
+	if (ret)
+		goto dev_destroy;
+
+	ret = fpga_register_dev_ops(pdev, &afu_fops, THIS_MODULE);
+	if (ret) {
+		fpga_dev_feature_uinit(pdev);
+		goto dev_destroy;
+	}
+
+	return 0;
+
+dev_destroy:
+	afu_dev_destroy(pdev);
+exit:
+	return ret;
+}
+
+static int afu_remove(struct platform_device *pdev)
+{
+	dev_dbg(&pdev->dev, "%s\n", __func__);
+
+	fpga_dev_feature_uinit(pdev);
+	fpga_unregister_dev_ops(pdev);
+	afu_dev_destroy(pdev);
+	return 0;
+}
+
+static struct platform_driver afu_driver = {
+	.driver	= {
+		.name    = "intel-fpga-port",
+	},
+	.probe   = afu_probe,
+	.remove  = afu_remove,
+};
+
+module_platform_driver(afu_driver);
+
+MODULE_DESCRIPTION("FPGA Accelerated Function Unit driver");
+MODULE_AUTHOR("Intel Corporation");
+MODULE_LICENSE("GPL v2");
+MODULE_ALIAS("platform:intel-fpga-port");
diff --git a/drivers/fpga/intel/afu.h b/drivers/fpga/intel/afu.h
new file mode 100644
index 000000000000..b79c6e5e953f
--- /dev/null
+++ b/drivers/fpga/intel/afu.h
@@ -0,0 +1,78 @@
+/*
+ * FPGA Accelerated Function Unit (AFU) Header
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *     Wu Hao <hao.wu@linux.intel.com>
+ *     Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *     Joseph Grecco <joe.grecco@intel.com>
+ *     Enno Luebbers <enno.luebbers@intel.com>
+ *     Tim Whisonant <tim.whisonant@intel.com>
+ *     Ananda Ravuri <ananda.ravuri@intel.com>
+ *     Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#ifndef __INTEL_AFU_H
+#define __INTEL_AFU_H
+
+#include "feature-dev.h"
+
+struct fpga_afu_region {
+	u32 index;
+	u32 flags;
+	u64 size;
+	u64 offset;
+	u64 phys;
+	struct list_head node;
+};
+
+struct fpga_afu_dma_region {
+	u64 user_addr;
+	u64 length;
+	u64 iova;
+	struct page **pages;
+	struct rb_node node;
+	bool in_use;
+};
+
+struct fpga_afu {
+	u64 region_cur_offset;
+	int num_regions;
+	u8 num_umsgs;
+	struct list_head regions;
+	struct rb_root dma_regions;
+
+	struct feature_platform_data *pdata;
+};
+
+void afu_region_init(struct feature_platform_data *pdata);
+int afu_region_add(struct feature_platform_data *pdata, u32 region_index,
+		   u64 region_size, u64 phys, u32 flags);
+void afu_region_destroy(struct feature_platform_data *pdata);
+int afu_get_region_by_index(struct feature_platform_data *pdata,
+			    u32 region_index, struct fpga_afu_region *pregion);
+int afu_get_region_by_offset(struct feature_platform_data *pdata,
+			    u64 offset, u64 size,
+			    struct fpga_afu_region *pregion);
+
+void afu_dma_region_init(struct feature_platform_data *pdata);
+void afu_dma_region_destroy(struct feature_platform_data *pdata);
+long afu_dma_map_region(struct feature_platform_data *pdata,
+		       u64 user_addr, u64 length, u64 *iova);
+long afu_dma_unmap_region(struct feature_platform_data *pdata, u64 iova);
+struct fpga_afu_dma_region *afu_dma_region_find(
+		struct feature_platform_data *pdata, u64 iova, u64 size);
+
+int port_hdr_test(struct platform_device *pdev, struct feature *feature);
+int port_err_test(struct platform_device *pdev, struct feature *feature);
+int port_umsg_test(struct platform_device *pdev, struct feature *feature);
+int port_stp_test(struct platform_device *pdev, struct feature *feature);
+
+extern struct feature_ops port_err_ops;
+
+#endif
diff --git a/drivers/fpga/intel/dma-region.c b/drivers/fpga/intel/dma-region.c
new file mode 100644
index 000000000000..e6bba9479f2f
--- /dev/null
+++ b/drivers/fpga/intel/dma-region.c
@@ -0,0 +1,372 @@
+/*
+ * Driver for FPGA Accelerated Function Unit (AFU) DMA Region Management
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Wu Hao <hao.wu@linux.intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include <linux/uaccess.h>
+#include <linux/sched/signal.h>
+
+#include "afu.h"
+
+static void put_all_pages(struct page **pages, int npages)
+{
+	int i;
+
+	for (i = 0; i < npages; i++)
+		if (pages[i] != NULL)
+			put_page(pages[i]);
+}
+
+void afu_dma_region_init(struct feature_platform_data *pdata)
+{
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+
+	afu->dma_regions = RB_ROOT;
+}
+
+static long afu_dma_adjust_locked_vm(struct device *dev, long npages, bool incr)
+{
+	unsigned long locked, lock_limit;
+	int ret = 0;
+
+	/* the task is exiting. */
+	if (!current->mm)
+		return 0;
+
+	down_write(&current->mm->mmap_sem);
+
+	if (incr) {
+		locked = current->mm->locked_vm + npages;
+		lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
+
+		if (locked > lock_limit && !capable(CAP_IPC_LOCK))
+			ret = -ENOMEM;
+		else
+			current->mm->locked_vm += npages;
+	} else {
+
+		if (WARN_ON_ONCE(npages > current->mm->locked_vm))
+			npages = current->mm->locked_vm;
+		current->mm->locked_vm -= npages;
+	}
+
+	dev_dbg(dev, "[%d] RLIMIT_MEMLOCK %c%ld %ld/%ld%s\n", current->pid,
+				incr ? '+' : '-',
+				npages << PAGE_SHIFT,
+				current->mm->locked_vm << PAGE_SHIFT,
+				rlimit(RLIMIT_MEMLOCK),
+				ret ? "- execeeded" : "");
+
+	up_write(&current->mm->mmap_sem);
+
+	return ret;
+}
+
+static long afu_dma_pin_pages(struct feature_platform_data *pdata,
+				struct fpga_afu_dma_region *region)
+{
+	long npages = region->length >> PAGE_SHIFT;
+	struct device *dev = &pdata->dev->dev;
+	long ret, pinned;
+
+	ret = afu_dma_adjust_locked_vm(dev, npages, true);
+	if (ret)
+		return ret;
+
+	region->pages = kcalloc(npages, sizeof(struct page *), GFP_KERNEL);
+	if (!region->pages) {
+		afu_dma_adjust_locked_vm(dev, npages, false);
+		return -ENOMEM;
+	}
+
+	pinned = get_user_pages_fast(region->user_addr, npages, 1,
+					region->pages);
+	if (pinned < 0) {
+		ret = pinned;
+		goto err_put_pages;
+	} else if (pinned != npages) {
+		ret = -EFAULT;
+		goto err;
+	}
+
+	dev_dbg(dev, "%ld pages pinned\n", pinned);
+
+	return 0;
+
+err_put_pages:
+	put_all_pages(region->pages, pinned);
+err:
+	kfree(region->pages);
+	afu_dma_adjust_locked_vm(dev, npages, false);
+	return ret;
+}
+
+static void afu_dma_unpin_pages(struct feature_platform_data *pdata,
+				struct fpga_afu_dma_region *region)
+{
+	long npages = region->length >> PAGE_SHIFT;
+	struct device *dev = &pdata->dev->dev;
+
+	put_all_pages(region->pages, npages);
+	kfree(region->pages);
+	afu_dma_adjust_locked_vm(dev, npages, false);
+
+	dev_dbg(dev, "%ld pages unpinned\n", npages);
+}
+
+static bool afu_dma_check_continuous_pages(struct fpga_afu_dma_region *region)
+{
+	int npages = region->length >> PAGE_SHIFT;
+	int i;
+
+	for (i = 0; i < npages - 1; i++)
+		if (page_to_pfn(region->pages[i]) + 1 !=
+					page_to_pfn(region->pages[i+1]))
+			return false;
+
+	return true;
+}
+
+static bool dma_region_check_iova(struct fpga_afu_dma_region *region,
+				  u64 iova, u64 size)
+{
+	if (!size && region->iova != iova)
+		return false;
+
+	return (region->iova <= iova) &&
+		(region->length + region->iova >= iova + size);
+}
+
+/* Need to be called with pdata->lock held */
+static int afu_dma_region_add(struct feature_platform_data *pdata,
+					struct fpga_afu_dma_region *region)
+{
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+	struct rb_node **new, *parent = NULL;
+
+	dev_dbg(&pdata->dev->dev, "add region (iova = %llx)\n",
+					(unsigned long long)region->iova);
+
+	new = &(afu->dma_regions.rb_node);
+
+	while (*new) {
+		struct fpga_afu_dma_region *this;
+
+		this = container_of(*new, struct fpga_afu_dma_region, node);
+
+		parent = *new;
+
+		if (dma_region_check_iova(this, region->iova, region->length))
+			return -EEXIST;
+
+		if (region->iova < this->iova)
+			new = &((*new)->rb_left);
+		else if (region->iova > this->iova)
+			new = &((*new)->rb_right);
+		else
+			return -EEXIST;
+	}
+
+	rb_link_node(&region->node, parent, new);
+	rb_insert_color(&region->node, &afu->dma_regions);
+
+	return 0;
+}
+
+/* Need to be called with pdata->lock held */
+static void afu_dma_region_remove(struct feature_platform_data *pdata,
+					struct fpga_afu_dma_region *region)
+{
+	struct fpga_afu *afu;
+
+	dev_dbg(&pdata->dev->dev, "del region (iova = %llx)\n",
+					(unsigned long long)region->iova);
+
+	afu = fpga_pdata_get_private(pdata);
+	rb_erase(&region->node, &afu->dma_regions);
+}
+
+/* Need to be called with pdata->lock held */
+void afu_dma_region_destroy(struct feature_platform_data *pdata)
+{
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+	struct rb_node *node = rb_first(&afu->dma_regions);
+	struct fpga_afu_dma_region *region;
+
+	while (node) {
+		region = container_of(node, struct fpga_afu_dma_region, node);
+
+		dev_dbg(&pdata->dev->dev, "del region (iova = %llx)\n",
+					(unsigned long long)region->iova);
+
+		rb_erase(node, &afu->dma_regions);
+
+		if (region->iova)
+			dma_unmap_page(fpga_pdata_to_pcidev(pdata),
+					region->iova, region->length,
+					DMA_BIDIRECTIONAL);
+
+		if (region->pages)
+			afu_dma_unpin_pages(pdata, region);
+
+		node = rb_next(node);
+		kfree(region);
+	}
+}
+
+/*
+ * It finds the dma region from the rbtree based on @iova and @size:
+ * - if @size == 0, it finds the dma region which starts from @iova
+ * - otherwise, it finds the dma region which fully contains
+ *   [@iova, @iova+size)
+ * If nothing is matched returns NULL.
+ *
+ * Need to be called with pdata->lock held.
+ */
+struct fpga_afu_dma_region *
+afu_dma_region_find(struct feature_platform_data *pdata, u64 iova, u64 size)
+{
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+	struct rb_node *node = afu->dma_regions.rb_node;
+	struct device *dev = &pdata->dev->dev;
+
+	while (node) {
+		struct fpga_afu_dma_region *region;
+
+		region = container_of(node, struct fpga_afu_dma_region, node);
+
+		if (dma_region_check_iova(region, iova, size)) {
+			dev_dbg(dev, "find region (iova = %llx)\n",
+				(unsigned long long)region->iova);
+			return region;
+		}
+
+		if (iova < region->iova)
+			node = node->rb_left;
+		else if (iova > region->iova)
+			node = node->rb_right;
+		else
+			/* the iova region is not fully covered. */
+			break;
+	}
+
+	dev_dbg(dev, "region with iova %llx and size %llx is not found\n",
+		(unsigned long long)iova, (unsigned long long)size);
+	return NULL;
+}
+
+static struct fpga_afu_dma_region *
+afu_dma_region_find_iova(struct feature_platform_data *pdata, u64 iova)
+{
+	return afu_dma_region_find(pdata, iova, 0);
+}
+
+long afu_dma_map_region(struct feature_platform_data *pdata,
+		       u64 user_addr, u64 length, u64 *iova)
+{
+	struct fpga_afu_dma_region *region;
+	int ret;
+
+	/*
+	 * Check Inputs, only accept page-aligned user memory region with
+	 * valid length.
+	 */
+	if (!PAGE_ALIGNED(user_addr) || !PAGE_ALIGNED(length) || !length)
+		return -EINVAL;
+
+	/* Check overflow */
+	if (user_addr + length < user_addr)
+		return -EINVAL;
+
+	if (!access_ok(VERIFY_WRITE, user_addr, length))
+		return -EINVAL;
+
+	region = kzalloc(sizeof(*region), GFP_KERNEL);
+	if (!region)
+		return -ENOMEM;
+
+	region->user_addr = user_addr;
+	region->length = length;
+
+	/* Pin the user memory region */
+	ret = afu_dma_pin_pages(pdata, region);
+	if (ret) {
+		dev_err(&pdata->dev->dev, "fail to pin memory region\n");
+		goto free_region;
+	}
+
+	/* Only accept continuous pages, return error if no */
+	if (!afu_dma_check_continuous_pages(region)) {
+		dev_err(&pdata->dev->dev, "pages are not continuous\n");
+		ret = -EINVAL;
+		goto unpin_pages;
+	}
+
+	/* As pages are continuous then start to do DMA mapping */
+	region->iova = dma_map_page(fpga_pdata_to_pcidev(pdata),
+				    region->pages[0], 0,
+				    region->length,
+				    DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(&pdata->dev->dev, region->iova)) {
+		dev_err(&pdata->dev->dev, "fail to map dma mapping\n");
+		ret = -EFAULT;
+		goto unpin_pages;
+	}
+
+	*iova = region->iova;
+
+	mutex_lock(&pdata->lock);
+	ret = afu_dma_region_add(pdata, region);
+	mutex_unlock(&pdata->lock);
+	if (ret) {
+		dev_err(&pdata->dev->dev, "fail to add dma region\n");
+		goto unmap_dma;
+	}
+
+	return 0;
+
+unmap_dma:
+	dma_unmap_page(fpga_pdata_to_pcidev(pdata),
+		       region->iova, region->length, DMA_BIDIRECTIONAL);
+unpin_pages:
+	afu_dma_unpin_pages(pdata, region);
+free_region:
+	kfree(region);
+	return ret;
+}
+
+long afu_dma_unmap_region(struct feature_platform_data *pdata, u64 iova)
+{
+	struct fpga_afu_dma_region *region;
+
+	mutex_lock(&pdata->lock);
+	region = afu_dma_region_find_iova(pdata, iova);
+	if (!region) {
+		mutex_unlock(&pdata->lock);
+		return -EINVAL;
+	}
+
+	if (region->in_use) {
+		mutex_unlock(&pdata->lock);
+		return -EBUSY;
+	}
+
+	afu_dma_region_remove(pdata, region);
+	mutex_unlock(&pdata->lock);
+
+	dma_unmap_page(fpga_pdata_to_pcidev(pdata),
+		       region->iova, region->length, DMA_BIDIRECTIONAL);
+	afu_dma_unpin_pages(pdata, region);
+	kfree(region);
+
+	return 0;
+}
diff --git a/drivers/fpga/intel/feature-dev.c b/drivers/fpga/intel/feature-dev.c
new file mode 100644
index 000000000000..04481e986833
--- /dev/null
+++ b/drivers/fpga/intel/feature-dev.c
@@ -0,0 +1,288 @@
+/*
+ * Intel FPGA Feature Device Framework Header
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Zhang Yi <Yi.Z.Zhang@intel.com>
+ *   Wu Hao <hao.wu@linux.intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include <linux/fs.h>
+
+#include "feature-dev.h"
+
+void feature_platform_data_add(struct feature_platform_data *pdata,
+			       int index, const char *name,
+			       int resource_index, void __iomem *ioaddr)
+{
+	WARN_ON(index >= pdata->num);
+
+	pdata->features[index].name = name;
+	pdata->features[index].resource_index = resource_index;
+	pdata->features[index].ioaddr = ioaddr;
+}
+
+int feature_platform_data_size(int num)
+{
+	return sizeof(struct feature_platform_data) +
+		num * sizeof(struct feature);
+}
+
+struct feature_platform_data *
+feature_platform_data_alloc_and_init(struct platform_device *dev, int num)
+{
+	struct feature_platform_data *pdata;
+
+	pdata = kzalloc(feature_platform_data_size(num), GFP_KERNEL);
+	if (pdata) {
+		pdata->dev = dev;
+		pdata->num = num;
+		mutex_init(&pdata->lock);
+	}
+
+	return pdata;
+}
+
+int fme_feature_num(void)
+{
+	return FME_FEATURE_ID_MAX;
+}
+
+int port_feature_num(void)
+{
+	return PORT_FEATURE_ID_MAX;
+}
+
+int fme_feature_to_resource_index(int feature_id)
+{
+	WARN_ON(feature_id >= FME_FEATURE_ID_MAX);
+	return feature_id;
+}
+
+void fpga_dev_feature_uinit(struct platform_device *pdev)
+{
+	struct feature *feature;
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+
+	fpga_dev_for_each_feature(pdata, feature)
+		if (feature->ops) {
+			feature->ops->uinit(pdev, feature);
+			feature->ops = NULL;
+		}
+}
+EXPORT_SYMBOL_GPL(fpga_dev_feature_uinit);
+
+static int
+feature_instance_init(struct platform_device *pdev,
+		      struct feature_platform_data *pdata,
+		      struct feature *feature, struct feature_driver *drv)
+{
+	int ret;
+
+	WARN_ON(!feature->ioaddr);
+
+	if (drv->ops->test) {
+		ret = drv->ops->test(pdev, feature);
+		if (ret)
+			return ret;
+	}
+
+	ret = drv->ops->init(pdev, feature);
+	if (ret)
+		return ret;
+
+	feature->ops = drv->ops;
+	return ret;
+}
+
+int fpga_dev_feature_init(struct platform_device *pdev,
+			  struct feature_driver *feature_drvs)
+{
+	struct feature *feature;
+	struct feature_driver *drv = feature_drvs;
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	int ret;
+
+	while (drv->ops) {
+		fpga_dev_for_each_feature(pdata, feature) {
+			/* skip the feature which is not initialized. */
+			if (!feature->name)
+				continue;
+
+			if (!strcmp(drv->name, feature->name)) {
+				ret = feature_instance_init(pdev, pdata,
+							    feature, drv);
+				if (ret)
+					goto exit;
+			}
+		}
+		drv++;
+	}
+	return 0;
+exit:
+	fpga_dev_feature_uinit(pdev);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(fpga_dev_feature_init);
+
+struct fpga_chardev_info {
+	const char *name;
+	dev_t devt;
+};
+
+/* indexe by enum fpga_devt_type */
+struct fpga_chardev_info fpga_chrdevs[] = {
+	{.name = FPGA_FEATURE_DEV_FME},		/* FPGA_DEVT_FME */
+	{.name = FPGA_FEATURE_DEV_PORT},	/* FPGA_DEVT_AFU */
+};
+
+void fpga_chardev_uinit(void)
+{
+	int i;
+
+	for (i = 0; i < FPGA_DEVT_MAX; i++)
+		if (MAJOR(fpga_chrdevs[i].devt)) {
+			unregister_chrdev_region(fpga_chrdevs[i].devt,
+						 MINORMASK);
+			fpga_chrdevs[i].devt = MKDEV(0, 0);
+		}
+}
+
+int fpga_chardev_init(void)
+{
+	int i, ret;
+
+	for (i = 0; i < FPGA_DEVT_MAX; i++) {
+		ret = alloc_chrdev_region(&fpga_chrdevs[i].devt, 0, MINORMASK,
+					  fpga_chrdevs[i].name);
+		if (ret)
+			goto exit;
+	}
+
+	return 0;
+
+exit:
+	fpga_chardev_uinit();
+	return ret;
+}
+
+dev_t fpga_get_devt(enum fpga_devt_type type, int id)
+{
+	WARN_ON(type >= FPGA_DEVT_MAX);
+
+	return MKDEV(MAJOR(fpga_chrdevs[type].devt), id);
+}
+
+int fpga_register_dev_ops(struct platform_device *pdev,
+			  const struct file_operations *fops,
+			  struct module *owner)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+
+	cdev_init(&pdata->cdev, fops);
+	pdata->cdev.owner = owner;
+
+	/*
+	 * set parent to the feature device so that its refcount is
+	 * decreased after the last refcount of cdev is gone, that
+	 * makes sure the feature device is valid during device
+	 * file's life-cycle.
+	 */
+	pdata->cdev.kobj.parent = &pdev->dev.kobj;
+	return cdev_add(&pdata->cdev, pdev->dev.devt, 1);
+}
+EXPORT_SYMBOL_GPL(fpga_register_dev_ops);
+
+void fpga_unregister_dev_ops(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+
+	cdev_del(&pdata->cdev);
+}
+EXPORT_SYMBOL_GPL(fpga_unregister_dev_ops);
+
+int fpga_port_id(struct platform_device *pdev)
+{
+	struct feature_port_header *port_hdr;
+	struct feature_port_capability capability;
+
+	port_hdr = get_feature_ioaddr_by_index(&pdev->dev,
+					       PORT_FEATURE_ID_HEADER);
+	WARN_ON(!port_hdr);
+
+	capability.csr = readq(&port_hdr->capability);
+	return capability.port_number;
+}
+EXPORT_SYMBOL_GPL(fpga_port_id);
+
+/*
+ * Enable Port by clear the port soft reset bit, which is set by default.
+ * The AFU is unable to respond to any MMIO access while in reset.
+ * __fpga_port_enable function should only be used after __fpga_port_disable
+ * function.
+ */
+void __fpga_port_enable(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct feature_port_header *port_hdr;
+	struct feature_port_control control;
+
+	WARN_ON(!pdata->disable_count);
+
+	if (--pdata->disable_count != 0)
+		return;
+
+	port_hdr = get_feature_ioaddr_by_index(&pdev->dev,
+					       PORT_FEATURE_ID_HEADER);
+	WARN_ON(!port_hdr);
+
+	control.csr = readq(&port_hdr->control);
+	control.port_sftrst = 0x0;
+	writeq(control.csr, &port_hdr->control);
+}
+EXPORT_SYMBOL_GPL(__fpga_port_enable);
+
+#define RST_POLL_INVL 10 /* us */
+#define RST_POLL_TIMEOUT 1000 /* us */
+
+int __fpga_port_disable(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct feature_port_header *port_hdr;
+	struct feature_port_control control;
+
+	if (pdata->disable_count++ != 0)
+		return 0;
+
+	port_hdr = get_feature_ioaddr_by_index(&pdev->dev,
+					       PORT_FEATURE_ID_HEADER);
+	WARN_ON(!port_hdr);
+
+	/* Set port soft reset */
+	control.csr = readq(&port_hdr->control);
+	control.port_sftrst = 0x1;
+	writeq(control.csr, &port_hdr->control);
+
+	/*
+	 * HW sets ack bit to 1 when all outstanding requests have been drained
+	 * on this port and minimum soft reset pulse width has elapsed.
+	 * Driver polls port_soft_reset_ack to determine if reset done by HW.
+	 */
+	control.port_sftrst_ack = 1;
+
+	if (fpga_wait_register_field(port_sftrst_ack, control,
+		&port_hdr->control, RST_POLL_TIMEOUT, RST_POLL_INVL)) {
+		dev_err(&pdev->dev, "timeout, fail to reset device\n");
+		return -ETIMEDOUT;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(__fpga_port_disable);
diff --git a/drivers/fpga/intel/feature-dev.h b/drivers/fpga/intel/feature-dev.h
new file mode 100644
index 000000000000..fc8e49f6c721
--- /dev/null
+++ b/drivers/fpga/intel/feature-dev.h
@@ -0,0 +1,1390 @@
+/*
+ * Intel FPGA Feature Device Framework Header
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Zhang Yi <Yi.Z.Zhang@intel.com>
+ *   Wu Hao <hao.wu@linux.intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#ifndef __INTEL_FPGA_FEATURE_H
+#define __INTEL_FPGA_FEATURE_H
+
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <linux/pci.h>
+#include <linux/uuid.h>
+#include <linux/delay.h>
+#include <linux/platform_device.h>
+
+/* each FPGA device has 4 ports at most. */
+#define MAX_FPGA_PORT_NUM 4
+/*
+ * Num of umsgs is up to 255, but only 32 umsgs allow hint mode per spec
+ * so limit max num to 32 for now.
+ */
+#define MAX_PORT_UMSG_NUM 32
+/* one for fme device */
+#define MAX_FEATURE_DEV_NUM	(MAX_FPGA_PORT_NUM + 1)
+
+#define FME_FEATURE_HEADER          "fme_hdr"
+#define FME_FEATURE_THERMAL_MGMT    "fme_thermal"
+#define FME_FEATURE_POWER_MGMT      "fme_power"
+#define FME_FEATURE_GLOBAL_PERF     "fme_gperf"
+#define FME_FEATURE_GLOBAL_ERR      "fme_error"
+#define FME_FEATURE_PR_MGMT         "fme_pr"
+
+#define PORT_FEATURE_HEADER         "port_hdr"
+#define PORT_FEATURE_UAFU           "port_uafu"
+#define PORT_FEATURE_ERR            "port_err"
+#define PORT_FEATURE_UMSG           "port_umsg"
+#define PORT_FEATURE_PR             "port_pr"
+#define PORT_FEATURE_STP            "port_stp"
+
+/*
+ * do not check the revision id as id may be dynamic under
+ * some cases, e.g, UAFU.
+ */
+#define SKIP_REVISION_CHECK		0xff
+
+#define FME_HEADER_REVISION		0
+#define FME_THERMAL_MGMT_REVISION	0
+#define FME_POWER_MGMT_REVISION	0
+#define FME_GLOBAL_PERF_REVISION	0
+#define FME_GLOBAL_ERR_REVISION	0
+#define FME_PR_MGMT_REVISION		1
+
+#define PORT_HEADER_REVISION		0
+/* UAFU's header info depends on the downloaded GBS */
+#define PORT_UAFU_REVISION		SKIP_REVISION_CHECK
+#define PORT_ERR_REVISION		0
+#define PORT_UMSG_REVISION		0
+#define PORT_PR_REVISION		0
+#define PORT_STP_REVISION		1
+
+/*
+ * All headers and structures must be byte-packed to match the
+ * SAS spec.
+ */
+#pragma pack(1)
+
+struct feature_header {
+	union {
+		u64 csr;
+		struct {
+			u16 id:12;
+			u8  revision:4;
+			u32 next_header_offset:24;
+			u32 reserved:20;
+			u8  type:4;
+		};
+	};
+};
+
+struct feature_afu_header {
+	uuid_le guid;
+	union {
+		u64 csr;
+		struct {
+			u64 next_afu:24;
+			u64 reserved:40;
+		};
+	};
+};
+
+struct feature_fme_capability {
+	union {
+		u64 csr;
+		struct {
+			u8  fabric_verid;	/* Fabric version ID */
+			u8  socket_id:1;	/* Socket id */
+			u8  rsvd1:3;		/* Reserved */
+			/* pci0 link available yes /no */
+			u8  pci0_link_avile:1;
+			/* pci1 link available yes /no */
+			u8  pci1_link_avile:1;
+			/* Coherent (QPI/UPI) link available yes /no */
+			u8  qpi_link_avile:1;
+			u8  rsvd2:1;		/* Reserved */
+			/* IOMMU or VT-d supported  yes/no */
+			u8  iommu_support:1;
+			u8  num_ports:3;	/* Number of ports */
+			u8  rsvd3:4;		/* Reserved */
+			/*
+			 * Address width supported in bits
+			 */
+			u8  address_width_bits:6;
+			u8  rsvd4:2;		/* Reserved */
+			/* Size of cache supported in kb */
+			u16 cache_size:12;
+			u8  cache_assoc:4;	/* Cache Associativity */
+			u16 rsvd5:15;		/* Reserved */
+			u8  lock_bit:1;		/* Lock bit */
+		};
+	};
+};
+
+#define FME_AFU_ACCESS_PF		0
+#define FME_AFU_ACCESS_VF		1
+
+struct feature_fme_port {
+	union {
+		u64 csr;
+		struct {
+			u32 port_offset:24;
+			u8  reserved1;
+			u8  port_bar:3;
+			u32 reserved2:20;
+			u8  afu_access_control:1;
+			u8  reserved3:4;
+			u8  port_implemented:1;
+			u8  reserved4:3;
+		};
+	};
+};
+
+struct feature_fme_fab_status {
+	union {
+		u64 csr;
+		struct {
+			u8  upilink_status:4;   /* UPI Link Status */
+			u8  rsvd1:4;		/* Reserved */
+			u8  pci0link_status:1;  /* pci0 link status */
+			u8  rsvd2:3;            /* Reserved */
+			u8  pci1link_status:1;  /* pci1 link status */
+			u64 rsvd3:51;           /* Reserved */
+		};
+	};
+};
+
+struct feature_fme_genprotrange2_base {
+	union {
+		u64 csr;
+		struct {
+			u16 rsvd1;           /* Reserved */
+			/* Base Address of memory range */
+			u8  protected_base_addrss:4;
+			u64 rsvd2:44;           /* Reserved */
+		};
+	};
+};
+
+struct feature_fme_genprotrange2_limit {
+	union {
+		u64 csr;
+		struct {
+			u16 rsvd1;           /* Reserved */
+			/* Limit Address of memory range */
+			u8  protected_limit_addrss:4;
+			u16 rsvd2:11;           /* Reserved */
+			u8  enable_pr:1;        /* Enable GENPROTRANGE check */
+			u32 rsvd3;           /* Reserved */
+		};
+	};
+};
+
+struct feature_fme_dxe_lock {
+	union {
+		u64 csr;
+		struct {
+			/*
+			 * Determines write access to the DXE region CSRs
+			 * 1 - CSR region is locked;
+			 * 0 - it is open for write access.
+			 */
+			u8  dxe_early_lock:1;
+			/*
+			 * Determines write access to the HSSI CSR
+			 * 1 - CSR region is locked;
+			 * 0 - it is open for write access.
+			 */
+			u8  dxe_late_lock:1;
+			u64 rsvd:62;
+		};
+	};
+};
+
+struct feature_fme_hssi_ctrl {
+	union {
+		u64 csr;
+		struct {
+			u32 data;	/* data */
+			u16 address;	/* address */
+			u16 command;	/* command */
+		};
+	};
+};
+
+struct feature_fme_hssi_start {
+	union {
+		u64 csr;
+		struct {
+			u32 data;	/* data */
+			u8  ck:1;	/* Acknowledge*/
+			u8  spare:1;	/* spare */
+			u32 rsvd:30;	/* Reserved */
+		};
+	};
+};
+
+struct feature_fme_header {
+	struct feature_header header;
+	struct feature_afu_header afu_header;
+	u64 reserved;
+	u64 scratchpad;
+	struct feature_fme_capability capability;
+	struct feature_fme_port port[MAX_FPGA_PORT_NUM];
+	struct feature_fme_fab_status fab_status;
+	u64 bitstream_id;
+	u64 bitstream_md;
+	struct feature_fme_genprotrange2_base genprotrange2_base;
+	struct feature_fme_genprotrange2_limit genprotrange2_limit;
+	struct feature_fme_dxe_lock dxe_lock;
+	struct feature_fme_hssi_ctrl hssi_ctrl;
+	struct feature_fme_hssi_start hssi_start;
+};
+
+struct feature_port_capability {
+	union {
+		u64 csr;
+		struct {
+			u8 port_number:2;	/* Port Number 0-3 */
+			u8 rsvd1:6;		/* Reserved */
+			u16 mmio_size;		/* User MMIO size in KB */
+			u8 rsvd2;		/* Reserved */
+			u8 sp_intr_num:4;	/* Supported interrupts num */
+			u32 rsvd3:28;		/* Reserved */
+		};
+	};
+};
+
+struct feature_port_control {
+	union {
+		u64 csr;
+		struct {
+			u8 port_sftrst:1;	/* Port Soft Reset */
+			u8 rsvd1:1;		/* Reserved */
+			u8 latency_tolerance:1;/* '1' >= 40us, '0' < 40us */
+			u8 rsvd2:1;		/* Reserved */
+			u8 port_sftrst_ack:1;	/* HW ACK for Soft Reset */
+			u64 rsvd3:59;		/* Reserved */
+		};
+	};
+};
+
+#define PORT_POWER_STATE_NORMAL		0
+#define PORT_POWER_STATE_AP1		1
+#define PORT_POWER_STATE_AP2		2
+#define PORT_POWER_STATE_AP6		6
+
+struct feature_port_status {
+	union {
+		u64 csr;
+		struct {
+			u8 port_freeze:1;	/* '1' - freezed '0' - normal */
+			u8 rsvd1:7;		/* Reserved */
+			u8 power_state:4;	/* Power State */
+			u64 rsvd2:52;		/* Reserved */
+		};
+	};
+};
+
+/* Port Header Register Set */
+struct feature_port_header {
+	struct feature_header header;
+	struct feature_afu_header afu_header;
+	u64 rsvd1;
+	u64 scratchpad;
+	struct feature_port_capability capability;
+	struct feature_port_control control;
+	struct feature_port_status status;
+	u64 rsvd2;
+	u64 user_clk_freq_cmd0;
+	u64 user_clk_freq_cmd1;
+	u64 user_clk_freq_sts0;
+	u64 user_clk_freq_sts1;
+};
+
+struct feature_fme_tmp_threshold {
+	union {
+		u64 csr;
+		struct {
+			u8  tmp_thshold1:7;	  /* temperature Threshold 1 */
+			/* temperature Threshold 1 enable/disable */
+			u8  tmp_thshold1_enable:1;
+			u8  tmp_thshold2:7;       /* temperature Threshold 2 */
+			/* temperature Threshold 2 enable /disable */
+			u8  tmp_thshold2_enable:1;
+			u8  pro_hot_setpoint:7;   /* Proc Hot set point */
+			u8  rsvd4:1;              /* Reserved */
+			u8  therm_trip_thshold:7; /* Thermeal Trip Threshold */
+			u8  rsvd3:1;              /* Reserved */
+			u8  thshold1_status:1;	  /* Threshold 1 Status */
+			u8  thshold2_status:1;    /* Threshold 2 Status */
+			u8  rsvd5:1;              /* Reserved */
+			/* Thermeal Trip Threshold status */
+			u8  therm_trip_thshold_status:1;
+			u8  rsvd6:4;		  /* Reserved */
+			/* Validation mode- Force Proc Hot */
+			u8  valmodeforce:1;
+			/* Validation mode - Therm trip Hot */
+			u8  valmodetherm:1;
+			u8  rsvd2:2;              /* Reserved */
+			u8  thshold_policy:1;     /* threshold policy */
+			u32 rsvd:19;              /* Reserved */
+		};
+	};
+};
+
+/* Temperature Sensor Read values format 1 */
+struct feature_fme_temp_rdsensor_fmt1 {
+	union {
+		u64 csr;
+		struct {
+			/* Reads out FPGA temperature in celsius */
+			u8  fpga_temp:7;
+			u8  rsvd0:1;			/* Reserved */
+			/* Temperature reading sequence number */
+			u16 tmp_reading_seq_num;
+			/* Temperature reading is valid */
+			u8  tmp_reading_valid:1;
+			u8  rsvd1:7;			/* Reserved */
+			u16 dbg_mode:10;		/* Debug mode */
+			u32 rsvd2:22;			/* Reserved */
+		};
+	};
+};
+
+/* Temperature sensor read values format 2 */
+struct feature_fme_temp_rdsensor_fmt2 {
+	u64 rsvd;	/* Reserved */
+};
+
+/* FME THERNAL FEATURE */
+struct feature_fme_thermal {
+	struct feature_header header;
+	struct feature_fme_tmp_threshold threshold;
+	struct feature_fme_temp_rdsensor_fmt1 rdsensor_fm1;
+	struct feature_fme_temp_rdsensor_fmt2 rdsensor_fm2;
+};
+
+/* Power Status register */
+struct feature_fme_pm_status {
+	union {
+		u64 csr;
+		struct {
+			/* FPGA Power consumed, The format is to be defined */
+			u32 pwr_consumed:18;
+			/* FPGA Latency Tolerance Reporting */
+			u8  fpga_latency_report:1;
+			u64 rsvd:45;			/* Reserved */
+		};
+	};
+};
+
+/* AP Thresholds */
+struct feature_fme_pm_ap_threshold {
+	union {
+		u64 csr;
+		struct {
+			/*
+			 * Number of clocks (5ns period) for assertion
+			 * of FME_data
+			 */
+			u8  threshold1:7;
+			u8  rsvd1:1;
+			u8  threshold2:7;
+			u8  rsvd2:1;
+			u8  threshold1_status:1;
+			u8  threshold2_status:1;
+			u64 rsvd3:46;		/* Reserved */
+		};
+	};
+};
+
+/* FME POWER FEATURE */
+struct feature_fme_power {
+	struct feature_header header;
+	struct feature_fme_pm_status status;
+	struct feature_fme_pm_ap_threshold threshold;
+};
+
+#define CACHE_CHANNEL_RD	0
+#define CACHE_CHANNEL_WR	1
+
+enum gperf_cache_events {
+	CACHE_RD_HIT,
+	CACHE_WR_HIT,
+	CACHE_RD_MISS,
+	CACHE_WR_MISS,
+	CACHE_RSVD, /* reserved */
+	CACHE_HOLD_REQ,
+	CACHE_DATA_WR_PORT_CONTEN,
+	CACHE_TAG_WR_PORT_CONTEN,
+	CACHE_TX_REQ_STALL,
+	CACHE_RX_REQ_STALL,
+	CACHE_EVICTIONS,
+};
+
+/* FPMON Cache Control */
+struct feature_fme_fpmon_ch_ctl {
+	union {
+		u64 csr;
+		struct {
+			u8  reset_counters:1;	/* Reset Counters */
+			u8  rsvd1:7;		/* Reserved */
+			u8  freeze:1;		/* Freeze if set to 1 */
+			u8  rsvd2:7;		/* Reserved */
+			u8  cache_event:4;	/* Select the cache event */
+			u8  cci_chsel:1;	/* Select the channel */
+			u64 rsvd3:43;		/* Reserved */
+		};
+	};
+};
+
+/* FPMON Cache Counter */
+struct feature_fme_fpmon_ch_ctr {
+	union {
+		u64 csr;
+		struct {
+			/* Cache Counter for even addresse */
+			u64 cache_counter:48;
+			u16 rsvd:12;		/* Reserved */
+			/* Cache Event being reported */
+			u8  event_code:4;
+		};
+	};
+};
+
+enum gperf_fab_events {
+	FAB_PCIE0_RD,
+	FAB_PCIE0_WR,
+	FAB_PCIE1_RD,
+	FAB_PCIE1_WR,
+	FAB_UPI_RD,
+	FAB_UPI_WR,
+	FAB_MMIO_RD,
+	FAB_MMIO_WR,
+};
+
+#define FAB_DISABLE_FILTER     0
+#define FAB_ENABLE_FILTER      1
+
+/* FPMON FAB Control */
+struct feature_fme_fpmon_fab_ctl {
+	union {
+		u64 csr;
+		struct {
+			u8  reset_counters:1;	/* Reset Counters */
+			u8  rsvd:7;		/* Reserved */
+			u8  freeze:1;		/* Set to 1 frozen counter */
+			u8  rsvd1:7;		/* Reserved */
+			u8  fab_evtcode:4;	/* Fabric Event Code */
+			u8  port_id:2;		/* Port ID */
+			u8  rsvd2:1;		/* Reserved */
+			u8  port_filter:1;	/* Port Filter */
+			u64 rsvd3:40;		/* Reserved */
+		};
+	};
+};
+
+/* FPMON Event Counter */
+struct feature_fme_fpmon_fab_ctr {
+	union {
+		u64 csr;
+		struct {
+			u64 fab_cnt:60;	/* Fabric event counter */
+			/* Fabric event code being reported */
+			u8  event_code:4;
+		};
+	};
+};
+
+/* FPMON Clock Counter */
+struct feature_fme_fpmon_clk_ctr {
+	u64 afu_interf_clock;		/* Clk_16UI (AFU clock) counter. */
+};
+
+enum gperf_vtd_events {
+	VTD_AFU0_MEM_RD_TRANS,
+	VTD_AFU1_MEM_RD_TRANS,
+	VTD_AFU0_MEM_WR_TRANS,
+	VTD_AFU1_MEM_WR_TRANS,
+	VTD_AFU0_TLB_RD_HIT,
+	VTD_AFU1_TLB_RD_HIT,
+	VTD_AFU0_TLB_WR_HIT,
+	VTD_AFU1_TLB_WR_HIT,
+};
+
+/* VT-d control register */
+struct feature_fme_fpmon_vtd_ctl {
+	union {
+		u64 csr;
+		struct {
+			u8  reset_counters:1;	/* Reset Counters */
+			u8  rsvd:7;		/* Reserved */
+			u8  freeze:1;		/* Set to 1 frozen counter */
+			u8  rsvd1:7;		/* Reserved */
+			u8  vtd_evtcode:4;	/* VTd and TLB event code */
+			u64 rsvd2:44;		/* Reserved */
+		};
+	};
+};
+
+/* VT-d event counter */
+struct feature_fme_fpmon_vtd_ctr {
+	union {
+		u64 csr;
+		struct {
+			u64 vtd_counter:48;	/* VTd event counter */
+			u16 rsvd:12;		/* Reserved */
+			u8 event_code:4; /* VTd event code */
+		};
+	};
+};
+
+/* FME GPERF FEATURE */
+struct feature_fme_gperf {
+	struct feature_header header;
+	struct feature_fme_fpmon_ch_ctl ch_ctl;
+	struct feature_fme_fpmon_ch_ctr ch_ctr0;
+	struct feature_fme_fpmon_ch_ctr ch_ctr1;
+	struct feature_fme_fpmon_fab_ctl fab_ctl;
+	struct feature_fme_fpmon_fab_ctr fab_ctr;
+	struct feature_fme_fpmon_clk_ctr clk;
+	struct feature_fme_fpmon_vtd_ctl vtd_ctl;
+	struct feature_fme_fpmon_vtd_ctr vtd_ctr;
+};
+
+struct feature_fme_error0 {
+#define FME_ERROR0_MASK        0xFFUL
+#define FME_ERROR0_MASK_DEFAULT 0x40UL  /* pcode workaround */
+	union {
+		u64 csr;
+		struct {
+			u8  fabric_err:1;	/* Fabric error */
+			u8  fabfifo_overflow:1;	/* Fabric fifo overflow */
+			u8  pcie0_poison:1;	/* PCIE0 Poison Detected */
+			u8  pcie1_poison:1;	/* PCIE1 Poison Detected */
+			u8  iommu_parity_err:1;	/* IOMMU Parity error */
+			/* AFU PF/VF access mismatch detected */
+			u8  afu_acc_mode_err:1;
+			u8  mbp_err:1;		/* Indicates an MBP event */
+			u64 rsvd:57;		/* Reserved */
+		};
+	};
+};
+
+/* PCIe0 Error Status register */
+struct feature_fme_pcie0_error {
+#define FME_PCIE0_ERROR_MASK   0xFFUL
+	union {
+		u64 csr;
+		struct {
+			u8  formattype_err:1;	/* TLP format/type error */
+			u8  MWAddr_err:1;	/* TLP MW address error */
+			u8  MWAddrLength_err:1;	/* TLP MW length error */
+			u8  MRAddr_err:1;	/* TLP MR address error */
+			u8  MRAddrLength_err:1;	/* TLP MR length error */
+			u8  cpl_tag_err:1;	/* TLP CPL tag error */
+			u8  cpl_status_err:1;	/* TLP CPL status error */
+			u8  cpl_timeout_err:1;	/* TLP CPL timeout */
+			u64 rsvd:54;		/* Reserved */
+			u8  vfnumb_err:1;	/* Number of error VF */
+			u8  funct_type_err:1;	/* Virtual (1) or Physical */
+		};
+	};
+};
+
+/* PCIe1 Error Status register */
+struct feature_fme_pcie1_error {
+#define FME_PCIE1_ERROR_MASK   0xFFUL
+	union {
+		u64 csr;
+		struct {
+			u8  formattype_err:1;	/* TLP format/type error */
+			u8  MWAddr_err:1;	/* TLP MW address error */
+			u8  MWAddrLength_err:1;	/* TLP MW length error */
+			u8  MRAddr_err:1;	/* TLP MR address error */
+			u8  MRAddrLength_err:1;	/* TLP MR length error */
+			u8  cpl_tag_err:1;	/* TLP CPL tag error */
+			u8  cpl_status_err:1;	/* TLP CPL status error */
+			u8  cpl_timeout_err:1;	/* TLP CPL timeout */
+			u64 rsvd:56;		/* Reserved */
+		};
+	};
+};
+
+
+/* FME First Error register */
+struct feature_fme_first_error {
+#define FME_FIRST_ERROR_MASK   ((1UL << 60) - 1)
+	union {
+		u64 csr;
+		struct {
+			/*
+			 * Indicates the Error Register that was
+			 * triggered first
+			 */
+			u64 err_reg_status:60;
+			/*
+			 * Holds 60 LSBs from the Error register that was
+			 * triggered first
+			 */
+			u8 errReg_id:4;
+		};
+	};
+};
+
+/* FME Next Error register */
+struct feature_fme_next_error {
+#define FME_NEXT_ERROR_MASK    ((1UL << 60) - 1)
+	union {
+		u64 csr;
+		struct {
+			/*
+			 * Indicates the Error Register that was
+			 * triggered second
+			 */
+			u64 err_reg_status:60;
+			/*
+			 * Holds 60 LSBs from the Error register that was
+			 * triggered second
+			 */
+			u8  errReg_id:4;
+		};
+	};
+};
+
+/* RAS GreenBS Error Status register */
+struct feature_fme_ras_gerror {
+#define FME_RAS_GERROR_MASK    0xFFFFUL
+	union {
+		u64 csr;
+		struct {
+			/* thremal threshold AP1 */
+			u8  temp_trash_ap1:1;
+			/* thremal threshold AP2 */
+			u8  temp_trash_ap2:1;
+			u8  pcie_error:1;	/* pcie Error */
+			u8  afufatal_error:1;	/* afu fatal error */
+			u8  proc_hot:1;		/* Indicates a ProcHot event */
+			/* Indicates an AFU PF/VF access mismatch */
+			u8  afu_acc_mode_err:1;
+			/* Injected Warning Error */
+			u8  injected_warning_err:1;
+			/* Indicates a Poison error from any of PCIe ports */
+			u8  pcie_poison_Err:1;
+			/* Green bitstream CRC Error */
+			u8  gb_crc_err:1;
+			/* Temperature threshold triggered AP6*/
+			u8  temp_thresh_AP6:1;
+			/* Power threshold triggered AP1 */
+			u8  power_thresh_AP1:1;
+			/* Power threshold triggered AP2 */
+			u8  power_thresh_AP2:1;
+			/* Indicates a MBP event */
+			u8  mbp_err:1;
+			u64 rsvd2:51;		/* Reserved */
+		};
+	};
+};
+
+/* RAS BlueBS Error Status register */
+struct feature_fme_ras_berror {
+#define FME_RAS_BERROR_MASK    0xFFFFUL
+	union {
+		u64 csr;
+		struct {
+			/* KTI Link layer error detected */
+			u8  ktilink_fatal_err:1;
+			/* tag-n-cache error detected */
+			u8  tagcch_fatal_err:1;
+			/* CCI error detected */
+			u8  cci_fatal_err:1;
+			/* KTI Protocol error detected */
+			u8  ktiprpto_fatal_err:1;
+			/* Fatal DRAM error detected */
+			u8  dram_fatal_err:1;
+			/* IOMMU detected */
+			u8  iommu_fatal_err:1;
+			/* Injected Fatal Error */
+			u8  injected_fatal_err:1;
+			u8  rsvd:1;
+			/* Catastrophic IOMMU Error */
+			u8  iommu_catast_err:1;
+			/* Catastrophic CRC Error */
+			u8  crc_catast_err:1;
+			/* Catastrophic Thermal Error */
+			u8  therm_catast_err:1;
+			/* Injected Catastrophic Error */
+			u8  injected_catast_err:1;
+			u64 rsvd1:52;
+		};
+	};
+};
+
+/* RAS Warning Error Status register */
+struct feature_fme_ras_werror {
+#define FME_RAS_WERROR_MASK    0x1UL
+	union {
+		u64 csr;
+		struct {
+			/*
+			 * Warning bit indicates that a green bitstream
+			 * fatal event occurred
+			 */
+			u8  event_warn_err:1;
+			u64 rsvd:63;		/* Reserved */
+		};
+	};
+};
+
+/* RAS Error injection register */
+struct feature_fme_ras_error_inj {
+#define FME_RAS_ERROR_INJ_MASK      0x7UL
+	union {
+		u64 csr;
+		struct {
+			u8  catast_error:1;	/* Catastrophic error flag */
+			u8  fatal_error:1;	/* Fatal error flag */
+			u8  warning_error:1;	/* Warning error flag */
+			u64 rsvd:61;		/* Reserved */
+		};
+	};
+};
+
+/* FME ERR FEATURE */
+struct feature_fme_err {
+	struct feature_header header;
+	struct feature_fme_error0 fme_err_mask;
+	struct feature_fme_error0 fme_err;
+	struct feature_fme_pcie0_error pcie0_err_mask;
+	struct feature_fme_pcie0_error pcie0_err;
+	struct feature_fme_pcie1_error pcie1_err_mask;
+	struct feature_fme_pcie1_error pcie1_err;
+	struct feature_fme_first_error fme_first_err;
+	struct feature_fme_next_error fme_next_err;
+	struct feature_fme_ras_gerror ras_gerr_mask;
+	struct feature_fme_ras_gerror ras_gerr;
+	struct feature_fme_ras_berror ras_berr_mask;
+	struct feature_fme_ras_berror ras_berr;
+	struct feature_fme_ras_werror ras_werr_mask;
+	struct feature_fme_ras_werror ras_werr;
+	struct feature_fme_ras_error_inj ras_error_inj;
+};
+
+/* FME Partial Reconfiguration Control */
+struct feature_fme_pr_ctl {
+	union {
+		u64 csr;
+		struct {
+			u8  pr_reset:1;		/* Reset PR Engine */
+			u8  rsvd3:3;		/* Reserved */
+			u8  pr_reset_ack:1;	/* Reset PR Engine Ack */
+			u8  rsvd4:3;		/* Reserved */
+			u8  pr_regionid:2;	/* PR Region ID */
+			u8  rsvd1:2;		/* Reserved */
+			u8  pr_start_req:1;	/* PR Start Request */
+			u8  pr_push_complete:1;	/* PR Data push complete */
+			u8  pr_kind:1;		/* PR Data push complete */
+			u32  rsvd:17;		/* Reserved */
+			u32 config_data;	/* Config data TBD */
+		};
+	};
+};
+
+/* FME Partial Reconfiguration Status */
+struct feature_fme_pr_status {
+	union {
+		u64 csr;
+		struct {
+			u16 pr_credit:9;	/* PR Credits */
+			u8  rsvd2:7;		/* Reserved */
+			u8  pr_status:1;	/* PR status */
+			u8  rsvd:3;		/* Reserved */
+			/* Altra PR Controller Block status */
+			u8  pr_contoller_status:3;
+			u8  rsvd1:1;            /* Reserved */
+			u8  pr_host_status:4;   /* PR Host status */
+			u8  rsvd3:4;		/* Reserved */
+			/* Security Block Status fields (TBD) */
+			u32 security_bstatus;
+		};
+	};
+};
+
+/* FME Partial Reconfiguration Data */
+struct feature_fme_pr_data {
+	union {
+		u64 csr;	/* PR data from the raw-binary file */
+		struct {
+			/* PR data from the raw-binary file */
+			u32 pr_data_raw;
+			u32 rsvd;
+		};
+	};
+};
+
+/* FME PR Public Key */
+struct feature_fme_pr_key {
+	u64 key;		/* FME PR Public Hash */
+};
+
+/* FME PR FEATURE */
+struct feature_fme_pr {
+	struct feature_header header;
+	/*Partial Reconfiguration control */
+	struct feature_fme_pr_ctl	ccip_fme_pr_control;
+
+	/* Partial Reconfiguration Status */
+	struct feature_fme_pr_status	ccip_fme_pr_status;
+
+	/* Partial Reconfiguration data */
+	struct feature_fme_pr_data	ccip_fme_pr_data;
+
+	/* Partial Reconfiguration data */
+	u64				ccip_fme_pr_err;
+
+	/* FME PR Publish HASH */
+	struct feature_fme_pr_key fme_pr_pub_harsh0;
+	struct feature_fme_pr_key fme_pr_pub_harsh1;
+	struct feature_fme_pr_key fme_pr_pub_harsh2;
+	struct feature_fme_pr_key fme_pr_pub_harsh3;
+
+	/* FME PR Private HASH */
+	struct feature_fme_pr_key fme_pr_priv_harsh0;
+	struct feature_fme_pr_key fme_pr_priv_harsh1;
+	struct feature_fme_pr_key fme_pr_priv_harsh2;
+	struct feature_fme_pr_key fme_pr_priv_harsh3;
+
+	/* FME PR License */
+	struct feature_fme_pr_key fme_pr_license0;
+	struct feature_fme_pr_key fme_pr_license1;
+	struct feature_fme_pr_key fme_pr_license2;
+	struct feature_fme_pr_key fme_pr_license3;
+
+	/* FME PR Session Key */
+	struct feature_fme_pr_key fme_pr_seskey0;
+	struct feature_fme_pr_key fme_pr_seskey1;
+	struct feature_fme_pr_key fme_pr_seskey2;
+	struct feature_fme_pr_key fme_pr_seskey3;
+
+	/* PR Interface ID */
+	struct feature_fme_pr_key fme_pr_intfc_id0_l;
+	struct feature_fme_pr_key fme_pr_intfc_id0_h;
+
+	struct feature_fme_pr_key fme_pr_intfc_id1_l;
+	struct feature_fme_pr_key fme_pr_intfc_id1_h;
+
+	struct feature_fme_pr_key fme_pr_intfc_id2_l;
+	struct feature_fme_pr_key fme_pr_intfc_id2_h;
+
+	struct feature_fme_pr_key fme_pr_intfc_id3_l;
+	struct feature_fme_pr_key fme_pr_intfc_id3_h;
+
+	/* MSIX filed to be Added */
+};
+
+#define PORT_ERR_MASK		0xfff0703ff001f
+struct feature_port_err_key {
+	union {
+		u64 csr;
+		struct {
+			/* Tx Channel0: Overflow */
+			u8 tx_ch0_overflow:1;
+			/* Tx Channel0: Invalid request encoding */
+			u8 tx_ch0_invaldreq :1;
+			/* Tx Channel0: Request with cl_len=3 not supported */
+			u8 tx_ch0_cl_len3:1;
+			/* Tx Channel0: Request with cl_len=2 not aligned 2CL */
+			u8 tx_ch0_cl_len2:1;
+			/* Tx Channel0: Request with cl_len=4 not aligned 4CL */
+			u8 tx_ch0_cl_len4:1;
+
+			u16 rsvd1:11;			/* Reserved */
+
+			/* Tx Channel1: Overflow */
+			u8 tx_ch1_overflow:1;
+			/* Tx Channel1: Invalid request encoding */
+			u8 tx_ch1_invaldreq:1;
+			/* Tx Channel1: Request with cl_len=3 not supported */
+			u8 tx_ch1_cl_len3:1;
+			/* Tx Channel1: Request with cl_len=2 not aligned 2CL */
+			u8 tx_ch1_cl_len2:1;
+			/* Tx Channel1: Request with cl_len=4 not aligned 4CL */
+			u8 tx_ch1_cl_len4:1;
+
+			/* Tx Channel1: Insufficient data payload */
+			u8 tx_ch1_insuff_data:1;
+			/* Tx Channel1: Data payload overrun */
+			u8 tx_ch1_data_overrun:1;
+			/* Tx Channel1 : Incorrect address */
+			u8 tx_ch1_incorr_addr:1;
+			/* Tx Channel1 : NON-Zero SOP Detected */
+			u8 tx_ch1_nzsop:1;
+			/* Tx Channel1 : Illegal VC_SEL, atomic request VLO */
+			u8 tx_ch1_illegal_vcsel:1;
+
+			u8 rsvd2:6;			/* Reserved */
+
+			/* MMIO Read Timeout in AFU */
+			u8 mmioread_timeout:1;
+
+			/* Tx Channel2: FIFO Overflow */
+			u8 tx_ch2_fifo_overflow:1;
+
+			/* MMIO read is not matching pending request */
+			u8 unexp_mmio_resp:1;
+
+			u8 rsvd3:5;			/* Reserved */
+
+			/* Number of pending Requests: counter overflow */
+			u8 tx_req_counter_overflow:1;
+			/* Req with Address violating SMM Range */
+			u8 llpr_smrr_err:1;
+			/* Req with Address violating second SMM Range */
+			u8 llpr_smrr2_err:1;
+			/* Req with Address violating ME Stolen message */
+			u8 llpr_mesg_err:1;
+			/* Req with Address violating Generic Protected Range */
+			u8 genprot_range_err:1;
+			/* Req with Address violating Legacy Range low */
+			u8 legrange_low_err:1;
+			/* Req with Address violating Legacy Range High */
+			u8 legrange_high_err:1;
+			/* Req with Address violating VGA memory range */
+			u8 vgmem_range_err:1;
+			u8 page_fault_err:1;		/* Page fault */
+			u8 pmr_err:1;			/* PMR Error */
+			u8 ap6_event:1;		/* AP6 event */
+			/* VF FLR detected on Port with PF access control */
+			u8 vfflr_access_err:1;
+			u16 rsvd4:12;			/* Reserved */
+		};
+	};
+};
+
+/* Port first error register, not contain all error bits in error register. */
+struct feature_port_first_err_key {
+	union {
+		u64 csr;
+		struct {
+			u8 tx_ch0_overflow:1;
+			u8 tx_ch0_invaldreq :1;
+			u8 tx_ch0_cl_len3:1;
+			u8 tx_ch0_cl_len2:1;
+			u8 tx_ch0_cl_len4:1;
+			u16 rsvd1:11;			/* Reserved */
+			u8 tx_ch1_overflow:1;
+			u8 tx_ch1_invaldreq:1;
+			u8 tx_ch1_cl_len3:1;
+			u8 tx_ch1_cl_len2:1;
+			u8 tx_ch1_cl_len4:1;
+			u8 tx_ch1_insuff_data:1;
+			u8 tx_ch1_data_overrun:1;
+			u8 tx_ch1_incorr_addr:1;
+			u8 tx_ch1_nzsop:1;
+			u8 tx_ch1_illegal_vcsel:1;
+			u8 rsvd2:6;			/* Reserved */
+			u8 mmioread_timeout:1;
+			u8 tx_ch2_fifo_overflow:1;
+			u8 rsvd3:6;			/* Reserved */
+			u8 tx_req_counter_overflow:1;
+			u32 rsvd4:23;			/* Reserved */
+		};
+	};
+};
+
+/* Port malformed Req0 */
+struct feature_port_malformed_req0 {
+	u64 header_lsb;
+};
+
+/* Port malformed Req1 */
+struct feature_port_malformed_req1 {
+	u64 header_msb;
+};
+
+/* Port debug register */
+struct feature_port_debug {
+	u64 port_debug;
+};
+
+/* PORT FEATURE ERROR */
+struct feature_port_error {
+	struct feature_header header;
+	struct feature_port_err_key error_mask;
+	struct feature_port_err_key port_error;
+	struct feature_port_first_err_key port_first_error;
+	struct feature_port_malformed_req0 malreq0;
+	struct feature_port_malformed_req1 malreq1;
+	struct feature_port_debug port_debug;
+};
+
+/* Port UMSG Capability */
+struct feature_port_umsg_cap {
+	union {
+		u64 csr;
+		struct {
+			/* Number of umsg allocated to this port */
+			u8 umsg_allocated;
+			/* Enable / Disable UMsg engine for this port */
+			u8 umsg_enable:1;
+			/* Usmg initialization status */
+			u8 umsg_init_complete:1;
+			/* IOMMU can not translate the umsg base address */
+			u8 umsg_trans_error:1;
+			u64 rsvd:53;		/* Reserved */
+		};
+	};
+};
+
+/* Port UMSG base address */
+struct feature_port_umsg_baseaddr {
+	union {
+		u64 csr;
+		struct {
+			u64 base_addr:48;	/* 48 bit physical address */
+			u16 rsvd;		/* Reserved */
+		};
+	};
+};
+
+struct feature_port_umsg_mode {
+	union {
+		u64 csr;
+		struct {
+			u32 umsg_hint_enable;	/* UMSG hint enable/disable */
+			u32 rsvd;		/* Reserved */
+		};
+	};
+};
+
+/* PORT FEATURE UMSG */
+struct feature_port_umsg {
+	struct feature_header header;
+	struct feature_port_umsg_cap capability;
+	struct feature_port_umsg_baseaddr baseaddr;
+	struct feature_port_umsg_mode mode;
+};
+
+/* STP region supports mmap operation, so use page aligned size. */
+#define PORT_FEATURE_STP_REGION_SIZE	PAGE_ALIGN(sizeof(struct feature_port_stp))
+
+/* Port STP status register (for debug only)*/
+struct feature_port_stp_status {
+	union {
+		u64 csr;
+		struct {
+			/* SLD Hub end-point read/write timeout */
+			u8 sld_ep_timeout:1;
+			/* Remote STP in reset/disable */
+			u8 rstp_disabled:1;
+			u8 unsupported_read:1;
+			/* MMIO timeout detected and faked with a response */
+			u8 mmio_timeout:1;
+			u8 txfifo_count:4;
+			u8 rxfifo_count:4;
+			u8 txfifo_overflow:1;
+			u8 txfifo_underflow:1;
+			u8 rxfifo_overflow:1;
+			u8 rxfifo_underflow:1;
+			/* Number of MMIO write requests */
+			u16 write_requests;
+			/* Number of MMIO read requests */
+			u16 read_requests;
+			/* Number of MMIO read responses */
+			u16 read_responses;
+		};
+	};
+};
+
+/*
+ * PORT FEATURE STP
+ * Most registers in STP region are not touched by driver, but mmapped to user
+ * space. So they are not defined in below data structure, as its actual size
+ * is 0x18c per spec.
+ */
+struct feature_port_stp {
+	struct feature_header header;
+	struct feature_port_stp_status stp_status;
+};
+
+#pragma pack()
+
+struct feature_driver {
+	const char *name;
+	struct feature_ops *ops;
+};
+
+struct feature {
+	const char *name;
+	int resource_index;
+	void __iomem *ioaddr;
+	struct feature_ops *ops;
+};
+
+struct feature_platform_data {
+	/* list the feature dev to cci_drvdata->port_dev_list. */
+	struct list_head node;
+
+	struct mutex lock;
+	int excl_open;
+	int open_count;
+	struct cdev cdev;
+	struct platform_device *dev;
+	unsigned int disable_count;
+
+	void *private;
+
+	int num;
+	int (*config_port)(struct platform_device *, u32, bool);
+	struct platform_device *(*fpga_for_each_port)(struct platform_device *,
+			void *, int (*match)(struct platform_device *, void *));
+	struct feature features[0];
+};
+
+static inline int
+feature_dev_use_excl_begin(struct feature_platform_data *pdata)
+{
+	/*
+	 * If device file is opened with O_EXCL flag, check the open_count
+	 * and set excl_open and increate open_count to ensure exclusive use.
+	 */
+	mutex_lock(&pdata->lock);
+	if (pdata->open_count) {
+		mutex_unlock(&pdata->lock);
+		return -EBUSY;
+	}
+	pdata->excl_open = 1;
+	pdata->open_count++;
+	mutex_unlock(&pdata->lock);
+
+	return 0;
+}
+
+static inline int feature_dev_use_begin(struct feature_platform_data *pdata)
+{
+	mutex_lock(&pdata->lock);
+	if (pdata->excl_open) {
+		mutex_unlock(&pdata->lock);
+		return -EBUSY;
+	}
+	pdata->open_count++;
+	mutex_unlock(&pdata->lock);
+
+	return 0;
+}
+
+static inline void __feature_dev_use_end(struct feature_platform_data *pdata)
+{
+	pdata->excl_open = 0;
+	pdata->open_count--;
+}
+
+static inline void feature_dev_use_end(struct feature_platform_data *pdata)
+{
+	mutex_lock(&pdata->lock);
+	__feature_dev_use_end(pdata);
+	mutex_unlock(&pdata->lock);
+}
+
+static inline void
+fpga_pdata_set_private(struct feature_platform_data *pdata, void *private)
+{
+	pdata->private = private;
+}
+
+static inline void *fpga_pdata_get_private(struct feature_platform_data *pdata)
+{
+	return pdata->private;
+}
+
+struct feature_ops {
+	int (*init)(struct platform_device *pdev, struct feature *feature);
+	void (*uinit)(struct platform_device *pdev, struct feature *feature);
+	long (*ioctl)(struct platform_device *pdev, struct feature *feature,
+				unsigned int cmd, unsigned long arg);
+	int (*test)(struct platform_device *pdev, struct feature *feature);
+};
+
+enum fme_feature_id {
+	FME_FEATURE_ID_HEADER = 0x0,
+
+	FME_FEATURE_ID_THERMAL_MGMT	= 0x1,
+	FME_FEATURE_ID_POWER_MGMT = 0x2,
+	FME_FEATURE_ID_GLOBAL_PERF = 0x3,
+	FME_FEATURE_ID_GLOBAL_ERR = 0x4,
+	FME_FEATURE_ID_PR_MGMT = 0x5,
+
+	/* one for fme header. */
+	FME_FEATURE_ID_MAX = 0x6,
+};
+
+enum port_feature_id {
+	PORT_FEATURE_ID_HEADER = 0x0,
+	PORT_FEATURE_ID_ERROR = 0x1,
+	PORT_FEATURE_ID_UMSG = 0x2,
+	PORT_FEATURE_ID_PR = 0x3,
+	PORT_FEATURE_ID_STP = 0x4,
+	PORT_FEATURE_ID_UAFU = 0x5,
+	PORT_FEATURE_ID_MAX = 0x6,
+};
+
+
+int fme_feature_num(void);
+int port_feature_num(void);
+
+#define FPGA_FEATURE_DEV_FME		"intel-fpga-fme"
+#define FPGA_FEATURE_DEV_PORT		"intel-fpga-port"
+
+void feature_platform_data_add(struct feature_platform_data *pdata,
+			       int index, const char *name,
+			       int resource_index, void __iomem *ioaddr);
+int feature_platform_data_size(int num);
+struct feature_platform_data *
+feature_platform_data_alloc_and_init(struct platform_device *dev, int num);
+
+void fpga_dev_feature_uinit(struct platform_device *pdev);
+int fpga_dev_feature_init(struct platform_device *pdev,
+			  struct feature_driver *feature_drvs);
+
+enum fpga_devt_type {
+	FPGA_DEVT_FME,
+	FPGA_DEVT_PORT,
+	FPGA_DEVT_MAX,
+};
+
+void fpga_chardev_uinit(void);
+int fpga_chardev_init(void);
+dev_t fpga_get_devt(enum fpga_devt_type type, int id);
+int fpga_register_dev_ops(struct platform_device *pdev,
+			  const struct file_operations *fops,
+			  struct module *owner);
+void fpga_unregister_dev_ops(struct platform_device *pdev);
+
+int fpga_port_id(struct platform_device *pdev);
+
+static inline int fpga_port_check_id(struct platform_device *pdev,
+				     void *pport_id)
+{
+	return fpga_port_id(pdev) == *(int *)pport_id;
+}
+
+void __fpga_port_enable(struct platform_device *pdev);
+int __fpga_port_disable(struct platform_device *pdev);
+
+static inline void fpga_port_enable(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+
+	mutex_lock(&pdata->lock);
+	__fpga_port_enable(pdev);
+	mutex_unlock(&pdata->lock);
+}
+
+static inline int fpga_port_disable(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	int ret;
+
+	mutex_lock(&pdata->lock);
+	ret = __fpga_port_disable(pdev);
+	mutex_unlock(&pdata->lock);
+
+	return ret;
+}
+
+static inline int __fpga_port_reset(struct platform_device *pdev)
+{
+	int ret;
+
+	ret = __fpga_port_disable(pdev);
+	if (ret)
+		return ret;
+
+	__fpga_port_enable(pdev);
+	return 0;
+}
+
+static inline int fpga_port_reset(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	int ret;
+
+	mutex_lock(&pdata->lock);
+	ret = __fpga_port_reset(pdev);
+	mutex_unlock(&pdata->lock);
+	return ret;
+}
+
+static inline
+struct platform_device *fpga_inode_to_feature_dev(struct inode *inode)
+{
+	struct feature_platform_data *pdata;
+
+	pdata = container_of(inode->i_cdev, struct feature_platform_data, cdev);
+	return pdata->dev;
+}
+
+static inline void __iomem *
+get_feature_ioaddr_by_index(struct device *dev, int index)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+
+	return pdata->features[index].ioaddr;
+}
+
+static inline bool is_feature_present(struct device *dev, int index)
+{
+	return !!get_feature_ioaddr_by_index(dev, index);
+}
+
+static inline struct device *
+fpga_feature_dev_to_pcidev(struct platform_device *dev)
+{
+	return dev->dev.parent->parent;
+}
+
+static inline struct device *
+fpga_pdata_to_pcidev(struct feature_platform_data *pdata)
+{
+	return fpga_feature_dev_to_pcidev(pdata->dev);
+}
+
+#define fpga_dev_for_each_feature(pdata, feature)			    \
+	for ((feature) = (pdata)->features;				    \
+	   (feature) < (pdata)->features + (pdata)->num; (feature)++)
+
+void check_features_header(struct pci_dev *pdev, struct feature_header *hdr,
+			   enum fpga_devt_type type, int id);
+
+/*
+ * Wait register's _field to be changed to the given value (_expect's _field)
+ * by polling with given interval and timeout.
+ */
+#define fpga_wait_register_field(_field, _expect, _reg_addr, _timeout, _invl)\
+({									     \
+	int wait = 0;							     \
+	int ret = -ETIMEDOUT;						     \
+	typeof(_expect) value;						     \
+	for (; wait <= _timeout; wait += _invl) {			     \
+		value.csr = readq(_reg_addr);				     \
+		if (_expect._field == value._field) {			     \
+			ret = 0;					     \
+			break;						     \
+		}							     \
+		udelay(_invl);						     \
+	}								     \
+	ret;								     \
+})
+
+#endif
diff --git a/drivers/fpga/intel/fme-error.c b/drivers/fpga/intel/fme-error.c
new file mode 100644
index 000000000000..212853d6de83
--- /dev/null
+++ b/drivers/fpga/intel/fme-error.c
@@ -0,0 +1,448 @@
+/*
+ * Driver for FPGA Management Engine Error Management
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/stddef.h>
+#include <linux/errno.h>
+#include <linux/uaccess.h>
+#include <linux/fpga/fpga-mgr.h>
+
+#include "feature-dev.h"
+#include "fme.h"
+
+static ssize_t errors_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_error0 fme_error0;
+
+	fme_error0.csr = readq(&fme_err->fme_err);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", fme_error0.csr);
+}
+
+static DEVICE_ATTR_RO(errors);
+
+static ssize_t first_error_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_first_error fme_first_err;
+
+	fme_first_err.csr = readq(&fme_err->fme_first_err);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n",
+			(unsigned long long)fme_first_err.err_reg_status);
+}
+
+static DEVICE_ATTR_RO(first_error);
+
+static ssize_t next_error_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_next_error fme_next_err;
+
+	fme_next_err.csr = readq(&fme_err->fme_next_err);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n",
+			(unsigned long long)fme_next_err.err_reg_status);
+}
+
+static DEVICE_ATTR_RO(next_error);
+
+static ssize_t clear_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev->parent);
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_error0 fme_error0;
+	struct feature_fme_first_error fme_first_err;
+	struct feature_fme_next_error fme_next_err;
+	u64 val;
+
+	if (kstrtou64(buf, 0, &val))
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	writeq(FME_ERROR0_MASK, &fme_err->fme_err_mask);
+
+	fme_error0.csr = readq(&fme_err->fme_err);
+	if (val != fme_error0.csr) {
+		count = -EBUSY;
+		goto exit;
+	}
+
+	fme_first_err.csr = readq(&fme_err->fme_first_err);
+	fme_next_err.csr = readq(&fme_err->fme_next_err);
+
+	writeq(fme_error0.csr & FME_ERROR0_MASK, &fme_err->fme_err);
+	writeq(fme_first_err.csr & FME_FIRST_ERROR_MASK,
+		&fme_err->fme_first_err);
+	writeq(fme_next_err.csr & FME_NEXT_ERROR_MASK,
+		&fme_err->fme_next_err);
+
+exit:
+	writeq(FME_ERROR0_MASK_DEFAULT, &fme_err->fme_err_mask);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static DEVICE_ATTR_WO(clear);
+
+static ssize_t revision_show(struct device *dev, struct device_attribute *attr,
+			     char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_header header;
+
+	header.csr = readq(&fme_err->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+
+static DEVICE_ATTR_RO(revision);
+
+static ssize_t pcie0_errors_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_pcie0_error pcie0_err;
+
+	pcie0_err.csr = readq(&fme_err->pcie0_err);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", pcie0_err.csr);
+}
+
+static ssize_t pcie0_errors_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev->parent);
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_pcie0_error pcie0_err;
+	u64 val;
+
+	if (kstrtou64(buf, 0, &val))
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	writeq(FME_PCIE0_ERROR_MASK, &fme_err->pcie0_err_mask);
+
+	pcie0_err.csr = readq(&fme_err->pcie0_err);
+	if (val != pcie0_err.csr)
+		count = -EBUSY;
+	else
+		writeq(pcie0_err.csr & FME_PCIE0_ERROR_MASK,
+				&fme_err->pcie0_err);
+
+	writeq(0UL, &fme_err->pcie0_err_mask);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(pcie0_errors);
+
+static ssize_t pcie1_errors_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_pcie1_error pcie1_err;
+
+	pcie1_err.csr = readq(&fme_err->pcie1_err);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", pcie1_err.csr);
+}
+
+static ssize_t pcie1_errors_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev->parent);
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_pcie1_error pcie1_err;
+	u64 val;
+
+	if (kstrtou64(buf, 0, &val))
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	writeq(FME_PCIE1_ERROR_MASK, &fme_err->pcie1_err_mask);
+
+	pcie1_err.csr = readq(&fme_err->pcie1_err);
+	if (val != pcie1_err.csr)
+		count = -EBUSY;
+	else
+		writeq(pcie1_err.csr & FME_PCIE1_ERROR_MASK,
+				&fme_err->pcie1_err);
+
+	writeq(0UL, &fme_err->pcie1_err_mask);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(pcie1_errors);
+
+static ssize_t gbs_errors_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_ras_gerror ras_gerr;
+
+	ras_gerr.csr = readq(&fme_err->ras_gerr);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", ras_gerr.csr);
+}
+
+static DEVICE_ATTR_RO(gbs_errors);
+
+static ssize_t bbs_errors_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_ras_berror ras_berr;
+
+	ras_berr.csr = readq(&fme_err->ras_berr);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", ras_berr.csr);
+}
+
+static DEVICE_ATTR_RO(bbs_errors);
+
+static ssize_t warning_errors_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_ras_werror ras_werr;
+
+	ras_werr.csr = readq(&fme_err->ras_werr);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%x\n", ras_werr.event_warn_err);
+}
+
+static ssize_t warning_errors_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev->parent);
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_ras_werror ras_werr;
+	u64 val;
+
+	if (kstrtou64(buf, 0, &val))
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	writeq(FME_RAS_WERROR_MASK, &fme_err->ras_werr_mask);
+
+	ras_werr.csr = readq(&fme_err->ras_werr);
+	if (val != ras_werr.csr)
+		count = -EBUSY;
+	else
+		writeq(ras_werr.csr & FME_RAS_WERROR_MASK,
+				&fme_err->ras_werr);
+
+	writeq(0UL, &fme_err->ras_werr_mask);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(warning_errors);
+
+static ssize_t inject_error_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_ras_error_inj ras_error_inj;
+
+	ras_error_inj.csr = readq(&fme_err->ras_error_inj);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n",
+			ras_error_inj.csr & FME_RAS_ERROR_INJ_MASK);
+}
+
+static ssize_t inject_error_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev->parent);
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(dev->parent,
+				FME_FEATURE_ID_GLOBAL_ERR);
+	struct feature_fme_ras_error_inj ras_error_inj;
+	int err;
+	u8 data;
+
+	mutex_lock(&pdata->lock);
+	ras_error_inj.csr = readq(&fme_err->ras_error_inj);
+
+	err = kstrtou8(buf, 0, &data);
+	if (err) {
+		mutex_unlock(&pdata->lock);
+		return err;
+	}
+
+	if (data <= FME_RAS_ERROR_INJ_MASK)
+		ras_error_inj.csr = data;
+	else {
+		mutex_unlock(&pdata->lock);
+		return -EINVAL;
+	}
+
+	writeq(ras_error_inj.csr, &fme_err->ras_error_inj);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(inject_error);
+
+static struct attribute *fme_errors_attrs[] = {
+	&dev_attr_errors.attr,
+	&dev_attr_first_error.attr,
+	&dev_attr_next_error.attr,
+	&dev_attr_clear.attr,
+	NULL,
+};
+
+struct attribute_group fme_errors_attr_group = {
+	.attrs	= fme_errors_attrs,
+	.name	= "fme-errors",
+};
+
+static struct attribute *errors_attrs[] = {
+	&dev_attr_revision.attr,
+	&dev_attr_pcie0_errors.attr,
+	&dev_attr_pcie1_errors.attr,
+	&dev_attr_gbs_errors.attr,
+	&dev_attr_bbs_errors.attr,
+	&dev_attr_warning_errors.attr,
+	&dev_attr_inject_error.attr,
+	NULL,
+};
+
+struct attribute_group errors_attr_group = {
+	.attrs	= errors_attrs,
+};
+
+static const struct attribute_group *error_groups[] = {
+	&fme_errors_attr_group,
+	&errors_attr_group,
+	NULL
+};
+
+static void fme_error_enable(struct platform_device *pdev)
+{
+	struct feature_fme_err *fme_err
+		= get_feature_ioaddr_by_index(&pdev->dev,
+			FME_FEATURE_ID_GLOBAL_ERR);
+
+	writeq(FME_ERROR0_MASK_DEFAULT, &fme_err->fme_err_mask);
+	writeq(0UL, &fme_err->pcie0_err_mask);
+	writeq(0UL, &fme_err->pcie1_err_mask);
+	writeq(0UL, &fme_err->ras_gerr_mask);
+	writeq(0UL, &fme_err->ras_berr_mask);
+	writeq(0UL, &fme_err->ras_werr_mask);
+}
+
+static int global_error_init(struct platform_device *pdev,
+		struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+	struct device *dev;
+	int ret = 0;
+
+	dev = kzalloc(sizeof(struct device), GFP_KERNEL);
+	if (!dev)
+		return -ENOMEM;
+
+	dev->parent = &pdev->dev;
+	dev->release = (void (*)(struct device *))kfree;
+	dev_set_name(dev, "errors");
+
+	fme_error_enable(pdev);
+
+	ret = device_register(dev);
+	if (ret) {
+		put_device(dev);
+		return ret;
+	}
+
+	ret = sysfs_create_groups(&dev->kobj, error_groups);
+	if (ret) {
+		device_unregister(dev);
+		return ret;
+	}
+
+	mutex_lock(&pdata->lock);
+	fme = fpga_pdata_get_private(pdata);
+	fme->dev_err = dev;
+	mutex_unlock(&pdata->lock);
+
+	return ret;
+}
+
+static void global_error_uinit(struct platform_device *pdev,
+		struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+
+	mutex_lock(&pdata->lock);
+	fme = fpga_pdata_get_private(pdata);
+	sysfs_remove_groups(&fme->dev_err->kobj, error_groups);
+	device_unregister(fme->dev_err);
+	fme->dev_err = NULL;
+	mutex_unlock(&pdata->lock);
+}
+
+struct feature_ops global_error_ops = {
+	.init = global_error_init,
+	.uinit = global_error_uinit,
+};
diff --git a/drivers/fpga/intel/fme-main.c b/drivers/fpga/intel/fme-main.c
new file mode 100644
index 000000000000..a6bb91179c68
--- /dev/null
+++ b/drivers/fpga/intel/fme-main.c
@@ -0,0 +1,911 @@
+/*
+ * Driver for FPGA Management Engine which implements all FPGA platform
+ * level management features.
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/stddef.h>
+#include <linux/errno.h>
+#include <linux/delay.h>
+#include <linux/uaccess.h>
+#include <linux/intel-fpga.h>
+
+#include <linux/fpga/fpga-mgr.h>
+
+#include "feature-dev.h"
+#include "fme.h"
+
+#define PWR_THRESHOLD_MAX       0x7F
+
+#define FME_DEV_ATTR(_name, _filename, _mode, _show, _store)	\
+struct device_attribute dev_attr_##_name =			\
+	__ATTR(_filename, _mode, _show, _store)
+
+static ssize_t revision_show(struct device *dev, struct device_attribute *attr,
+			     char *buf)
+{
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	struct feature_header header;
+
+	header.csr = readq(&fme_hdr->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+
+static DEVICE_ATTR_RO(revision);
+
+static ssize_t ports_num_show(struct device *dev, struct device_attribute *attr,
+			char *buf)
+{
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	struct feature_fme_capability fme_capability;
+
+	fme_capability.csr = readq(&fme_hdr->capability);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", fme_capability.num_ports);
+}
+
+static DEVICE_ATTR_RO(ports_num);
+
+static ssize_t cache_size_show(struct device *dev,
+			struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	struct feature_fme_capability fme_capability;
+
+	fme_capability.csr = readq(&fme_hdr->capability);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", fme_capability.cache_size);
+}
+
+static DEVICE_ATTR_RO(cache_size);
+
+static ssize_t version_show(struct device *dev, struct device_attribute *attr,
+			char *buf)
+{
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	struct feature_fme_capability fme_capability;
+
+	fme_capability.csr = readq(&fme_hdr->capability);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", fme_capability.fabric_verid);
+}
+
+static DEVICE_ATTR_RO(version);
+
+static ssize_t socket_id_show(struct device *dev,
+			struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	struct feature_fme_capability fme_capability;
+
+	fme_capability.csr = readq(&fme_hdr->capability);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", fme_capability.socket_id);
+}
+
+static DEVICE_ATTR_RO(socket_id);
+
+static ssize_t bitstream_id_show(struct device *dev,
+			struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	u64 bitstream_id = readq(&fme_hdr->bitstream_id);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", bitstream_id);
+}
+
+static DEVICE_ATTR_RO(bitstream_id);
+
+static ssize_t bitstream_metadata_show(struct device *dev,
+			struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	u64 bitstream_md = readq(&fme_hdr->bitstream_md);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", bitstream_md);
+}
+
+static DEVICE_ATTR_RO(bitstream_metadata);
+
+static const struct attribute *fme_hdr_attrs[] = {
+	&dev_attr_revision.attr,
+	&dev_attr_ports_num.attr,
+	&dev_attr_cache_size.attr,
+	&dev_attr_version.attr,
+	&dev_attr_socket_id.attr,
+	&dev_attr_bitstream_id.attr,
+	&dev_attr_bitstream_metadata.attr,
+	NULL,
+};
+
+static int fme_hdr_init(struct platform_device *pdev, struct feature *feature)
+{
+	int ret;
+	struct feature_fme_header *fme_hdr = feature->ioaddr;
+
+	dev_dbg(&pdev->dev, "FME HDR Init.\n");
+	dev_dbg(&pdev->dev, "FME cap %llx.\n", fme_hdr->capability.csr);
+
+	ret = sysfs_create_files(&pdev->dev.kobj, fme_hdr_attrs);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static void fme_hdr_uinit(struct platform_device *pdev, struct feature *feature)
+{
+	dev_dbg(&pdev->dev, "FME HDR UInit.\n");
+	sysfs_remove_files(&pdev->dev.kobj, fme_hdr_attrs);
+}
+
+struct feature_ops fme_hdr_ops = {
+	.init = fme_hdr_init,
+	.uinit = fme_hdr_uinit,
+};
+
+static ssize_t thermal_revision_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_header header;
+
+	header.csr = readq(&fme_thermal->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+
+static FME_DEV_ATTR(thermal_revision, revision, 0444,
+		    thermal_revision_show, NULL);
+
+static ssize_t thermal_threshold1_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_tmp_threshold tmp_threshold;
+
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", tmp_threshold.tmp_thshold1);
+}
+
+static ssize_t thermal_threshold1_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	struct feature_fme_tmp_threshold tmp_threshold;
+	struct feature_fme_capability fme_capability;
+	int err;
+	u8 tmp_threshold1;
+
+	mutex_lock(&pdata->lock);
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	err = kstrtou8(buf, 0, &tmp_threshold1);
+	if (err) {
+		mutex_unlock(&pdata->lock);
+		return err;
+	}
+
+	fme_capability.csr = readq(&fme_hdr->capability);
+
+	if (fme_capability.lock_bit == 1) {
+		mutex_unlock(&pdata->lock);
+		return -EBUSY;
+	} else if (tmp_threshold1 > 100) {
+		mutex_unlock(&pdata->lock);
+		return -EINVAL;
+	} else if (tmp_threshold1 == 0) {
+		tmp_threshold.tmp_thshold1_enable = 0;
+		tmp_threshold.tmp_thshold1 = tmp_threshold1;
+	} else {
+		tmp_threshold.tmp_thshold1_enable = 1;
+		tmp_threshold.tmp_thshold1 = tmp_threshold1;
+	}
+
+	writeq(tmp_threshold.csr, &fme_thermal->threshold);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static DEVICE_ATTR(threshold1, 0644,
+	thermal_threshold1_show, thermal_threshold1_store);
+
+static ssize_t thermal_threshold2_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_tmp_threshold tmp_threshold;
+
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", tmp_threshold.tmp_thshold2);
+}
+
+static ssize_t thermal_threshold2_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_header *fme_hdr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_HEADER);
+	struct feature_fme_tmp_threshold tmp_threshold;
+	struct feature_fme_capability fme_capability;
+	int err;
+	u8 tmp_threshold2;
+
+	mutex_lock(&pdata->lock);
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	err = kstrtou8(buf, 0, &tmp_threshold2);
+	if (err) {
+		mutex_unlock(&pdata->lock);
+		return err;
+	}
+
+	fme_capability.csr = readq(&fme_hdr->capability);
+
+	if (fme_capability.lock_bit == 1) {
+		mutex_unlock(&pdata->lock);
+		return -EBUSY;
+	} else if (tmp_threshold2 > 100) {
+		mutex_unlock(&pdata->lock);
+		return -EINVAL;
+	} else if (tmp_threshold2 == 0) {
+		tmp_threshold.tmp_thshold2_enable = 0;
+		tmp_threshold.tmp_thshold2 = tmp_threshold2;
+	} else {
+		tmp_threshold.tmp_thshold2_enable = 1;
+		tmp_threshold.tmp_thshold2 = tmp_threshold2;
+	}
+
+	writeq(tmp_threshold.csr, &fme_thermal->threshold);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static DEVICE_ATTR(threshold2, 0644,
+	thermal_threshold2_show, thermal_threshold2_store);
+
+static ssize_t thermal_threshold_trip_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_tmp_threshold tmp_threshold;
+
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+				tmp_threshold.therm_trip_thshold);
+}
+
+static DEVICE_ATTR(threshold_trip, 0444, thermal_threshold_trip_show, NULL);
+
+static ssize_t thermal_threshold1_reached_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_tmp_threshold tmp_threshold;
+
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+				tmp_threshold.thshold1_status);
+}
+
+static DEVICE_ATTR(threshold1_reached, 0444,
+	thermal_threshold1_reached_show, NULL);
+
+static ssize_t thermal_threshold2_reached_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_tmp_threshold tmp_threshold;
+
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+				tmp_threshold.thshold2_status);
+}
+
+static DEVICE_ATTR(threshold2_reached, 0444,
+	thermal_threshold2_reached_show, NULL);
+
+static ssize_t thermal_threshold1_policy_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_tmp_threshold tmp_threshold;
+
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+				tmp_threshold.thshold_policy);
+}
+
+static ssize_t thermal_threshold1_policy_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_tmp_threshold tmp_threshold;
+	int err;
+	u8 thshold_policy;
+
+	mutex_lock(&pdata->lock);
+	tmp_threshold.csr = readq(&fme_thermal->threshold);
+
+	err = kstrtou8(buf, 0, &thshold_policy);
+	if (err) {
+		mutex_unlock(&pdata->lock);
+		return err;
+	}
+
+	if (thshold_policy == 0)
+		tmp_threshold.thshold_policy = 0;
+	else if (thshold_policy == 1)
+		tmp_threshold.thshold_policy = 1;
+	else {
+		mutex_unlock(&pdata->lock);
+		return -EINVAL;
+	}
+
+	writeq(tmp_threshold.csr, &fme_thermal->threshold);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static DEVICE_ATTR(threshold1_policy, 0644,
+	thermal_threshold1_policy_show, thermal_threshold1_policy_store);
+
+static ssize_t thermal_temperature_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_thermal *fme_thermal
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_THERMAL_MGMT);
+	struct feature_fme_temp_rdsensor_fmt1 temp_rdsensor_fmt1;
+
+	temp_rdsensor_fmt1.csr = readq(&fme_thermal->rdsensor_fm1);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+				temp_rdsensor_fmt1.fpga_temp);
+}
+
+static DEVICE_ATTR(temperature, 0444, thermal_temperature_show, NULL);
+
+static struct attribute *thermal_mgmt_attrs[] = {
+	&dev_attr_thermal_revision.attr,
+	&dev_attr_threshold1.attr,
+	&dev_attr_threshold2.attr,
+	&dev_attr_threshold_trip.attr,
+	&dev_attr_threshold1_reached.attr,
+	&dev_attr_threshold2_reached.attr,
+	&dev_attr_threshold1_policy.attr,
+	&dev_attr_temperature.attr,
+	NULL,
+};
+
+static struct attribute_group thermal_mgmt_attr_group = {
+	.attrs	= thermal_mgmt_attrs,
+	.name	= "thermal_mgmt",
+};
+
+static int thermal_mgmt_init(struct platform_device *pdev,
+				struct feature *feature)
+{
+	int ret;
+
+	ret = sysfs_create_group(&pdev->dev.kobj, &thermal_mgmt_attr_group);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static void thermal_mgmt_uinit(struct platform_device *pdev,
+				struct feature *feature)
+{
+	sysfs_remove_group(&pdev->dev.kobj, &thermal_mgmt_attr_group);
+}
+
+struct feature_ops thermal_mgmt_ops = {
+	.init = thermal_mgmt_init,
+	.uinit = thermal_mgmt_uinit,
+};
+
+static ssize_t pwr_revision_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_header header;
+
+	header.csr = readq(&fme_power->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+
+static FME_DEV_ATTR(pwr_revision, revision, 0444, pwr_revision_show, NULL);
+
+static ssize_t consumed_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_status pm_status;
+
+	pm_status.csr = readq(&fme_power->status);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%x\n", pm_status.pwr_consumed);
+}
+
+static DEVICE_ATTR_RO(consumed);
+
+static ssize_t pwr_threshold1_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_ap_threshold pm_ap_threshold;
+
+	pm_ap_threshold.csr = readq(&fme_power->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%x\n", pm_ap_threshold.threshold1);
+}
+
+static ssize_t pwr_threshold1_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_ap_threshold pm_ap_threshold;
+	u8 threshold;
+	int err;
+
+	mutex_lock(&pdata->lock);
+	pm_ap_threshold.csr = readq(&fme_power->threshold);
+
+	err = kstrtou8(buf, 0, &threshold);
+	if (err) {
+		mutex_unlock(&pdata->lock);
+		return err;
+	}
+
+	if (threshold <= PWR_THRESHOLD_MAX)
+		pm_ap_threshold.threshold1 = threshold;
+	else {
+		mutex_unlock(&pdata->lock);
+		return -EINVAL;
+	}
+
+	writeq(pm_ap_threshold.csr, &fme_power->threshold);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static FME_DEV_ATTR(pwr_threshold1, threshold1, 0644, pwr_threshold1_show,
+		    pwr_threshold1_store);
+
+static ssize_t pwr_threshold2_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_ap_threshold pm_ap_threshold;
+
+	pm_ap_threshold.csr = readq(&fme_power->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%x\n",
+				pm_ap_threshold.threshold2);
+}
+
+static ssize_t pwr_threshold2_store(struct device *dev,
+	struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(dev);
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_ap_threshold pm_ap_threshold;
+	u8 threshold;
+	int err;
+
+	mutex_lock(&pdata->lock);
+	pm_ap_threshold.csr = readq(&fme_power->threshold);
+
+	err = kstrtou8(buf, 0, &threshold);
+	if (err) {
+		mutex_unlock(&pdata->lock);
+		return err;
+	}
+
+	if (threshold <= PWR_THRESHOLD_MAX)
+		pm_ap_threshold.threshold2 = threshold;
+	else {
+		mutex_unlock(&pdata->lock);
+		return -EINVAL;
+	}
+
+	writeq(pm_ap_threshold.csr, &fme_power->threshold);
+	mutex_unlock(&pdata->lock);
+
+	return count;
+}
+
+static FME_DEV_ATTR(pwr_threshold2, threshold2, 0644, pwr_threshold2_show,
+		    pwr_threshold2_store);
+
+static ssize_t threshold1_status_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_ap_threshold pm_ap_threshold;
+
+	pm_ap_threshold.csr = readq(&fme_power->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%u\n",
+				pm_ap_threshold.threshold1_status);
+}
+
+static DEVICE_ATTR_RO(threshold1_status);
+
+static ssize_t threshold2_status_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_ap_threshold pm_ap_threshold;
+
+	pm_ap_threshold.csr = readq(&fme_power->threshold);
+
+	return scnprintf(buf, PAGE_SIZE, "%u\n",
+				pm_ap_threshold.threshold2_status);
+}
+static DEVICE_ATTR_RO(threshold2_status);
+
+static ssize_t rtl_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_power *fme_power
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_POWER_MGMT);
+	struct feature_fme_pm_status pm_status;
+
+	pm_status.csr = readq(&fme_power->status);
+
+	return scnprintf(buf, PAGE_SIZE, "%u\n",
+				pm_status.fpga_latency_report);
+}
+
+static DEVICE_ATTR_RO(rtl);
+
+static struct attribute *power_mgmt_attrs[] = {
+	&dev_attr_pwr_revision.attr,
+	&dev_attr_consumed.attr,
+	&dev_attr_pwr_threshold1.attr,
+	&dev_attr_pwr_threshold2.attr,
+	&dev_attr_threshold1_status.attr,
+	&dev_attr_threshold2_status.attr,
+	&dev_attr_rtl.attr,
+	NULL,
+};
+
+static struct attribute_group power_mgmt_attr_group = {
+	.attrs	= power_mgmt_attrs,
+	.name	= "power_mgmt",
+};
+
+static int power_mgmt_init(struct platform_device *pdev,
+				struct feature *feature)
+{
+	int ret;
+
+	ret = sysfs_create_group(&pdev->dev.kobj, &power_mgmt_attr_group);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static void power_mgmt_uinit(struct platform_device *pdev,
+				struct feature *feature)
+{
+	sysfs_remove_group(&pdev->dev.kobj, &power_mgmt_attr_group);
+}
+
+struct feature_ops power_mgmt_ops = {
+	.init = power_mgmt_init,
+	.uinit = power_mgmt_uinit,
+};
+
+static struct feature_driver fme_feature_drvs[] = {
+	{
+		.name = FME_FEATURE_HEADER,
+		.ops = &fme_hdr_ops,
+	},
+	{
+		.name = FME_FEATURE_THERMAL_MGMT,
+		.ops = &thermal_mgmt_ops,
+	},
+	{
+		.name = FME_FEATURE_POWER_MGMT,
+		.ops = &power_mgmt_ops,
+	},
+	{
+		.name = FME_FEATURE_GLOBAL_ERR,
+		.ops = &global_error_ops,
+	},
+	{
+		.name = FME_FEATURE_PR_MGMT,
+		.ops = &pr_mgmt_ops,
+	},
+	{
+		.name = FME_FEATURE_GLOBAL_PERF,
+		.ops = &global_perf_ops,
+	},
+	{
+		.ops = NULL,
+	},
+};
+
+static long fme_ioctl_check_extension(struct feature_platform_data *pdata,
+				     unsigned long arg)
+{
+	/* No extension support for now */
+	return 0;
+}
+
+static int fme_ioctl_config_port(struct feature_platform_data *pdata,
+				 u32 port_id, u32 flags, bool is_release)
+{
+	struct platform_device *fme_pdev = pdata->dev;
+	struct feature_fme_header *fme_hdr;
+	struct feature_fme_capability capability;
+
+	if (flags)
+		return -EINVAL;
+
+	fme_hdr = get_feature_ioaddr_by_index(
+		&fme_pdev->dev, FME_FEATURE_ID_HEADER);
+	capability.csr = readq(&fme_hdr->capability);
+
+	if (port_id >= capability.num_ports)
+		return -EINVAL;
+
+	return pdata->config_port(fme_pdev, port_id, is_release);
+}
+
+static long fme_ioctl_release_port(struct feature_platform_data *pdata,
+				   void __user *arg)
+{
+	struct fpga_fme_port_release release;
+	unsigned long minsz;
+
+	minsz = offsetofend(struct fpga_fme_port_release, port_id);
+
+	if (copy_from_user(&release, arg, minsz))
+		return -EFAULT;
+
+	if (release.argsz < minsz)
+		return -EINVAL;
+
+	return fme_ioctl_config_port(pdata, release.port_id,
+				     release.flags, true);
+}
+
+static long fme_ioctl_assign_port(struct feature_platform_data *pdata,
+				  void __user *arg)
+{
+	struct fpga_fme_port_assign assign;
+	unsigned long minsz;
+
+	minsz = offsetofend(struct fpga_fme_port_assign, port_id);
+
+	if (copy_from_user(&assign, arg, minsz))
+		return -EFAULT;
+
+	if (assign.argsz < minsz)
+		return -EINVAL;
+
+	return fme_ioctl_config_port(pdata, assign.port_id,
+				     assign.flags, false);
+}
+
+static int fme_open(struct inode *inode, struct file *filp)
+{
+	struct platform_device *fdev = fpga_inode_to_feature_dev(inode);
+	struct feature_platform_data *pdata = dev_get_platdata(&fdev->dev);
+	int ret;
+
+	if (WARN_ON(!pdata))
+		return -ENODEV;
+
+	if (filp->f_flags & O_EXCL)
+		ret = feature_dev_use_excl_begin(pdata);
+	else
+		ret = feature_dev_use_begin(pdata);
+
+	if (ret)
+		return ret;
+
+	dev_dbg(&fdev->dev, "Device File Opened %d Times\n", pdata->open_count);
+	filp->private_data = pdata;
+	return 0;
+}
+
+static int fme_release(struct inode *inode, struct file *filp)
+{
+	struct feature_platform_data *pdata = filp->private_data;
+	struct platform_device *pdev = pdata->dev;
+
+	dev_dbg(&pdev->dev, "Device File Release\n");
+	feature_dev_use_end(pdata);
+	return 0;
+}
+
+static long fme_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	struct feature_platform_data *pdata = filp->private_data;
+	struct platform_device *pdev = pdata->dev;
+	struct feature *f;
+	long ret;
+
+	dev_dbg(&pdev->dev, "%s cmd 0x%x\n", __func__, cmd);
+
+	switch (cmd) {
+	case FPGA_GET_API_VERSION:
+		return FPGA_API_VERSION;
+	case FPGA_CHECK_EXTENSION:
+		return fme_ioctl_check_extension(pdata, arg);
+	case FPGA_FME_PORT_RELEASE:
+		return fme_ioctl_release_port(pdata, (void __user *)arg);
+	case FPGA_FME_PORT_ASSIGN:
+		return fme_ioctl_assign_port(pdata, (void __user *)arg);
+	default:
+		/*
+		 * Let sub-feature's ioctl function to handle the cmd
+		 * Sub-feature's ioctl returns -ENODEV when cmd is not
+		 * handled in this sub feature, and returns 0 and other
+		 * error code if cmd is handled.
+		 */
+		fpga_dev_for_each_feature(pdata, f) {
+			if (f->ops && f->ops->ioctl) {
+				ret = f->ops->ioctl(pdev, f, cmd, arg);
+				if (ret == -ENODEV)
+					continue;
+				else
+					return ret;
+			}
+		}
+	}
+
+	return -EINVAL;
+}
+
+static const struct file_operations fme_fops = {
+	.owner		= THIS_MODULE,
+	.open		= fme_open,
+	.release	= fme_release,
+	.unlocked_ioctl = fme_ioctl,
+};
+
+static int fme_dev_init(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+
+	fme = devm_kzalloc(&pdev->dev, sizeof(*fme), GFP_KERNEL);
+	if (!fme)
+		return -ENOMEM;
+
+	fme->pdata = pdata;
+
+	mutex_lock(&pdata->lock);
+	fpga_pdata_set_private(pdata, fme);
+	mutex_unlock(&pdata->lock);
+	return 0;
+}
+
+static void fme_dev_destroy(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+
+	mutex_lock(&pdata->lock);
+	fme = fpga_pdata_get_private(pdata);
+	fpga_pdata_set_private(pdata, NULL);
+	mutex_unlock(&pdata->lock);
+
+	devm_kfree(&pdev->dev, fme);
+}
+
+static int fme_probe(struct platform_device *pdev)
+{
+	int ret;
+
+	ret = fme_dev_init(pdev);
+	if (ret)
+		goto exit;
+
+	ret = fpga_dev_feature_init(pdev, fme_feature_drvs);
+	if (ret)
+		goto dev_destroy;
+
+	ret = fpga_register_dev_ops(pdev, &fme_fops, THIS_MODULE);
+	if (ret)
+		goto feature_uinit;
+
+	return 0;
+
+feature_uinit:
+	fpga_dev_feature_uinit(pdev);
+dev_destroy:
+	fme_dev_destroy(pdev);
+exit:
+	return ret;
+}
+
+static int fme_remove(struct platform_device *pdev)
+{
+	fpga_dev_feature_uinit(pdev);
+	fpga_unregister_dev_ops(pdev);
+	fme_dev_destroy(pdev);
+	return 0;
+}
+
+static struct platform_driver fme_driver = {
+	.driver	= {
+		.name    = FPGA_FEATURE_DEV_FME,
+	},
+	.probe   = fme_probe,
+	.remove  = fme_remove,
+};
+
+module_platform_driver(fme_driver);
+
+MODULE_DESCRIPTION("FPGA Management Engine driver");
+MODULE_AUTHOR("Intel Corporation");
+MODULE_LICENSE("GPL v2");
+MODULE_ALIAS("platform:intel-fpga-fme");
diff --git a/drivers/fpga/intel/fme-perf.c b/drivers/fpga/intel/fme-perf.c
new file mode 100644
index 000000000000..e139e3f8db97
--- /dev/null
+++ b/drivers/fpga/intel/fme-perf.c
@@ -0,0 +1,715 @@
+/*
+ * Driver for FPGA Global Performance
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include "feature-dev.h"
+#include "fme.h"
+
+struct perf_obj_attributte {
+	struct attribute attr;
+	ssize_t (*show)(struct perf_object *pobj, char *buf);
+	ssize_t (*store)(struct perf_object *pobj,
+			 const char *buf, size_t n);
+};
+
+#define to_perf_obj_attr(_attr)					\
+		container_of(_attr, struct perf_obj_attributte, attr)
+#define to_perf_obj(_kobj)					\
+		container_of(_kobj, struct perf_object, kobj)
+
+static ssize_t perf_obj_attr_show(struct kobject *kobj,
+				     struct attribute *__attr, char *buf)
+{
+	struct perf_obj_attributte *attr = to_perf_obj_attr(__attr);
+	struct perf_object *pobj = to_perf_obj(kobj);
+	ssize_t ret = -EIO;
+
+	if (attr->show)
+		ret = attr->show(pobj, buf);
+	return ret;
+}
+
+static ssize_t perf_obj_attr_store(struct kobject *kobj,
+				      struct attribute *__attr,
+				      const char *buf, size_t n)
+{
+	struct perf_obj_attributte *attr = to_perf_obj_attr(__attr);
+	struct perf_object *pobj = to_perf_obj(kobj);
+	ssize_t ret = -EIO;
+
+	if (attr->store)
+		ret = attr->store(pobj, buf, n);
+	return ret;
+}
+
+static const struct sysfs_ops perf_obj_sysfs_ops = {
+	.show = perf_obj_attr_show,
+	.store = perf_obj_attr_store,
+};
+
+static void perf_obj_release(struct kobject *kobj)
+{
+	kfree(to_perf_obj(kobj));
+}
+
+static struct kobj_type perf_obj_ktype = {
+	.sysfs_ops = &perf_obj_sysfs_ops,
+	.release = perf_obj_release,
+};
+
+#define PERF_OBJ_ATTR(_name, _filename, _mode, _show, _store)	\
+struct perf_obj_attributte perf_obj_attr_##_name =		\
+	__ATTR(_filename, _mode, _show, _store)
+#define PERF_OBJ_ATTR_RW(_name)					\
+	struct perf_obj_attributte perf_obj_attr_##_name = __ATTR_RW(_name)
+#define PERF_OBJ_ATTR_RO(_name)					\
+	struct perf_obj_attributte perf_obj_attr_##_name = __ATTR_RO(_name)
+#define PERF_OBJ_ATTR_WO(_name)					\
+	struct perf_obj_attributte perf_obj_attr_##_name = __ATTR_WO(_name)
+
+static ssize_t revision_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_gperf *gperf;
+	struct feature_header header;
+
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	header.csr = readq(&gperf->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+static PERF_OBJ_ATTR_RO(revision);
+
+static ssize_t clock_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_gperf *gperf;
+	struct feature_fme_fpmon_clk_ctr clk;
+
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	clk.afu_interf_clock = readq(&gperf->clk);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", clk.afu_interf_clock);
+}
+static PERF_OBJ_ATTR_RO(clock);
+
+static struct attribute *clock_attrs[] = {
+	&perf_obj_attr_revision.attr,
+	&perf_obj_attr_clock.attr,
+	NULL,
+};
+
+static struct attribute_group clock_attr_group = {
+	.attrs = clock_attrs,
+};
+
+static ssize_t freeze_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_gperf *gperf;
+	struct feature_fme_fpmon_ch_ctl ctl;
+
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	ctl.csr = readq(&gperf->ch_ctl);
+	return scnprintf(buf, PAGE_SIZE, "%d\n", ctl.freeze);
+}
+
+static ssize_t freeze_store(struct perf_object *pobj,
+		 const char *buf, size_t n)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(pobj->fme_dev);
+	struct feature_fme_gperf *gperf;
+	struct feature_fme_fpmon_ch_ctl ctl;
+	bool state;
+
+	if (strtobool(buf, &state))
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	ctl.csr = readq(&gperf->ch_ctl);
+	ctl.freeze = state;
+	writeq(ctl.csr, &gperf->ch_ctl);
+	mutex_unlock(&pdata->lock);
+
+	return n;
+}
+static PERF_OBJ_ATTR_RW(freeze);
+
+#define GPERF_TIMEOUT	30
+
+static ssize_t read_cache_counter(struct perf_object *pobj, char *buf,
+				  u8 channel, enum gperf_cache_events event)
+{
+	struct device *fme_dev = pobj->fme_dev;
+	struct feature_platform_data *pdata = dev_get_platdata(fme_dev);
+	struct feature_fme_gperf *gperf;
+	struct feature_fme_fpmon_ch_ctl ctl;
+	struct feature_fme_fpmon_ch_ctr ctr0, ctr1;
+	u64 counter;
+
+	mutex_lock(&pdata->lock);
+	gperf = get_feature_ioaddr_by_index(fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+
+	/* set channel access type and cache event code. */
+	ctl.csr = readq(&gperf->ch_ctl);
+	ctl.cci_chsel = channel;
+	ctl.cache_event = event;
+	writeq(ctl.csr, &gperf->ch_ctl);
+
+	/* check the event type in the counter registers */
+	ctr0.event_code = event;
+
+	if (fpga_wait_register_field(event_code, ctr0,
+				     &gperf->ch_ctr0, GPERF_TIMEOUT, 1)) {
+		dev_err(fme_dev,
+		"timeout, unmatched cache event type in counter registers.\n");
+		mutex_unlock(&pdata->lock);
+		return -ETIMEDOUT;
+	}
+
+	ctr0.csr = readq(&gperf->ch_ctr0);
+	ctr1.csr = readq(&gperf->ch_ctr1);
+	counter = ctr0.cache_counter + ctr1.cache_counter;
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", counter);
+}
+
+#define CACHE_SHOW(name, type, event)					\
+static ssize_t name##_show(struct perf_object *pobj, char *buf)		\
+{									\
+	return read_cache_counter(pobj, buf, type, event);		\
+}									\
+static PERF_OBJ_ATTR_RO(name)
+
+CACHE_SHOW(read_hit, CACHE_CHANNEL_RD, CACHE_RD_HIT);
+CACHE_SHOW(read_miss, CACHE_CHANNEL_RD, CACHE_RD_MISS);
+CACHE_SHOW(write_hit, CACHE_CHANNEL_WR, CACHE_WR_HIT);
+CACHE_SHOW(write_miss, CACHE_CHANNEL_WR, CACHE_WR_MISS);
+CACHE_SHOW(hold_request, CACHE_CHANNEL_RD, CACHE_HOLD_REQ);
+CACHE_SHOW(tx_req_stall, CACHE_CHANNEL_RD, CACHE_TX_REQ_STALL);
+CACHE_SHOW(rx_req_stall, CACHE_CHANNEL_RD, CACHE_RX_REQ_STALL);
+CACHE_SHOW(rx_eviction, CACHE_CHANNEL_RD, CACHE_EVICTIONS);
+CACHE_SHOW(data_write_port_contention, CACHE_CHANNEL_WR,
+	   CACHE_DATA_WR_PORT_CONTEN);
+CACHE_SHOW(tag_write_port_contention, CACHE_CHANNEL_WR,
+	   CACHE_TAG_WR_PORT_CONTEN);
+
+static struct attribute *cache_attrs[] = {
+	&perf_obj_attr_read_hit.attr,
+	&perf_obj_attr_read_miss.attr,
+	&perf_obj_attr_write_hit.attr,
+	&perf_obj_attr_write_miss.attr,
+	&perf_obj_attr_hold_request.attr,
+	&perf_obj_attr_data_write_port_contention.attr,
+	&perf_obj_attr_tag_write_port_contention.attr,
+	&perf_obj_attr_tx_req_stall.attr,
+	&perf_obj_attr_rx_req_stall.attr,
+	&perf_obj_attr_rx_eviction.attr,
+	&perf_obj_attr_freeze.attr,
+	NULL,
+};
+
+static struct attribute_group cache_attr_group = {
+	.name = "cache",
+	.attrs = cache_attrs,
+};
+
+static const struct attribute_group *perf_dev_attr_groups[] = {
+	&clock_attr_group,
+	&cache_attr_group,
+	NULL,
+};
+
+ssize_t vtd_freeze_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_fpmon_vtd_ctl ctl;
+	struct feature_fme_gperf *gperf;
+
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					FME_FEATURE_ID_GLOBAL_PERF);
+	ctl.csr = readq(&gperf->vtd_ctl);
+	return scnprintf(buf, PAGE_SIZE, "%d\n", ctl.freeze);
+}
+
+ssize_t vtd_freeze_store(struct perf_object *pobj,
+		 const char *buf, size_t n)
+{
+	struct feature_platform_data *pdata;
+	struct feature_fme_fpmon_vtd_ctl ctl;
+	struct feature_fme_gperf *gperf;
+	bool state;
+
+	if (strtobool(buf, &state))
+		return -EINVAL;
+
+	pdata = dev_get_platdata(pobj->fme_dev);
+	mutex_lock(&pdata->lock);
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	ctl.csr = readq(&gperf->vtd_ctl);
+	ctl.freeze = state;
+	writeq(ctl.csr, &gperf->vtd_ctl);
+	mutex_unlock(&pdata->lock);
+
+	return n;
+}
+
+static PERF_OBJ_ATTR(vtd_freeze, freeze, 0644, vtd_freeze_show,
+		     vtd_freeze_store);
+static struct attribute *iommu_top_attrs[] = {
+	&perf_obj_attr_vtd_freeze.attr,
+	NULL,
+};
+
+static struct attribute_group iommu_top_attr_group = {
+	.attrs = iommu_top_attrs,
+};
+
+static const struct attribute_group *iommu_top_attr_groups[] = {
+	&iommu_top_attr_group,
+	NULL,
+};
+
+static ssize_t read_iommu_counter(struct perf_object *pobj,
+				  enum gperf_vtd_events base_event, char *buf)
+{
+	struct feature_platform_data *pdata;
+	struct feature_fme_fpmon_vtd_ctl ctl;
+	struct feature_fme_fpmon_vtd_ctr ctr;
+	struct feature_fme_gperf *gperf;
+	enum gperf_vtd_events event = base_event + pobj->id;
+	u64 counter;
+
+	pdata = dev_get_platdata(pobj->fme_dev);
+	mutex_lock(&pdata->lock);
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	ctl.csr = readq(&gperf->vtd_ctl);
+	ctl.vtd_evtcode = event;
+	writeq(ctl.csr, &gperf->vtd_ctl);
+
+	ctr.event_code = event;
+
+	if (fpga_wait_register_field(event_code, ctr,
+				     &gperf->vtd_ctr, GPERF_TIMEOUT, 1)) {
+		dev_err(pobj->fme_dev,
+		"timeout, unmatched VTd event type in counter registers.\n");
+		mutex_unlock(&pdata->lock);
+		return -ETIMEDOUT;
+	}
+
+	ctr.csr = readq(&gperf->vtd_ctr);
+	counter = ctr.vtd_counter;
+	mutex_unlock(&pdata->lock);
+
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", counter);
+}
+
+#define VTD_SHOW(name, base_event)					\
+static ssize_t name##_show(struct perf_object *pobj, char *buf)		\
+{									\
+	return read_iommu_counter(pobj, base_event, buf);		\
+}									\
+static PERF_OBJ_ATTR_RO(name)
+
+VTD_SHOW(read_transaction, VTD_AFU0_MEM_RD_TRANS);
+VTD_SHOW(write_transaction, VTD_AFU0_MEM_WR_TRANS);
+VTD_SHOW(tlb_read_hit, VTD_AFU0_TLB_RD_HIT);
+VTD_SHOW(tlb_write_hit, VTD_AFU0_TLB_WR_HIT);
+
+static struct attribute *iommu_attrs[] = {
+	&perf_obj_attr_read_transaction.attr,
+	&perf_obj_attr_write_transaction.attr,
+	&perf_obj_attr_tlb_read_hit.attr,
+	&perf_obj_attr_tlb_write_hit.attr,
+	NULL,
+};
+
+static struct attribute_group iommu_attr_group = {
+	.attrs = iommu_attrs,
+};
+
+static const struct attribute_group *iommu_attr_groups[] = {
+	&iommu_attr_group,
+	NULL,
+};
+
+static bool fabric_pobj_is_enabled(struct perf_object *pobj,
+		struct feature_fme_gperf *gperf)
+{
+	struct feature_fme_fpmon_fab_ctl ctl;
+
+	ctl.csr = readq(&gperf->fab_ctl);
+
+	if (ctl.port_filter == FAB_DISABLE_FILTER)
+		return pobj->id == PERF_OBJ_ROOT_ID;
+
+	return pobj->id == ctl.port_id;
+}
+
+static ssize_t read_fabric_counter(struct perf_object *pobj,
+				  enum gperf_fab_events fab_event, char *buf)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(pobj->fme_dev);
+	struct feature_fme_fpmon_fab_ctl ctl;
+	struct feature_fme_fpmon_fab_ctr ctr;
+	struct feature_fme_gperf *gperf;
+	u64 counter = 0;
+
+	mutex_lock(&pdata->lock);
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+
+	/* if it is disabled, force the counter to return zero. */
+	if (!fabric_pobj_is_enabled(pobj, gperf))
+		goto exit;
+
+	ctl.csr = readq(&gperf->fab_ctl);
+	ctl.fab_evtcode = fab_event;
+	writeq(ctl.csr, &gperf->fab_ctl);
+
+	ctr.event_code = fab_event;
+
+	if (fpga_wait_register_field(event_code, ctr,
+				     &gperf->fab_ctr, GPERF_TIMEOUT, 1)) {
+		dev_err(pobj->fme_dev,
+		"timeout, unmatched VTd event type in counter registers.\n");
+		mutex_unlock(&pdata->lock);
+		return -ETIMEDOUT;
+	}
+
+	ctr.csr = readq(&gperf->fab_ctr);
+	counter = ctr.fab_cnt;
+exit:
+	mutex_unlock(&pdata->lock);
+	return scnprintf(buf, PAGE_SIZE, "0x%llx\n", counter);
+}
+
+#define FAB_SHOW(name, event)						\
+static ssize_t name##_show(struct perf_object *pobj, char *buf)		\
+{									\
+	return read_fabric_counter(pobj, event, buf);			\
+}									\
+static PERF_OBJ_ATTR_RO(name)
+
+FAB_SHOW(pcie0_read, FAB_PCIE0_RD);
+FAB_SHOW(pcie0_write, FAB_PCIE0_WR);
+FAB_SHOW(pcie1_read, FAB_PCIE1_RD);
+FAB_SHOW(pcie1_write, FAB_PCIE1_WR);
+FAB_SHOW(upi_read, FAB_UPI_RD);
+FAB_SHOW(upi_write, FAB_UPI_WR);
+FAB_SHOW(mmio_read, FAB_MMIO_RD);
+FAB_SHOW(mmio_write, FAB_MMIO_WR);
+
+static ssize_t fab_enable_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_gperf *gperf;
+	int status;
+
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+
+	status = fabric_pobj_is_enabled(pobj, gperf);
+	return scnprintf(buf, PAGE_SIZE, "%d\n", status);
+}
+
+/*
+ * If enable one port or all port event counter in fabric, other
+ * fabric event counter originally enabled will be disable automatically.
+ */
+static ssize_t fab_enable_store(struct perf_object *pobj,
+		 const char *buf, size_t n)
+{
+	struct feature_platform_data *pdata  = dev_get_platdata(pobj->fme_dev);
+	struct feature_fme_fpmon_fab_ctl ctl;
+	struct feature_fme_gperf *gperf;
+	bool state;
+
+	if (strtobool(buf, &state))
+		return -EINVAL;
+
+	if (!state)
+		return -EINVAL;
+
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+
+	/* if it is already enabled. */
+	if (fabric_pobj_is_enabled(pobj, gperf))
+		return n;
+
+	mutex_lock(&pdata->lock);
+	ctl.csr = readq(&gperf->fab_ctl);
+	if (pobj->id == PERF_OBJ_ROOT_ID)
+		ctl.port_filter = FAB_DISABLE_FILTER;
+	else {
+		ctl.port_filter = FAB_ENABLE_FILTER;
+		ctl.port_id = pobj->id;
+	}
+
+	writeq(ctl.csr, &gperf->fab_ctl);
+	mutex_unlock(&pdata->lock);
+
+	return n;
+}
+
+static PERF_OBJ_ATTR(fab_enable, enable, 0644, fab_enable_show,
+		     fab_enable_store);
+
+static struct attribute *fabric_attrs[] = {
+	&perf_obj_attr_pcie0_read.attr,
+	&perf_obj_attr_pcie0_write.attr,
+	&perf_obj_attr_pcie1_read.attr,
+	&perf_obj_attr_pcie1_write.attr,
+	&perf_obj_attr_upi_read.attr,
+	&perf_obj_attr_upi_write.attr,
+	&perf_obj_attr_mmio_read.attr,
+	&perf_obj_attr_mmio_write.attr,
+	&perf_obj_attr_fab_enable.attr,
+	NULL,
+};
+
+static struct attribute_group fabric_attr_group = {
+	.attrs = fabric_attrs,
+};
+
+static const struct attribute_group *fabric_attr_groups[] = {
+	&fabric_attr_group,
+	NULL,
+};
+
+static ssize_t fab_freeze_show(struct perf_object *pobj, char *buf)
+{
+	struct feature_fme_gperf *gperf;
+	struct feature_fme_fpmon_fab_ctl ctl;
+
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	ctl.csr = readq(&gperf->fab_ctl);
+	return scnprintf(buf, PAGE_SIZE, "%d\n", ctl.freeze);
+}
+
+static ssize_t fab_freeze_store(struct perf_object *pobj,
+		 const char *buf, size_t n)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(pobj->fme_dev);
+	struct feature_fme_gperf *gperf;
+	struct feature_fme_fpmon_fab_ctl ctl;
+	bool state;
+
+	if (strtobool(buf, &state))
+		return -EINVAL;
+
+	mutex_lock(&pdata->lock);
+	gperf = get_feature_ioaddr_by_index(pobj->fme_dev,
+					    FME_FEATURE_ID_GLOBAL_PERF);
+	ctl.csr = readq(&gperf->fab_ctl);
+	ctl.freeze = state;
+	writeq(ctl.csr, &gperf->fab_ctl);
+	mutex_unlock(&pdata->lock);
+
+	return n;
+}
+
+static PERF_OBJ_ATTR(fab_freeze, freeze, 0644, fab_freeze_show,
+		     fab_freeze_store);
+
+static struct attribute *fabric_top_attrs[] = {
+	&perf_obj_attr_fab_freeze.attr,
+	NULL,
+};
+
+static struct attribute_group fabric_top_attr_group = {
+	.attrs = fabric_top_attrs,
+};
+
+static const struct attribute_group *fabric_top_attr_groups[] = {
+	&fabric_attr_group,
+	&fabric_top_attr_group,
+	NULL,
+};
+
+static struct perf_object *
+create_perf_obj(struct device *fme_dev, struct kobject *parent, int id,
+		const struct attribute_group **groups, const char *name)
+{
+	struct perf_object *pobj;
+	int ret;
+
+	pobj = kzalloc(sizeof(*pobj), GFP_KERNEL);
+	if (!pobj)
+		return ERR_PTR(-ENOMEM);
+
+	pobj->id = id;
+	pobj->fme_dev = fme_dev;
+	pobj->attr_groups = groups;
+	INIT_LIST_HEAD(&pobj->node);
+	INIT_LIST_HEAD(&pobj->children);
+
+	if (id != PERF_OBJ_ROOT_ID)
+		ret = kobject_init_and_add(&pobj->kobj, &perf_obj_ktype,
+				   parent, "%s%d", name, id);
+	else
+		ret = kobject_init_and_add(&pobj->kobj, &perf_obj_ktype,
+				   parent, "%s", name);
+	if (ret)
+		goto put_exit;
+
+	if (pobj->attr_groups) {
+		ret = sysfs_create_groups(&pobj->kobj, pobj->attr_groups);
+		if (ret)
+			goto put_exit;
+	}
+
+	return pobj;
+
+put_exit:
+	kobject_put(&pobj->kobj);
+	return ERR_PTR(ret);
+}
+
+static void destroy_perf_obj(struct perf_object *pobj)
+{
+	struct perf_object *obj, *obj_tmp;
+
+	list_for_each_entry_safe(obj, obj_tmp, &pobj->children, node)
+		destroy_perf_obj(obj);
+
+	list_del(&pobj->node);
+	if (pobj->attr_groups)
+		sysfs_remove_groups(&pobj->kobj, pobj->attr_groups);
+	kobject_put(&pobj->kobj);
+}
+
+#define PERF_MAX_PORT_NUM	2
+
+static int create_perf_iommu_obj(struct perf_object *perf_dev)
+{
+	struct perf_object *pobj;
+	struct feature_fme_header *fme_hdr;
+	struct feature_fme_capability fme_capability;
+	int i;
+
+	fme_hdr = get_feature_ioaddr_by_index(perf_dev->fme_dev,
+					      FME_FEATURE_ID_HEADER);
+
+	/* check if iommu is not supported on this device. */
+	fme_capability.csr = readq(&fme_hdr->capability);
+	if (!fme_capability.iommu_support)
+		return 0;
+
+	pobj = create_perf_obj(perf_dev->fme_dev, &perf_dev->kobj,
+			   PERF_OBJ_ROOT_ID, iommu_top_attr_groups, "iommu");
+	if (IS_ERR(pobj))
+		return PTR_ERR(pobj);
+
+	list_add(&pobj->node, &perf_dev->children);
+
+	for (i = 0; i < PERF_MAX_PORT_NUM; i++) {
+		struct perf_object *obj;
+
+		obj = create_perf_obj(perf_dev->fme_dev, &pobj->kobj, i,
+				       iommu_attr_groups, "afu");
+		if (IS_ERR(obj))
+			return PTR_ERR(obj);
+
+		list_add(&obj->node, &pobj->children);
+	}
+
+	return 0;
+}
+
+static int create_perf_fabric_obj(struct perf_object *perf_dev)
+{
+	struct perf_object *pobj;
+	int i;
+
+	pobj = create_perf_obj(perf_dev->fme_dev, &perf_dev->kobj,
+			   PERF_OBJ_ROOT_ID, fabric_top_attr_groups, "fabric");
+	if (IS_ERR(pobj))
+		return PTR_ERR(pobj);
+
+	list_add(&pobj->node, &perf_dev->children);
+
+	for (i = 0; i < PERF_MAX_PORT_NUM; i++) {
+		struct perf_object *obj;
+
+		obj = create_perf_obj(perf_dev->fme_dev, &pobj->kobj, i,
+				       fabric_attr_groups, "port");
+		if (IS_ERR(obj))
+			return PTR_ERR(obj);
+
+		list_add(&obj->node, &pobj->children);
+	}
+
+	return 0;
+}
+
+static struct perf_object *create_perf_dev(struct platform_device *pdev)
+{
+	return create_perf_obj(&pdev->dev, &pdev->dev.kobj,
+			   PERF_OBJ_ROOT_ID, perf_dev_attr_groups, "perf");
+}
+
+static int fme_perf_init(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+	struct perf_object *perf_dev;
+	int ret;
+
+	perf_dev = create_perf_dev(pdev);
+	if (IS_ERR(perf_dev))
+		return PTR_ERR(perf_dev);
+
+	ret = create_perf_iommu_obj(perf_dev);
+	if (ret) {
+		destroy_perf_obj(perf_dev);
+		return ret;
+	}
+
+	ret = create_perf_fabric_obj(perf_dev);
+	if (ret) {
+		destroy_perf_obj(perf_dev);
+		return ret;
+	}
+
+	fme = fpga_pdata_get_private(pdata);
+	fme->perf_dev = perf_dev;
+	return 0;
+}
+
+static void
+fme_perf_uinit(struct platform_device *pdev, struct feature *feature)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+
+	fme = fpga_pdata_get_private(pdata);
+	destroy_perf_obj(fme->perf_dev);
+	fme->perf_dev = NULL;
+}
+
+struct feature_ops global_perf_ops = {
+	.init = fme_perf_init,
+	.uinit = fme_perf_uinit,
+};
diff --git a/drivers/fpga/intel/fme-pr.c b/drivers/fpga/intel/fme-pr.c
new file mode 100644
index 000000000000..ac3851b57661
--- /dev/null
+++ b/drivers/fpga/intel/fme-pr.c
@@ -0,0 +1,469 @@
+/*
+ * Driver for FPGA Partial Reconfiguration
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Christopher Rauer <christopher.rauer@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include <linux/types.h>
+#include <linux/device.h>
+#include <linux/delay.h>
+#include <linux/vmalloc.h>
+#include <linux/uaccess.h>
+#include <linux/stddef.h> /* offsetofend */
+#include <linux/vfio.h> /* offsetofend in pre-4.1.0 kernels */
+#include <linux/intel-fpga.h>
+#include <linux/fpga/fpga-mgr.h>
+
+#include "feature-dev.h"
+#include "fme.h"
+
+#define PR_WAIT_TIMEOUT		8000000
+
+#define PR_HOST_STATUS_IDLE	0
+
+DEFINE_FPGA_PR_ERR_MSG(pr_err_msg);
+
+#ifdef CONFIG_AS_AVX512
+static inline void copy512(void *src, void *dst)
+{
+	asm volatile("vmovdqu64 (%0), %%zmm0;"
+		     "vmovntdq %%zmm0, (%1);"
+		     :
+		     : "r"(src), "r"(dst));
+}
+#else
+static inline void copy512(void *src, void *dst)
+{
+	WARN_ON_ONCE(1);
+}
+#endif
+
+static ssize_t revision_show(struct device *dev, struct device_attribute *attr,
+			     char *buf)
+{
+	struct feature_fme_pr *fme_pr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_PR_MGMT);
+	struct feature_header header;
+
+	header.csr = readq(&fme_pr->header);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", header.revision);
+}
+
+static DEVICE_ATTR_RO(revision);
+
+static ssize_t interface_id_show(struct device *dev,
+	struct device_attribute *attr, char *buf)
+{
+	struct feature_fme_pr_key intfc_id0_l, intfc_id0_h;
+	struct feature_fme_pr *fme_pr
+		= get_feature_ioaddr_by_index(dev, FME_FEATURE_ID_PR_MGMT);
+
+	intfc_id0_l.key = readq(&fme_pr->fme_pr_intfc_id0_l);
+	intfc_id0_h.key = readq(&fme_pr->fme_pr_intfc_id0_h);
+
+	return scnprintf(buf, PAGE_SIZE, "%016llx%016llx\n",
+			intfc_id0_h.key, intfc_id0_l.key);
+}
+
+static DEVICE_ATTR_RO(interface_id);
+
+static struct attribute *pr_mgmt_attrs[] = {
+	&dev_attr_revision.attr,
+	&dev_attr_interface_id.attr,
+	NULL,
+};
+
+struct attribute_group pr_mgmt_attr_group = {
+	.attrs	= pr_mgmt_attrs,
+	.name	= "pr",
+};
+
+static u64
+pr_err_handle(struct platform_device *pdev, struct feature_fme_pr *fme_pr)
+{
+	struct feature_fme_pr_status fme_pr_status;
+	unsigned long err_code;
+	u64 fme_pr_error;
+	int i = 0;
+
+	fme_pr_status.csr = readq(&fme_pr->ccip_fme_pr_status);
+	if (!fme_pr_status.pr_status)
+		return 0;
+
+	err_code = fme_pr_error = readq(&fme_pr->ccip_fme_pr_err);
+	for_each_set_bit(i, &err_code, PR_MAX_ERR_NUM)
+		dev_dbg(&pdev->dev, "%s\n", pr_err_msg[i]);
+	writeq(fme_pr_error, &fme_pr->ccip_fme_pr_err);
+	return fme_pr_error;
+}
+
+static int fme_pr_write_init(struct fpga_manager *mgr,
+		struct fpga_image_info *info, const char *buf, size_t count)
+{
+	struct fpga_fme *fme = mgr->priv;
+	struct platform_device *pdev;
+	struct feature_fme_pr *fme_pr;
+	struct feature_fme_pr_ctl fme_pr_ctl;
+	struct feature_fme_pr_status fme_pr_status;
+
+	pdev = fme->pdata->dev;
+	fme_pr = get_feature_ioaddr_by_index(&pdev->dev,
+				FME_FEATURE_ID_PR_MGMT);
+	if (!fme_pr)
+		return -EINVAL;
+
+	if (WARN_ON(info->flags != FPGA_MGR_PARTIAL_RECONFIG))
+		return -EINVAL;
+
+	dev_dbg(&pdev->dev, "resetting PR before initiated PR\n");
+
+	fme_pr_ctl.csr = readq(&fme_pr->ccip_fme_pr_control);
+	fme_pr_ctl.pr_reset = 1;
+	writeq(fme_pr_ctl.csr, &fme_pr->ccip_fme_pr_control);
+
+	fme_pr_ctl.pr_reset_ack = 1;
+
+	if (fpga_wait_register_field(pr_reset_ack, fme_pr_ctl,
+		&fme_pr->ccip_fme_pr_control, PR_WAIT_TIMEOUT, 1)) {
+		dev_err(&pdev->dev, "maximum PR timeout\n");
+		return -ETIMEDOUT;
+	}
+
+	fme_pr_ctl.csr = readq(&fme_pr->ccip_fme_pr_control);
+	fme_pr_ctl.pr_reset = 0;
+	writeq(fme_pr_ctl.csr, &fme_pr->ccip_fme_pr_control);
+
+	dev_dbg(&pdev->dev,
+		"waiting for PR resource in HW to be initialized and ready\n");
+
+	fme_pr_status.pr_host_status = PR_HOST_STATUS_IDLE;
+
+	if (fpga_wait_register_field(pr_host_status, fme_pr_status,
+		&fme_pr->ccip_fme_pr_status, PR_WAIT_TIMEOUT, 1)) {
+		dev_err(&pdev->dev, "maximum PR timeout\n");
+		return -ETIMEDOUT;
+	}
+
+	dev_dbg(&pdev->dev, "check if have any previous PR error\n");
+	pr_err_handle(pdev, fme_pr);
+	return 0;
+}
+
+static int fme_pr_write(struct fpga_manager *mgr,
+			const char *buf, size_t count)
+{
+	struct fpga_fme *fme = mgr->priv;
+	struct platform_device *pdev;
+	struct feature_fme_pr *fme_pr;
+	struct feature_fme_pr_ctl fme_pr_ctl;
+	struct feature_fme_pr_status fme_pr_status;
+	struct feature_fme_pr_data fme_pr_data;
+	int delay = 0, pr_credit;
+
+	pdev = fme->pdata->dev;
+	fme_pr = get_feature_ioaddr_by_index(&pdev->dev,
+				FME_FEATURE_ID_PR_MGMT);
+
+	dev_dbg(&pdev->dev, "set PR port ID and start request\n");
+
+	fme_pr_ctl.csr = readq(&fme_pr->ccip_fme_pr_control);
+	fme_pr_ctl.pr_regionid = fme->port_id;
+	fme_pr_ctl.pr_start_req = 1;
+	writeq(fme_pr_ctl.csr, &fme_pr->ccip_fme_pr_control);
+
+	dev_dbg(&pdev->dev, "pushing data from bitstream to HW\n");
+
+	fme_pr_status.csr = readq(&fme_pr->ccip_fme_pr_status);
+	pr_credit = fme_pr_status.pr_credit;
+
+	while (count > 0) {
+		while (pr_credit <= 1) {
+			if (delay++ > PR_WAIT_TIMEOUT) {
+				dev_err(&pdev->dev, "maximum try\n");
+
+				fme->pr_err = pr_err_handle(pdev, fme_pr);
+				if (fme->pr_err)
+					return -EIO;
+
+				return -ETIMEDOUT;
+			}
+			udelay(1);
+
+			fme_pr_status.csr = readq(&fme_pr->ccip_fme_pr_status);
+			pr_credit = fme_pr_status.pr_credit;
+		};
+
+		if (count >= fme->pr_bandwidth) {
+			switch (fme->pr_bandwidth) {
+			case 4:
+				fme_pr_data.rsvd = 0;
+				fme_pr_data.pr_data_raw = *((u32 *)buf);
+				writeq(fme_pr_data.csr,
+				       &fme_pr->ccip_fme_pr_data);
+				break;
+			case 64:
+				copy512((void *)buf,
+					&fme_pr->fme_pr_pub_harsh3.key);
+				break;
+			default:
+				return -EFAULT;
+			}
+
+			buf += fme->pr_bandwidth;
+			count -= fme->pr_bandwidth;
+			pr_credit--;
+		} else {
+			WARN_ON(1);
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static int fme_pr_write_complete(struct fpga_manager *mgr,
+			struct fpga_image_info *info)
+{
+	struct fpga_fme *fme = mgr->priv;
+	struct platform_device *pdev;
+	struct feature_fme_pr *fme_pr;
+	struct feature_fme_pr_ctl fme_pr_ctl;
+
+	pdev = fme->pdata->dev;
+	fme_pr = get_feature_ioaddr_by_index(&pdev->dev,
+				FME_FEATURE_ID_PR_MGMT);
+
+	fme_pr_ctl.csr = readq(&fme_pr->ccip_fme_pr_control);
+	fme_pr_ctl.pr_push_complete = 1;
+	writeq(fme_pr_ctl.csr, &fme_pr->ccip_fme_pr_control);
+
+	dev_dbg(&pdev->dev, "green bitstream push complete\n");
+	dev_dbg(&pdev->dev, "waiting for HW to release PR resource\n");
+
+	fme_pr_ctl.pr_start_req = 0;
+
+	if (fpga_wait_register_field(pr_start_req, fme_pr_ctl,
+		&fme_pr->ccip_fme_pr_control, PR_WAIT_TIMEOUT, 1)) {
+		dev_err(&pdev->dev, "maximum try.\n");
+		return -ETIMEDOUT;
+	}
+
+	dev_dbg(&pdev->dev, "PR operation complete, checking status\n");
+	fme->pr_err = pr_err_handle(pdev, fme_pr);
+	if (fme->pr_err)
+		return -EIO;
+
+	dev_dbg(&pdev->dev, "PR done successfully\n");
+	return 0;
+}
+
+static enum fpga_mgr_states fme_pr_state(struct fpga_manager *mgr)
+{
+	return FPGA_MGR_STATE_UNKNOWN;
+}
+
+static const struct fpga_manager_ops fme_pr_ops = {
+	.write_init = fme_pr_write_init,
+	.write = fme_pr_write,
+	.write_complete = fme_pr_write_complete,
+	.state = fme_pr_state,
+};
+
+static int fme_pr(struct platform_device *pdev, unsigned long arg)
+{
+	void __user *argp = (void __user *)arg;
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct fpga_fme *fme;
+	struct fpga_manager *mgr;
+	struct feature_fme_header *fme_hdr;
+	struct feature_fme_capability fme_capability;
+	struct fpga_image_info info;
+	struct fpga_fme_port_pr port_pr;
+	struct platform_device *port;
+	unsigned long minsz;
+	void *buf = NULL;
+	size_t length;
+	int ret = 0;
+
+	minsz = offsetofend(struct fpga_fme_port_pr, status);
+
+	if (copy_from_user(&port_pr, argp, minsz))
+		return -EFAULT;
+
+	if (port_pr.argsz < minsz || port_pr.flags)
+		return -EINVAL;
+
+	/* get fme header region */
+	fme_hdr = get_feature_ioaddr_by_index(&pdev->dev,
+					FME_FEATURE_ID_HEADER);
+	if (WARN_ON(!fme_hdr))
+		return -EINVAL;
+
+	/* check port id */
+	fme_capability.csr = readq(&fme_hdr->capability);
+	if (port_pr.port_id >= fme_capability.num_ports) {
+		dev_dbg(&pdev->dev, "port number more than maximum\n");
+		return -EINVAL;
+	}
+
+	if (!access_ok(VERIFY_READ, port_pr.buffer_address,
+				    port_pr.buffer_size))
+		return -EFAULT;
+
+	mutex_lock(&pdata->lock);
+	fme = fpga_pdata_get_private(pdata);
+	/* fme device has been unregistered. */
+	if (!fme) {
+		ret = -EINVAL;
+		goto unlock_exit;
+	}
+
+	/*
+	 * Padding extra zeros to align PR buffer with PR bandwidth, HW will
+	 * ignore these zeros automatically.
+	 */
+	length = ALIGN(port_pr.buffer_size, fme->pr_bandwidth);
+
+	buf = vzalloc(length);
+	if (!buf) {
+		ret = -ENOMEM;
+		goto unlock_exit;
+	}
+
+	if (copy_from_user(buf, (void __user *)port_pr.buffer_address,
+					       port_pr.buffer_size)) {
+		ret = -EFAULT;
+		goto free_exit;
+	}
+
+	memset(&info, 0, sizeof(struct fpga_image_info));
+	info.flags = FPGA_MGR_PARTIAL_RECONFIG;
+
+	mgr = fpga_mgr_get(&pdev->dev);
+	if (IS_ERR(mgr)) {
+		ret = PTR_ERR(mgr);
+		goto free_exit;
+	}
+
+	fme->pr_err = 0;
+	fme->port_id = port_pr.port_id;
+
+	/* Find and get port device by index */
+	port = pdata->fpga_for_each_port(pdev, &fme->port_id,
+					 fpga_port_check_id);
+	WARN_ON(!port);
+
+	/* Disable Port before PR */
+	fpga_port_disable(port);
+
+	ret = fpga_mgr_buf_load(mgr, &info, buf, length);
+	port_pr.status = fme->pr_err;
+
+	/* Re-enable Port after PR finished */
+	fpga_port_enable(port);
+
+	put_device(&port->dev);
+
+	fpga_mgr_put(mgr);
+free_exit:
+	vfree(buf);
+unlock_exit:
+	mutex_unlock(&pdata->lock);
+	if (copy_to_user((void __user *)arg, &port_pr, minsz))
+		return -EFAULT;
+	return ret;
+}
+
+static int fpga_fme_pr_probe(struct platform_device *pdev)
+{
+	struct feature_platform_data *pdata = dev_get_platdata(&pdev->dev);
+	struct feature_fme_pr *fme_pr;
+	struct feature_header fme_pr_header;
+	struct fpga_fme *priv;
+	int ret;
+
+	mutex_lock(&pdata->lock);
+	priv = fpga_pdata_get_private(pdata);
+
+	fme_pr = get_feature_ioaddr_by_index(&pdev->dev,
+				FME_FEATURE_ID_PR_MGMT);
+
+	fme_pr_header.csr = readq(&fme_pr->header);
+	if (fme_pr_header.revision == 2) {
+		dev_dbg(&pdev->dev, "using 512-bit PR\n");
+		priv->pr_bandwidth = 64;
+	} else {
+		dev_dbg(&pdev->dev, "using 32-bit PR\n");
+		priv->pr_bandwidth = 4;
+	}
+
+	ret = fpga_mgr_register(&pdata->dev->dev,
+		"Intel FPGA Manager", &fme_pr_ops, priv);
+	mutex_unlock(&pdata->lock);
+
+	return ret;
+}
+
+static int fpga_fme_pr_remove(struct platform_device *pdev)
+{
+	fpga_mgr_unregister(&pdev->dev);
+	return 0;
+}
+
+static int pr_mgmt_init(struct platform_device *pdev, struct feature *feature)
+{
+	int ret;
+
+	ret = fpga_fme_pr_probe(pdev);
+	if (ret)
+		return ret;
+
+	ret = sysfs_create_group(&pdev->dev.kobj, &pr_mgmt_attr_group);
+	if (ret)
+		fpga_fme_pr_remove(pdev);
+
+	return ret;
+}
+
+static void pr_mgmt_uinit(struct platform_device *pdev, struct feature *feature)
+{
+	sysfs_remove_group(&pdev->dev.kobj, &pr_mgmt_attr_group);
+	fpga_fme_pr_remove(pdev);
+}
+
+static long fme_pr_ioctl(struct platform_device *pdev, struct feature *feature,
+	unsigned int cmd, unsigned long arg)
+{
+	long ret;
+
+	switch (cmd) {
+	case FPGA_FME_PORT_PR:
+		ret = fme_pr(pdev, arg);
+		break;
+	default:
+		ret = -ENODEV;
+	}
+
+	return ret;
+}
+
+struct feature_ops pr_mgmt_ops = {
+	.init = pr_mgmt_init,
+	.uinit = pr_mgmt_uinit,
+	.ioctl = fme_pr_ioctl,
+};
diff --git a/drivers/fpga/intel/fme.h b/drivers/fpga/intel/fme.h
new file mode 100644
index 000000000000..12e09ea6a6ff
--- /dev/null
+++ b/drivers/fpga/intel/fme.h
@@ -0,0 +1,61 @@
+/*
+ * FPGA Management Engine Drier Header
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#ifndef __INTEL_FME_PR_H
+#define __INTEL_FME_PR_H
+
+#define PERF_OBJ_ROOT_ID	(~0)
+struct perf_object {
+	/*
+	 * instance id. PERF_OBJ_ROOT_ID indicates it is a parent
+	 * object which counts performance counters for all instances.
+	 */
+	int id;
+
+	/* the sysfs files are associated with this object. */
+	const struct attribute_group **attr_groups;
+
+	/* the fme feature device. */
+	struct device *fme_dev;
+
+	/*
+	 * they are used to construct parent-children hierarchy.
+	 *
+	 * 'node' is used to link itself to parent's children list.
+	 * 'children' is used to link its children objects together.
+	 */
+	struct list_head node;
+	struct list_head children;
+
+	struct kobject kobj;
+};
+
+struct fpga_fme {
+	u8  port_id;
+	u64 pr_err;
+	int pr_bandwidth;
+	struct device *dev_err;
+	struct perf_object *perf_dev;
+	struct feature_platform_data *pdata;
+};
+
+extern struct feature_ops global_error_ops;
+extern struct feature_ops pr_mgmt_ops;
+extern struct feature_ops global_perf_ops;
+#endif
diff --git a/drivers/fpga/intel/pcie.c b/drivers/fpga/intel/pcie.c
new file mode 100644
index 000000000000..b0e8593284fa
--- /dev/null
+++ b/drivers/fpga/intel/pcie.c
@@ -0,0 +1,1272 @@
+/*
+ * Driver for the PCIe device which locates between CPU and accelerated
+ * function units(AFUs) and allows them to communicate with each other.
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Zhang Yi <Yi.Z.Zhang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *   Joseph Grecco <joe.grecco@intel.com>
+ *   Enno Luebbers <enno.luebbers@intel.com>
+ *   Tim Whisonant <tim.whisonant@intel.com>
+ *   Ananda Ravuri <ananda.ravuri@intel.com>
+ *   Mitchel, Henry <henry.mitchel@intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include <linux/pci.h>
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/stddef.h>
+#include <linux/errno.h>
+#include <linux/aer.h>
+#include <linux/uuid.h>
+#include <linux/kdev_t.h>
+#include <linux/mfd/core.h>
+
+#include "feature-dev.h"
+
+#define DRV_VERSION	"EXPERIMENTAL VERSION"
+#define DRV_NAME	"intel-fpga-pci"
+
+static DEFINE_MUTEX(fpga_id_mutex);
+
+enum fpga_id_type {
+	/*
+	 * fpga parent device id allocation and mapping, parent device
+	 * is the container of fme device and port device
+	 */
+	PARENT_ID,
+	/* fme id allocation and mapping */
+	FME_ID,
+	/* port id allocation and mapping */
+	PORT_ID,
+	FPGA_ID_MAX,
+};
+
+/* it is protected by fpga_id_mutex */
+static struct idr fpga_ids[FPGA_ID_MAX];
+
+struct cci_drvdata {
+	int device_id;
+	struct device *fme_dev;
+
+	struct mutex lock;
+	struct list_head port_dev_list;
+	/* number of released ports which can be configured as VF  */
+	int released_port_num;
+
+	struct list_head regions;
+};
+
+struct cci_pci_region {
+	int bar;
+	void __iomem *ioaddr;
+
+	struct list_head node;
+};
+
+static void fpga_ids_init(void)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(fpga_ids); i++)
+		idr_init(fpga_ids + i);
+}
+
+static void fpga_ids_destroy(void)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(fpga_ids); i++)
+		idr_destroy(fpga_ids + i);
+}
+
+static int alloc_fpga_id(enum fpga_id_type type, struct device *dev)
+{
+	int id;
+
+	WARN_ON(type >= FPGA_ID_MAX);
+	mutex_lock(&fpga_id_mutex);
+	id = idr_alloc(fpga_ids + type, dev, 0, 0, GFP_KERNEL);
+	mutex_unlock(&fpga_id_mutex);
+	return id;
+}
+
+static void free_fpga_id(enum fpga_id_type type, int id)
+{
+	WARN_ON(type >= FPGA_ID_MAX);
+	mutex_lock(&fpga_id_mutex);
+	idr_remove(fpga_ids + type, id);
+	mutex_unlock(&fpga_id_mutex);
+}
+
+static void cci_pci_add_port_dev(struct pci_dev *pdev,
+				 struct platform_device *port_dev)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pdev->dev);
+	struct feature_platform_data *pdata = dev_get_platdata(&port_dev->dev);
+
+	mutex_lock(&drvdata->lock);
+	list_add(&pdata->node, &drvdata->port_dev_list);
+	get_device(&pdata->dev->dev);
+	mutex_unlock(&drvdata->lock);
+}
+
+static void cci_pci_remove_port_devs(struct pci_dev *pdev)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pdev->dev);
+	struct feature_platform_data *pdata, *ptmp;
+
+	mutex_lock(&drvdata->lock);
+	list_for_each_entry_safe(pdata, ptmp, &drvdata->port_dev_list, node) {
+		struct platform_device *port_dev = pdata->dev;
+
+		/* the port should be unregistered first. */
+		WARN_ON(device_is_registered(&port_dev->dev));
+		list_del(&pdata->node);
+		free_fpga_id(PORT_ID, port_dev->id);
+		put_device(&port_dev->dev);
+	}
+	mutex_unlock(&drvdata->lock);
+}
+
+static struct platform_device *cci_pci_lookup_port_by_id(struct pci_dev *pdev,
+							 int port_id)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pdev->dev);
+	struct feature_platform_data *pdata;
+
+	list_for_each_entry(pdata, &drvdata->port_dev_list, node)
+		if (fpga_port_check_id(pdata->dev, &port_id))
+			return pdata->dev;
+
+	return NULL;
+}
+
+/* info collection during feature dev build. */
+struct build_feature_devs_info {
+	struct pci_dev *pdev;
+
+	/*
+	 * PCI BAR mapping info. Parsing feature list starts from
+	 * BAR 0 and switch to different BARs to parse Port
+	 */
+	void __iomem *ioaddr;
+	void __iomem *ioend;
+	int current_bar;
+
+	/* points to FME header where the port offset is figured out. */
+	void __iomem *pfme_hdr;
+
+	/* the container device for all feature devices */
+	struct device *parent_dev;
+
+	/* current feature device */
+	struct platform_device *feature_dev;
+};
+
+static void cci_pci_release_regions(struct pci_dev *pdev)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pdev->dev);
+	struct cci_pci_region *tmp, *region;
+
+	list_for_each_entry_safe(region, tmp, &drvdata->regions, node) {
+		list_del(&region->node);
+		if (region->ioaddr)
+			pci_iounmap(pdev, region->ioaddr);
+		devm_kfree(&pdev->dev, region);
+	}
+}
+
+static void __iomem *cci_pci_ioremap_bar(struct pci_dev *pdev, int bar)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pdev->dev);
+	struct cci_pci_region *region;
+
+	list_for_each_entry(region, &drvdata->regions, node)
+		if (region->bar == bar) {
+			dev_dbg(&pdev->dev, "BAR %d region exists\n", bar);
+			return region->ioaddr;
+		}
+
+	region = devm_kzalloc(&pdev->dev, sizeof(*region), GFP_KERNEL);
+	if (!region)
+		return NULL;
+
+	region->bar = bar;
+	region->ioaddr = pci_ioremap_bar(pdev, bar);
+	if (!region->ioaddr) {
+		dev_err(&pdev->dev, "can't ioremap memory from BAR %d.\n", bar);
+		devm_kfree(&pdev->dev, region);
+		return NULL;
+	}
+
+	list_add(&region->node, &drvdata->regions);
+	return region->ioaddr;
+}
+
+static int parse_start_from(struct build_feature_devs_info *binfo, int bar)
+{
+	binfo->ioaddr = cci_pci_ioremap_bar(binfo->pdev, bar);
+	if (!binfo->ioaddr)
+		return -ENOMEM;
+
+	binfo->current_bar = bar;
+	binfo->ioend = binfo->ioaddr + pci_resource_len(binfo->pdev, bar);
+	return 0;
+}
+
+static int parse_start(struct build_feature_devs_info *binfo)
+{
+	/* fpga feature list starts from BAR 0 */
+	return parse_start_from(binfo, 0);
+}
+
+/* switch the memory mapping to BAR# @bar */
+static int parse_switch_to(struct build_feature_devs_info *binfo, int bar)
+{
+	return parse_start_from(binfo, bar);
+}
+
+static int attach_port_dev(struct platform_device *pdev, int port_id)
+{
+	struct feature_fme_header *fme_hdr;
+	struct feature_fme_port port;
+	struct device *pci_dev = fpga_feature_dev_to_pcidev(pdev);
+	struct cci_drvdata *drvdata = dev_get_drvdata(pci_dev);
+	struct platform_device *port_dev;
+	int ret;
+
+	fme_hdr = get_feature_ioaddr_by_index(&pdev->dev,
+					      FME_FEATURE_ID_HEADER);
+
+	mutex_lock(&drvdata->lock);
+	port.csr = readq(&fme_hdr->port[port_id]);
+	if (port.afu_access_control == FME_AFU_ACCESS_VF) {
+		dev_dbg(&pdev->dev, "port_%d has already been turned to VF.\n",
+			port_id);
+		mutex_unlock(&drvdata->lock);
+		return -EBUSY;
+	}
+
+	port_dev = cci_pci_lookup_port_by_id(to_pci_dev(pci_dev), port_id);
+	if (!port_dev) {
+		dev_err(&pdev->dev, "port_%d is not detected.\n", port_id);
+		ret = -EINVAL;
+		goto exit;
+	}
+
+	if (device_is_registered(&port_dev->dev)) {
+		dev_dbg(pci_dev, "port_%d is not released.\n", port_id);
+		ret = -EBUSY;
+		goto exit;
+	}
+
+	dev_dbg(pci_dev, "now re-assign port_%d:%s\n", port_id, port_dev->name);
+
+	ret = platform_device_add(port_dev);
+	if (ret)
+		goto exit;
+
+	get_device(&port_dev->dev);
+	feature_dev_use_end(dev_get_platdata(&port_dev->dev));
+	drvdata->released_port_num--;
+exit:
+	mutex_unlock(&drvdata->lock);
+	return ret;
+}
+
+static int detach_port_dev(struct platform_device *pdev, int port_id)
+{
+	struct device *dev = fpga_feature_dev_to_pcidev(pdev);
+	struct cci_drvdata *drvdata = dev_get_drvdata(dev);
+	struct platform_device *port_dev;
+	int ret;
+
+	mutex_lock(&drvdata->lock);
+	port_dev = cci_pci_lookup_port_by_id(to_pci_dev(dev), port_id);
+	if (!port_dev) {
+		dev_err(&pdev->dev, "port_%d is not detected.\n", port_id);
+		ret = -EINVAL;
+		goto exit;
+	}
+
+	if (!device_is_registered(&port_dev->dev)) {
+		dev_dbg(&pdev->dev,
+		   "port_%d is released or already assigned a VF.\n", port_id);
+		ret = -EBUSY;
+		goto exit;
+	}
+
+	ret = feature_dev_use_excl_begin(dev_get_platdata(&port_dev->dev));
+	if (ret)
+		goto exit;
+
+	platform_device_del(port_dev);
+	put_device(&port_dev->dev);
+	drvdata->released_port_num++;
+exit:
+	mutex_unlock(&drvdata->lock);
+	return ret;
+}
+
+static int
+config_port(struct platform_device *pdev, u32 port_id, bool release)
+{
+	/* Todo: some potential check */
+	if (release)
+		return detach_port_dev(pdev, port_id);
+
+	return attach_port_dev(pdev, port_id);
+}
+
+static struct platform_device *fpga_for_each_port(struct platform_device *pdev,
+		     void *data, int (*match)(struct platform_device *, void *))
+{
+	struct device *pci_dev = fpga_feature_dev_to_pcidev(pdev);
+	struct cci_drvdata *drvdata = dev_get_drvdata(pci_dev);
+	struct feature_platform_data *pdata;
+	struct platform_device *port_dev;
+
+	mutex_lock(&drvdata->lock);
+	list_for_each_entry(pdata, &drvdata->port_dev_list, node) {
+		port_dev = pdata->dev;
+
+		if (match(port_dev, data) && get_device(&port_dev->dev))
+			goto exit;
+	}
+	port_dev = NULL;
+exit:
+	mutex_unlock(&drvdata->lock);
+	return port_dev;
+}
+
+static struct build_feature_devs_info *
+build_info_alloc_and_init(struct pci_dev *pdev)
+{
+	struct build_feature_devs_info *binfo;
+
+	binfo = devm_kzalloc(&pdev->dev, sizeof(*binfo), GFP_KERNEL);
+	if (binfo)
+		binfo->pdev = pdev;
+
+	return binfo;
+}
+
+static enum fpga_id_type feature_dev_id_type(struct platform_device *pdev)
+{
+	if (!strcmp(pdev->name, FPGA_FEATURE_DEV_FME))
+		return FME_ID;
+
+	if (!strcmp(pdev->name, FPGA_FEATURE_DEV_PORT))
+		return PORT_ID;
+
+	WARN_ON(1);
+	return FPGA_ID_MAX;
+}
+
+/*
+ * register current feature device, it is called when we need to switch to
+ * another feature parsing or we have parsed all features
+ */
+static int build_info_commit_dev(struct build_feature_devs_info *binfo)
+{
+	int ret;
+
+	if (!binfo->feature_dev)
+		return 0;
+
+	ret = platform_device_add(binfo->feature_dev);
+	if (!ret) {
+		struct cci_drvdata *drvdata;
+
+		drvdata = dev_get_drvdata(&binfo->pdev->dev);
+		if (feature_dev_id_type(binfo->feature_dev) == PORT_ID)
+			cci_pci_add_port_dev(binfo->pdev, binfo->feature_dev);
+		else
+			drvdata->fme_dev = get_device(&binfo->feature_dev->dev);
+
+		/*
+		 * reset it to avoid build_info_free() freeing their resource.
+		 *
+		 * The resource of successfully registered feature devices
+		 * will be freed by platform_device_unregister(). See the
+		 * comments in build_info_create_dev().
+		 */
+		binfo->feature_dev = NULL;
+	}
+
+	return ret;
+}
+
+static int
+build_info_create_dev(struct build_feature_devs_info *binfo,
+		      enum fpga_id_type type, int feature_nr, const char *name)
+{
+	struct platform_device *fdev;
+	struct resource *res;
+	struct feature_platform_data *pdata;
+	enum fpga_devt_type devt_type = FPGA_DEVT_FME;
+	int ret;
+
+	if (type == PORT_ID)
+		devt_type = FPGA_DEVT_PORT;
+
+	/* we will create a new device, commit current device first */
+	ret = build_info_commit_dev(binfo);
+	if (ret)
+		return ret;
+
+	/*
+	 * we use -ENODEV as the initialization indicator which indicates
+	 * whether the id need to be reclaimed
+	 */
+	fdev = binfo->feature_dev = platform_device_alloc(name, -ENODEV);
+	if (!fdev)
+		return -ENOMEM;
+
+	fdev->id = alloc_fpga_id(type, &fdev->dev);
+	if (fdev->id < 0)
+		return fdev->id;
+
+	fdev->dev.parent = binfo->parent_dev;
+	fdev->dev.devt = fpga_get_devt(devt_type, fdev->id);
+
+	/*
+	 * we need not care the memory which is associated with the
+	 * platform device. After call platform_device_unregister(),
+	 * it will be automatically freed by device's
+	 * release() callback, platform_device_release().
+	 */
+	pdata = feature_platform_data_alloc_and_init(fdev, feature_nr);
+	if (!pdata)
+		return -ENOMEM;
+
+	if (type == FME_ID) {
+		pdata->config_port = config_port;
+		pdata->fpga_for_each_port = fpga_for_each_port;
+	}
+
+	/*
+	 * the count should be initialized to 0 to make sure
+	 *__fpga_port_enable() following __fpga_port_disable()
+	 * works properly.
+	 */
+	WARN_ON(pdata->disable_count);
+
+	fdev->dev.platform_data = pdata;
+	fdev->num_resources = feature_nr;
+	fdev->resource = kcalloc(feature_nr, sizeof(*res), GFP_KERNEL);
+	if (!fdev->resource)
+		return -ENOMEM;
+
+	return 0;
+}
+
+static int remove_feature_dev(struct device *dev, void *data)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+
+	platform_device_unregister(pdev);
+	return 0;
+}
+
+static int remove_parent_dev(struct device *dev, void *data)
+{
+	/* remove platform devices attached in the parent device */
+	device_for_each_child(dev, NULL, remove_feature_dev);
+	device_unregister(dev);
+	return 0;
+}
+
+static void remove_all_devs(struct pci_dev *pdev)
+{
+	/* remove parent device and all its children. */
+	device_for_each_child(&pdev->dev, NULL, remove_parent_dev);
+}
+
+static void build_info_free(struct build_feature_devs_info *binfo)
+{
+	if (!IS_ERR_OR_NULL(binfo->parent_dev))
+		remove_all_devs(binfo->pdev);
+
+	/*
+	 * it is a valid id, free it. See comments in
+	 * build_info_create_dev()
+	 */
+	if (binfo->feature_dev && binfo->feature_dev->id >= 0)
+		free_fpga_id(feature_dev_id_type(binfo->feature_dev),
+			     binfo->feature_dev->id);
+
+	platform_device_put(binfo->feature_dev);
+
+	devm_kfree(&binfo->pdev->dev, binfo);
+}
+
+#define FEATURE_TYPE_AFU	0x1
+#define FEATURE_TYPE_PRIVATE	0x3
+
+/*
+ * FPGA hardware bug, FME/PORT GUID didn't follow the UUID/GUID rules, it
+ * it reported at:
+ *    https://hsdes.intel.com/appstore/article/#/1504370325
+ */
+/* #define FEATURE_FME_GUID "BFAF2AE9-4A52-46E3-82FE-38F0F9E17764" */
+#define FEATURE_FME_GUID "f9e17764-38f0-82fe-e346-524ae92aafbf"
+#define FEATURE_PORT_GUID "6b355b87-b06c-9642-eb42-8d139398b43a"
+
+static bool feature_is_fme(struct feature_afu_header *afu_hdr)
+{
+	uuid_le u;
+
+	uuid_le_to_bin(FEATURE_FME_GUID, &u);
+
+	return !uuid_le_cmp(u, afu_hdr->guid);
+}
+
+static bool feature_is_port(struct feature_afu_header *afu_hdr)
+{
+	uuid_le u;
+
+	uuid_le_to_bin(FEATURE_PORT_GUID, &u);
+
+	return !uuid_le_cmp(u, afu_hdr->guid);
+}
+
+/*
+ * UAFU GUID is dynamic as it can be changed after FME downloads different
+ * Green Bitstream to the port, so we treat the unknown GUIDs which are
+ * attached on port's feature list as UAFU.
+ */
+static bool feature_is_UAFU(struct build_feature_devs_info *binfo)
+{
+	if (!binfo->feature_dev ||
+	      feature_dev_id_type(binfo->feature_dev) != PORT_ID)
+		return false;
+
+	return true;
+}
+
+static void
+build_info_add_sub_feature(struct build_feature_devs_info *binfo,
+			   int feature_id, const char *feature_name,
+			   resource_size_t resource_size, void __iomem *start)
+{
+
+	struct platform_device *fdev = binfo->feature_dev;
+	struct feature_platform_data *pdata = dev_get_platdata(&fdev->dev);
+	struct resource *res = &fdev->resource[feature_id];
+
+	res->start = pci_resource_start(binfo->pdev, binfo->current_bar) +
+		start - binfo->ioaddr;
+	res->end = res->start + resource_size - 1;
+	res->flags = IORESOURCE_MEM;
+	res->name = feature_name;
+
+	feature_platform_data_add(pdata, feature_id,
+				  feature_name, feature_id, start);
+}
+
+struct feature_info {
+	const char *name;
+	resource_size_t resource_size;
+	int feature_index;
+	int revision_id;
+};
+
+/* indexed by fme feature IDs which are defined in 'enum fme_feature_id'. */
+static struct feature_info fme_features[] = {
+	{
+		.name = FME_FEATURE_HEADER,
+		.resource_size = sizeof(struct feature_fme_header),
+		.feature_index = FME_FEATURE_ID_HEADER,
+		.revision_id = FME_HEADER_REVISION
+	},
+	{
+		.name = FME_FEATURE_THERMAL_MGMT,
+		.resource_size = sizeof(struct feature_fme_thermal),
+		.feature_index = FME_FEATURE_ID_THERMAL_MGMT,
+		.revision_id = FME_THERMAL_MGMT_REVISION
+	},
+	{
+		.name = FME_FEATURE_POWER_MGMT,
+		.resource_size = sizeof(struct feature_fme_power),
+		.feature_index = FME_FEATURE_ID_POWER_MGMT,
+		.revision_id = FME_POWER_MGMT_REVISION
+	},
+	{
+		.name = FME_FEATURE_GLOBAL_PERF,
+		.resource_size = sizeof(struct feature_fme_gperf),
+		.feature_index = FME_FEATURE_ID_GLOBAL_PERF,
+		.revision_id = FME_GLOBAL_PERF_REVISION
+	},
+	{
+		.name = FME_FEATURE_GLOBAL_ERR,
+		.resource_size = sizeof(struct feature_fme_err),
+		.feature_index = FME_FEATURE_ID_GLOBAL_ERR,
+		.revision_id = FME_GLOBAL_ERR_REVISION
+	},
+	{
+		.name = FME_FEATURE_PR_MGMT,
+		.resource_size = sizeof(struct feature_fme_pr),
+		.feature_index = FME_FEATURE_ID_PR_MGMT,
+		.revision_id = FME_PR_MGMT_REVISION
+	}
+};
+
+static struct feature_info port_features[] = {
+	{
+		.name = PORT_FEATURE_HEADER,
+		.resource_size = sizeof(struct feature_port_header),
+		.feature_index = PORT_FEATURE_ID_HEADER,
+		.revision_id = PORT_HEADER_REVISION
+	},
+	{
+		.name = PORT_FEATURE_ERR,
+		.resource_size = sizeof(struct feature_port_error),
+		.feature_index = PORT_FEATURE_ID_ERROR,
+		.revision_id = PORT_ERR_REVISION
+	},
+	{
+		.name = PORT_FEATURE_UMSG,
+		.resource_size = sizeof(struct feature_port_umsg),
+		.feature_index = PORT_FEATURE_ID_UMSG,
+		.revision_id = PORT_UMSG_REVISION
+	},
+	/* SAS V0.8 didn't cover the port sub-feature PR, */
+	/* But the FEATURE ID 0x12 still there. Waiting for SAS update */
+	{
+		.name = PORT_FEATURE_PR,
+		.resource_size = 0,
+		.feature_index = PORT_FEATURE_ID_PR,
+		.revision_id = PORT_PR_REVISION
+	},
+	{
+		.name = PORT_FEATURE_STP,
+		.resource_size = PORT_FEATURE_STP_REGION_SIZE,
+		.feature_index = PORT_FEATURE_ID_STP,
+		.revision_id = PORT_STP_REVISION
+	},
+	{
+		.name = PORT_FEATURE_UAFU,
+		/* UAFU feature size should be read from PORT_CAP.MMIOSIZE.
+		 * Will set uafu feature size while parse port device.
+		 */
+		.resource_size = 0,
+		.feature_index = PORT_FEATURE_ID_UAFU,
+		.revision_id = PORT_UAFU_REVISION
+	},
+};
+
+static int
+create_feature_instance(struct build_feature_devs_info *binfo,
+			void __iomem *start, struct feature_info *finfo)
+{
+	struct feature_header *hdr = start;
+
+	if (binfo->ioend - start < finfo->resource_size)
+		return -EINVAL;
+
+	if (finfo->revision_id != SKIP_REVISION_CHECK
+		&& hdr->revision != finfo->revision_id) {
+		dev_err(&binfo->pdev->dev,
+		"feature %s revision :default:%x, now at:%x, mis-match.\n",
+		finfo->name, finfo->revision_id, hdr->revision);
+	}
+
+	build_info_add_sub_feature(binfo, finfo->feature_index, finfo->name,
+				   finfo->resource_size, start);
+	return 0;
+}
+
+static int parse_feature_fme(struct build_feature_devs_info *binfo,
+			     void __iomem *start)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&binfo->pdev->dev);
+	int ret;
+
+	ret = build_info_create_dev(binfo, FME_ID, fme_feature_num(),
+					FPGA_FEATURE_DEV_FME);
+	if (ret)
+		return ret;
+
+	if (drvdata->fme_dev) {
+		dev_err(&binfo->pdev->dev, "Multiple FMEs are detected.\n");
+		return -EINVAL;
+	}
+
+	return create_feature_instance(binfo, start,
+				       &fme_features[FME_FEATURE_ID_HEADER]);
+}
+
+static int parse_feature_fme_private(struct build_feature_devs_info *binfo,
+				     struct feature_header *hdr)
+{
+	struct feature_header header;
+
+	header.csr = readq(hdr);
+
+	if (header.id >= ARRAY_SIZE(fme_features)) {
+		dev_info(&binfo->pdev->dev, "FME feature id %x is not supported yet.\n",
+			 header.id);
+		return 0;
+	}
+
+	check_features_header(binfo->pdev, hdr, FPGA_DEVT_FME, header.id);
+
+	return create_feature_instance(binfo, hdr, &fme_features[header.id]);
+}
+
+static int parse_feature_port(struct build_feature_devs_info *binfo,
+			     void __iomem *start)
+{
+	int ret;
+
+	ret = build_info_create_dev(binfo, PORT_ID, port_feature_num(),
+					FPGA_FEATURE_DEV_PORT);
+	if (ret)
+		return ret;
+
+	return create_feature_instance(binfo, start,
+				       &port_features[PORT_FEATURE_ID_HEADER]);
+}
+
+static void enable_port_uafu(struct build_feature_devs_info *binfo,
+			     void __iomem *start)
+{
+	enum port_feature_id id = PORT_FEATURE_ID_UAFU;
+	struct feature_port_header *port_hdr;
+	struct feature_port_capability capability;
+	struct feature_port_control control;
+
+	port_hdr = (struct feature_port_header *)start;
+	capability.csr = readq(&port_hdr->capability);
+	control.csr = readq(&port_hdr->control);
+	port_features[id].resource_size = capability.mmio_size << 10;
+
+	/*
+	 * From SAS spec, to Enable UAFU, we should reset related port,
+	 * or the whole mmio space in this UAFU will be invalid
+	 */
+	if (port_features[id].resource_size)
+		fpga_port_reset(binfo->feature_dev);
+}
+
+static int parse_feature_port_private(struct build_feature_devs_info *binfo,
+				      struct feature_header *hdr)
+{
+	struct feature_header header;
+	enum port_feature_id id;
+
+	header.csr = readq(hdr);
+	/*
+	 * the region of port feature id is [0x10, 0x13], + 1 to reserve 0
+	 * which is dedicated for port-hdr.
+	 */
+	id = (header.id & 0x000f) + 1;
+
+	if (id >= ARRAY_SIZE(port_features)) {
+		dev_info(&binfo->pdev->dev, "Port feature id %x is not supported yet.\n",
+			 header.id);
+		return 0;
+	}
+
+	check_features_header(binfo->pdev, hdr, FPGA_DEVT_PORT, id);
+
+	return create_feature_instance(binfo, hdr, &port_features[id]);
+}
+
+static int parse_feature_port_uafu(struct build_feature_devs_info *binfo,
+				 struct feature_header *hdr)
+{
+	enum port_feature_id id = PORT_FEATURE_ID_UAFU;
+	int ret;
+
+	if (port_features[id].resource_size) {
+		ret = create_feature_instance(binfo, hdr, &port_features[id]);
+		port_features[id].resource_size = 0;
+	} else {
+		dev_err(&binfo->pdev->dev, "the uafu feature header is mis-configured.\n");
+		ret = -EINVAL;
+	}
+
+	return ret;
+}
+
+static int parse_feature_afus(struct build_feature_devs_info *binfo,
+			      struct feature_header *hdr)
+{
+	int ret;
+	struct feature_afu_header *afu_hdr, header;
+	void __iomem *start;
+	void __iomem *end = binfo->ioend;
+
+	start = hdr;
+	for (; start < end; start += header.next_afu) {
+		if (end - start < (sizeof(*afu_hdr) + sizeof(*hdr)))
+			return -EINVAL;
+
+		hdr = start;
+		afu_hdr = (struct feature_afu_header *) (hdr + 1);
+		header.csr = readq(&afu_hdr->csr);
+
+		if (feature_is_fme(afu_hdr)) {
+			ret = parse_feature_fme(binfo, hdr);
+			check_features_header(binfo->pdev,
+				hdr, FPGA_DEVT_FME, 0);
+			binfo->pfme_hdr = hdr;
+			if (ret)
+				return ret;
+		} else if (feature_is_port(afu_hdr)) {
+			ret = parse_feature_port(binfo, hdr);
+			check_features_header(binfo->pdev,
+				hdr, FPGA_DEVT_PORT, 0);
+			enable_port_uafu(binfo, hdr);
+			if (ret)
+				return ret;
+		} else if (feature_is_UAFU(binfo)) {
+			ret = parse_feature_port_uafu(binfo, hdr);
+			if (ret)
+				return ret;
+		} else
+			dev_info(&binfo->pdev->dev, "AFU GUID %pUl is not supported yet.\n",
+				 afu_hdr->guid.b);
+
+		if (!header.next_afu)
+			break;
+	}
+
+	return 0;
+}
+
+static int parse_feature_private(struct build_feature_devs_info *binfo,
+				 struct feature_header *hdr)
+{
+	struct feature_header header;
+
+	header.csr = readq(hdr);
+
+	if (!binfo->feature_dev) {
+		dev_err(&binfo->pdev->dev, "the private feature %x does not belong to any AFU.\n",
+			header.id);
+		return -EINVAL;
+	}
+
+	switch (feature_dev_id_type(binfo->feature_dev)) {
+	case FME_ID:
+		return parse_feature_fme_private(binfo, hdr);
+	case PORT_ID:
+		return parse_feature_port_private(binfo, hdr);
+	default:
+		dev_info(&binfo->pdev->dev, "private feature %x belonging to AFU %s is not supported yet.\n",
+			 header.id, binfo->feature_dev->name);
+	}
+	return 0;
+}
+
+static int parse_feature(struct build_feature_devs_info *binfo,
+			 struct feature_header *hdr)
+{
+	struct feature_header header;
+	int ret = 0;
+
+	header.csr = readq(hdr);
+
+	switch (header.type) {
+	case FEATURE_TYPE_AFU:
+		ret = parse_feature_afus(binfo, hdr);
+		break;
+	case FEATURE_TYPE_PRIVATE:
+		ret = parse_feature_private(binfo, hdr);
+		break;
+	default:
+		dev_info(&binfo->pdev->dev,
+			 "Feature Type %x is not supported.\n", hdr->type);
+	};
+
+	return ret;
+}
+
+static int
+parse_feature_list(struct build_feature_devs_info *binfo, void __iomem *start)
+{
+	struct feature_header *hdr, header;
+	void __iomem *end = binfo->ioend;
+	int ret = 0;
+
+	for (; start < end; start += header.next_header_offset) {
+		if (end - start < sizeof(*hdr)) {
+			dev_err(&binfo->pdev->dev, "The region is too small to contain a feature.\n");
+			ret =  -EINVAL;
+			break;
+		}
+
+		hdr = (struct feature_header *)start;
+		ret = parse_feature(binfo, hdr);
+		if (ret)
+			break;
+
+		header.csr = readq(hdr);
+		if (!header.next_header_offset)
+			break;
+	}
+
+	return ret;
+}
+
+static int parse_ports_from_fme(struct build_feature_devs_info *binfo)
+{
+	struct feature_fme_header *fme_hdr;
+	struct feature_fme_port port;
+	int i = 0, ret = 0;
+
+	if (binfo->pfme_hdr == NULL) {
+		dev_dbg(&binfo->pdev->dev, "VF is detected.\n");
+		return ret;
+	}
+
+	fme_hdr = binfo->pfme_hdr;
+
+	do {
+		port.csr = readq(&fme_hdr->port[i]);
+		if (!port.port_implemented)
+			break;
+
+		ret = parse_switch_to(binfo, port.port_bar);
+		if (ret)
+			break;
+
+		ret = parse_feature_list(binfo,
+				binfo->ioaddr + port.port_offset);
+		if (ret)
+			break;
+	} while (++i < MAX_FPGA_PORT_NUM);
+
+	return ret;
+}
+
+static int create_init_drvdata(struct pci_dev *pdev)
+{
+	struct cci_drvdata *drvdata;
+
+	drvdata = devm_kzalloc(&pdev->dev, sizeof(*drvdata), GFP_KERNEL);
+	if (!drvdata)
+		return -ENOMEM;
+
+	drvdata->device_id = alloc_fpga_id(PARENT_ID, NULL);
+	if (drvdata->device_id < 0) {
+		int ret = drvdata->device_id;
+
+		devm_kfree(&pdev->dev, drvdata);
+		return ret;
+	}
+
+	mutex_init(&drvdata->lock);
+	INIT_LIST_HEAD(&drvdata->port_dev_list);
+	INIT_LIST_HEAD(&drvdata->regions);
+
+	dev_set_drvdata(&pdev->dev, drvdata);
+	return 0;
+}
+
+static void destroy_drvdata(struct pci_dev *pdev)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pdev->dev);
+
+	if (drvdata->fme_dev) {
+		/* fme device should be unregistered first. */
+		WARN_ON(device_is_registered(drvdata->fme_dev));
+		free_fpga_id(FME_ID, to_platform_device(drvdata->fme_dev)->id);
+		put_device(drvdata->fme_dev);
+	}
+
+	cci_pci_remove_port_devs(pdev);
+	cci_pci_release_regions(pdev);
+	dev_set_drvdata(&pdev->dev, NULL);
+	free_fpga_id(PARENT_ID, drvdata->device_id);
+	devm_kfree(&pdev->dev, drvdata);
+}
+
+static struct class *fpga_class;
+
+struct device *fpga_create_parent_dev(struct pci_dev *pdev, int id)
+{
+	struct device *dev;
+
+	dev = device_create(fpga_class, &pdev->dev, MKDEV(0, 0), NULL,
+			    "intel-fpga-dev.%d", id);
+	if (IS_ERR(dev)) {
+		dev_err(&pdev->dev, "create parent device failed %ld.\n",
+			PTR_ERR(dev));
+		return dev;
+	}
+
+	/*
+	 * it is safe to modify some device fields here as:
+	 *   a) the device is not attached to any bus, i.e, no driver
+	 *      will match with this device;
+	 *   b) it is a single device, i.e, no child will access its
+	 *      resource.
+	 */
+	dev->dma_mask = pdev->dev.dma_mask;
+	dev->dma_parms = pdev->dev.dma_parms;
+	dev->coherent_dma_mask = pdev->dev.coherent_dma_mask;
+	return dev;
+}
+
+static int cci_pci_create_feature_devs(struct pci_dev *pdev)
+{
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pdev->dev);
+	struct build_feature_devs_info *binfo;
+	int ret;
+
+	binfo = build_info_alloc_and_init(pdev);
+	if (!binfo)
+		return -ENOMEM;
+
+	binfo->parent_dev = fpga_create_parent_dev(pdev, drvdata->device_id);
+	if (IS_ERR(binfo->parent_dev)) {
+		ret = PTR_ERR(binfo->parent_dev);
+		goto free_binfo_exit;
+	}
+
+	ret = parse_start(binfo);
+	if (ret)
+		goto free_binfo_exit;
+
+	ret = parse_feature_list(binfo, binfo->ioaddr);
+	if (ret)
+		goto free_binfo_exit;
+
+	ret = parse_ports_from_fme(binfo);
+	if (ret)
+		goto free_binfo_exit;
+
+	ret = build_info_commit_dev(binfo);
+	if (ret)
+		goto free_binfo_exit;
+
+	/*
+	 * everything is okay, reset ->parent_dev to stop it being
+	 * freed by build_info_free()
+	 */
+	binfo->parent_dev = NULL;
+
+free_binfo_exit:
+	build_info_free(binfo);
+	return ret;
+}
+
+/* PCI Device ID */
+#define PCIe_DEVICE_ID_RCiEP0_INTG_XEON     0xBCC0
+#define PCIe_DEVICE_ID_RCiEP0_DISCRETE      0x09C4
+/* VF Device */
+#define PCIe_DEVICE_ID_VF_INTG_XEON         0xBCC1
+#define PCIe_DEVICE_ID_VF_DISCRETE          0x09C5
+
+static struct pci_device_id cci_pcie_id_tbl[] = {
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCIe_DEVICE_ID_RCiEP0_INTG_XEON),},
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCIe_DEVICE_ID_VF_INTG_XEON),},
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCIe_DEVICE_ID_RCiEP0_DISCRETE),},
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCIe_DEVICE_ID_VF_DISCRETE),},
+	{0,}
+};
+MODULE_DEVICE_TABLE(pci, cci_pcie_id_tbl);
+
+static void port_config_vf(struct device *fme_dev, int port_id, bool is_vf)
+{
+	struct feature_fme_header *fme_hdr;
+	struct feature_fme_port port;
+	int type = is_vf ? FME_AFU_ACCESS_VF : FME_AFU_ACCESS_PF;
+
+	fme_hdr = get_feature_ioaddr_by_index(fme_dev,
+		FME_FEATURE_ID_HEADER);
+
+	WARN_ON(!fme_hdr);
+
+	port.csr = readq(&fme_hdr->port[port_id]);
+	WARN_ON(!port.port_implemented);
+
+	port.afu_access_control = type;
+	writeq(port.csr, &fme_hdr->port[port_id]);
+}
+
+static int cci_pci_sriov_configure(struct pci_dev *pcidev, int num_vfs)
+{
+	int ret;
+	int vf_ports = 0;
+	struct device *fme_dev;
+	struct cci_drvdata *drvdata = dev_get_drvdata(&pcidev->dev);
+	struct feature_platform_data *pdata;
+
+	mutex_lock(&drvdata->lock);
+
+	fme_dev = drvdata->fme_dev;
+	WARN_ON(!fme_dev);
+
+	if (drvdata->released_port_num < num_vfs) {
+		ret = -EBUSY;
+		goto unlock_exit;
+	}
+
+	if (!num_vfs)
+		pci_disable_sriov(pcidev);
+	else {
+		ret = pci_enable_sriov(pcidev, num_vfs);
+		if (ret)
+			goto unlock_exit;
+	}
+
+	list_for_each_entry(pdata, &drvdata->port_dev_list, node) {
+		int id = fpga_port_id(pdata->dev);
+
+		if (device_is_registered(&pdata->dev->dev))
+			continue;
+
+		if (!num_vfs) {
+			port_config_vf(fme_dev, id, false);
+			dev_dbg(&pcidev->dev, "port_%d is turned to PF.\n", id);
+			continue;
+		}
+
+		port_config_vf(fme_dev, id, true);
+		dev_dbg(&pcidev->dev, "port_%d is turned to VF.\n", id);
+		if (++vf_ports == num_vfs)
+			break;
+	}
+
+	ret = num_vfs;
+unlock_exit:
+	mutex_unlock(&drvdata->lock);
+	return ret;
+}
+
+static
+int cci_pci_probe(struct pci_dev *pcidev, const struct pci_device_id *pcidevid)
+{
+	int ret;
+
+	ret = pci_enable_device(pcidev);
+	if (ret < 0) {
+		dev_err(&pcidev->dev, "Failed to enable device %d.\n", ret);
+		goto exit;
+	}
+
+	ret = pci_enable_pcie_error_reporting(pcidev);
+	if (ret && ret != -EINVAL)
+		dev_info(&pcidev->dev, "PCIE AER unavailable %d.\n", ret);
+
+	ret = pci_request_regions(pcidev, DRV_NAME);
+	if (ret) {
+		dev_err(&pcidev->dev, "Failed to request regions.\n");
+		goto disable_error_report_exit;
+	}
+
+	pci_set_master(pcidev);
+	pci_save_state(pcidev);
+
+	if (!dma_set_mask(&pcidev->dev, DMA_BIT_MASK(64))) {
+		dma_set_coherent_mask(&pcidev->dev, DMA_BIT_MASK(64));
+	} else if (!dma_set_mask(&pcidev->dev, DMA_BIT_MASK(32))) {
+		dma_set_coherent_mask(&pcidev->dev, DMA_BIT_MASK(32));
+	} else {
+		ret = -EIO;
+		dev_err(&pcidev->dev, "No suitable DMA support available.\n");
+		goto release_region_exit;
+	}
+
+	ret = create_init_drvdata(pcidev);
+	if (ret)
+		goto release_region_exit;
+
+	ret = cci_pci_create_feature_devs(pcidev);
+	if (ret)
+		goto destroy_drvdata_exit;
+
+	return 0;
+
+destroy_drvdata_exit:
+	destroy_drvdata(pcidev);
+release_region_exit:
+	pci_release_regions(pcidev);
+disable_error_report_exit:
+	pci_disable_pcie_error_reporting(pcidev);
+	pci_disable_device(pcidev);
+exit:
+	return ret;
+}
+
+static
+void cci_pci_remove(struct pci_dev *pcidev)
+{
+	/* disable sriov. */
+	if (dev_is_pf(&pcidev->dev))
+		cci_pci_sriov_configure(pcidev, 0);
+
+	remove_all_devs(pcidev);
+
+	pci_disable_pcie_error_reporting(pcidev);
+
+	destroy_drvdata(pcidev);
+	pci_release_regions(pcidev);
+	pci_disable_device(pcidev);
+}
+
+static struct pci_driver cci_pci_driver = {
+	.name = DRV_NAME,
+	.id_table = cci_pcie_id_tbl,
+	.probe = cci_pci_probe,
+	.remove = cci_pci_remove,
+	.sriov_configure = cci_pci_sriov_configure
+};
+
+static int __init ccidrv_init(void)
+{
+	int ret;
+
+	pr_info("Intel(R) FPGA PCIe Driver: Version %s\n", DRV_VERSION);
+
+	fpga_ids_init();
+
+	ret = fpga_chardev_init();
+	if (ret)
+		goto exit_ids;
+
+	fpga_class = class_create(THIS_MODULE, "fpga");
+	if (IS_ERR(fpga_class)) {
+		ret = PTR_ERR(fpga_class);
+		goto exit_chardev;
+	}
+
+	ret = pci_register_driver(&cci_pci_driver);
+	if (ret) {
+		class_destroy(fpga_class);
+exit_chardev:
+		fpga_chardev_uinit();
+exit_ids:
+		fpga_ids_destroy();
+	}
+
+	return ret;
+}
+
+static void __exit ccidrv_exit(void)
+{
+	pci_unregister_driver(&cci_pci_driver);
+	class_destroy(fpga_class);
+	fpga_chardev_uinit();
+	fpga_ids_destroy();
+}
+
+module_init(ccidrv_init);
+module_exit(ccidrv_exit);
+
+MODULE_DESCRIPTION("FPGA PCIe Device Drive");
+MODULE_AUTHOR("Intel Corporation");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/fpga/intel/pcie_check.c b/drivers/fpga/intel/pcie_check.c
new file mode 100644
index 000000000000..e707d72aad4a
--- /dev/null
+++ b/drivers/fpga/intel/pcie_check.c
@@ -0,0 +1,121 @@
+/*
+ * check the pcie parsed header with the default value in SAS spec
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Zhang Yi <Yi.Z.Zhang@intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ */
+
+#include <linux/pci.h>
+#include <linux/kdev_t.h>
+#include <linux/stddef.h>
+#include "feature-dev.h"
+
+#define DFH_CCI_VERSION				0x1
+#define DFH_CCI_MINREVERSION			0x0
+#define DFH_TYPE_PRIVATE			0x3
+#define DFH_TYPE_AFU				0x1
+
+#define FME_FEATURE_HEADER_TYPE			DFH_TYPE_AFU
+#define FME_FEATURE_HEADER_NEXT_OFFSET		0x1000
+#define FME_FEATURE_HEADER_ID			DFH_CCI_VERSION
+#define FME_FEATURE_HEADER_VERSION		DFH_CCI_MINREVERSION
+
+#define FME_FEATURE_THERMAL_MGMT_TYPE		DFH_TYPE_PRIVATE
+#define FME_FEATURE_THERMAL_MGMT_NEXT_OFFSET	0x1000
+#define FME_FEATURE_THERMAL_MGMT_ID		0x1
+#define FME_FEATURE_THERMAL_MGMT_VERSION	0x0
+
+#define FME_FEATURE_POWER_MGMT_TYPE		DFH_TYPE_PRIVATE
+#define FME_FEATURE_POWER_MGMT_NEXT_OFFSET	0x1000
+#define FME_FEATURE_POWER_MGMT_ID		0x2
+#define FME_FEATURE_POWER_MGMT_VERSION		0x0
+
+#define FME_FEATURE_GLOBAL_PERF_TYPE		DFH_TYPE_PRIVATE
+#define FME_FEATURE_GLOBAL_PERF_NEXT_OFFSET	0x1000
+#define FME_FEATURE_GLOBAL_PERF_ID		0x3
+#define FME_FEATURE_GLOBAL_PERF_VERSION		0x0
+
+#define FME_FEATURE_GLOBAL_ERR_TYPE		DFH_TYPE_PRIVATE
+#define FME_FEATURE_GLOBAL_ERR_NEXT_OFFSET	0x1000
+#define FME_FEATURE_GLOBAL_ERR_ID		0x4
+#define FME_FEATURE_GLOBAL_ERR_VERSION		0x0
+
+#define FME_FEATURE_PR_MGMT_TYPE		DFH_TYPE_PRIVATE
+#define FME_FEATURE_PR_MGMT_NEXT_OFFSET		0x0
+#define FME_FEATURE_PR_MGMT_ID			0x5
+#define FME_FEATURE_PR_MGMT_VERSION		0x0
+
+#define PORT_FEATURE_HEADER_TYPE		DFH_TYPE_AFU
+#define PORT_FEATURE_HEADER_NEXT_OFFSET		0x1000
+#define PORT_FEATURE_HEADER_ID			DFH_CCI_VERSION
+#define PORT_FEATURE_HEADER_VERSION		DFH_CCI_MINREVERSION
+
+#define PORT_FEATURE_ERR_TYPE			DFH_TYPE_PRIVATE
+#define PORT_FEATURE_ERR_NEXT_OFFSET		0x1000
+#define PORT_FEATURE_ERR_ID			0x10
+#define PORT_FEATURE_ERR_VERSION		0x0
+
+#define PORT_FEATURE_UMSG_TYPE			DFH_TYPE_PRIVATE
+#define PORT_FEATURE_UMSG_NEXT_OFFSET		0x2000
+#define PORT_FEATURE_UMSG_ID			0x11
+#define PORT_FEATURE_UMSG_VERSION		0x0
+
+#define PORT_FEATURE_STP_TYPE			DFH_TYPE_PRIVATE
+#define PORT_FEATURE_STP_NEXT_OFFSET		0x0
+#define PORT_FEATURE_STP_ID			0x13
+#define PORT_FEATURE_STP_VERSION		0x0
+
+#define DEFAULT_REG(name)	{.id = name##_ID, .revision = name##_VERSION,\
+				.next_header_offset = name##_NEXT_OFFSET,\
+				.type = name##_TYPE,}
+
+static struct feature_header default_port_feature_hdr[] = {
+	DEFAULT_REG(PORT_FEATURE_HEADER),
+	DEFAULT_REG(PORT_FEATURE_ERR),
+	DEFAULT_REG(PORT_FEATURE_UMSG),
+	{.csr = 0,},
+	DEFAULT_REG(PORT_FEATURE_STP),
+	{.csr = 0,},
+};
+
+static struct feature_header default_fme_feature_hdr[] = {
+	DEFAULT_REG(FME_FEATURE_HEADER),
+	DEFAULT_REG(FME_FEATURE_THERMAL_MGMT),
+	DEFAULT_REG(FME_FEATURE_POWER_MGMT),
+	DEFAULT_REG(FME_FEATURE_GLOBAL_PERF),
+	DEFAULT_REG(FME_FEATURE_GLOBAL_ERR),
+	DEFAULT_REG(FME_FEATURE_PR_MGMT),
+};
+
+void check_features_header(struct pci_dev *pdev, struct feature_header *hdr,
+			   enum fpga_devt_type type, int id)
+{
+	struct feature_header *default_header, header;
+
+	if (type == FPGA_DEVT_FME) {
+		default_header = default_fme_feature_hdr;
+		WARN_ON(id >= ARRAY_SIZE(default_fme_feature_hdr));
+	} else if (type == FPGA_DEVT_PORT) {
+		default_header = default_port_feature_hdr;
+		WARN_ON(id >= ARRAY_SIZE(default_port_feature_hdr));
+	} else {
+		WARN_ON(1);
+		return;
+	}
+
+	header.csr = readq(hdr);
+
+	if (memcmp(&header, default_header + id, sizeof(header)))
+		dev_err(&pdev->dev,
+			"check header failed. current hdr:%llx - default_value:%llx.\n",
+			header.csr, *(u64 *)(default_header + id));
+	else
+		dev_dbg(&pdev->dev,
+			"check header pass.\n");
+}
diff --git a/drivers/fpga/intel/region.c b/drivers/fpga/intel/region.c
new file mode 100644
index 000000000000..8819561f1392
--- /dev/null
+++ b/drivers/fpga/intel/region.c
@@ -0,0 +1,130 @@
+/*
+ * Driver for FPGA Accelerated Function Unit (AFU) Region Management
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Wu Hao <hao.wu@linux.intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#include "afu.h"
+
+void afu_region_init(struct feature_platform_data *pdata)
+{
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+
+	INIT_LIST_HEAD(&afu->regions);
+}
+
+#define for_each_region(region, afu)	\
+	list_for_each_entry((region), &(afu)->regions, node)
+static struct fpga_afu_region *get_region_by_index(struct fpga_afu *afu,
+						   u32 region_index)
+{
+	struct fpga_afu_region *region;
+
+	for_each_region(region, afu)
+		if (region->index == region_index)
+			return region;
+
+	return NULL;
+}
+
+int afu_region_add(struct feature_platform_data *pdata, u32 region_index,
+		   u64 region_size, u64 phys, u32 flags)
+{
+	struct fpga_afu_region *region;
+	struct fpga_afu *afu;
+	int ret = 0;
+
+	region = devm_kzalloc(&pdata->dev->dev, sizeof(*region), GFP_KERNEL);
+	if (!region)
+		return -ENOMEM;
+
+	region->index = region_index;
+	region->size = region_size;
+	region->phys = phys;
+	region->flags = flags;
+
+	mutex_lock(&pdata->lock);
+
+	afu = fpga_pdata_get_private(pdata);
+
+	/* check if @index already exists */
+	if (get_region_by_index(afu, region_index)) {
+		mutex_unlock(&pdata->lock);
+		ret = -EEXIST;
+		goto exit;
+	}
+
+	region_size = PAGE_ALIGN(region_size);
+	region->offset = afu->region_cur_offset;
+	list_add(&region->node, &afu->regions);
+
+	afu->region_cur_offset += region_size;
+	afu->num_regions++;
+	mutex_unlock(&pdata->lock);
+	return 0;
+
+exit:
+	devm_kfree(&pdata->dev->dev, region);
+	return ret;
+}
+
+void afu_region_destroy(struct feature_platform_data *pdata)
+{
+	struct fpga_afu_region *tmp, *region;
+	struct fpga_afu *afu = fpga_pdata_get_private(pdata);
+
+	list_for_each_entry_safe(region, tmp, &afu->regions, node) {
+		list_del(&region->node);
+		devm_kfree(&pdata->dev->dev, region);
+	}
+}
+
+int afu_get_region_by_index(struct feature_platform_data *pdata,
+			    u32 region_index, struct fpga_afu_region *pregion)
+{
+	struct fpga_afu_region *region;
+	struct fpga_afu *afu;
+	int ret = 0;
+
+	mutex_lock(&pdata->lock);
+	afu = fpga_pdata_get_private(pdata);
+	region = get_region_by_index(afu, region_index);
+	if (!region) {
+		ret = -EINVAL;
+		goto exit;
+	}
+	*pregion = *region;
+exit:
+	mutex_unlock(&pdata->lock);
+	return ret;
+}
+
+int afu_get_region_by_offset(struct feature_platform_data *pdata,
+			    u64 offset, u64 size,
+			    struct fpga_afu_region *pregion)
+{
+	struct fpga_afu_region *region;
+	struct fpga_afu *afu;
+	int ret = 0;
+
+	mutex_lock(&pdata->lock);
+	afu = fpga_pdata_get_private(pdata);
+	for_each_region(region, afu)
+		if (region->offset <= offset &&
+		   region->offset + region->size >= offset + size) {
+			*pregion = *region;
+			goto exit;
+		}
+	ret = -EINVAL;
+exit:
+	mutex_unlock(&pdata->lock);
+	return ret;
+}
diff --git a/include/uapi/linux/intel-fpga.h b/include/uapi/linux/intel-fpga.h
new file mode 100644
index 000000000000..2a28e39df981
--- /dev/null
+++ b/include/uapi/linux/intel-fpga.h
@@ -0,0 +1,282 @@
+/*
+ * Header File for Intel FPGA User API
+ *
+ * Copyright 2016 Intel Corporation, Inc.
+ *
+ * Authors:
+ *   Kang Luwei <luwei.kang@intel.com>
+ *   Zhang Yi <yi.z.zhang@intel.com>
+ *   Wu Hao <hao.wu@linux.intel.com>
+ *   Xiao Guangrong <guangrong.xiao@linux.intel.com>
+ *
+ * This work is licensed under the terms of the GNU GPL version 2. See
+ * the COPYING file in the top-level directory.
+ *
+ */
+
+#ifndef _UAPI_INTEL_FPGA_H
+#define _UAPI_INTEL_FPGA_H
+
+#ifdef USER_LINUX_TYPES
+#include <intel-fpga/glist.h>
+#else
+#include <linux/types.h>
+#endif
+
+#define FPGA_API_VERSION 0
+
+#define INTEL_FPGA_DRIVER_VER_MAJOR  0
+#define INTEL_FPGA_DRIVER_VER_MINOR  9
+#define INTEL_FPGA_DRIVER_VER_REV    0
+#define INTEL_FPGA_DRIVER_VER_BUILD  
+#define INTEL_FPGA_DRIVER_VERSION    "0.9.0"
+
+/*
+ * The IOCTL interface for Intel FPGA is designed for extensibility by
+ * embedding the structure length (argsz) and flags into structures passed
+ * between kernel and userspace. This design referenced the VFIO IOCTL
+ * interface (include/uapi/linux/vfio.h).
+ */
+
+#define FPGA_MAGIC 0xB5
+
+#define FPGA_BASE 0
+#define PORT_BASE 0x40
+#define FME_BASE 0x80
+
+/* Common IOCTLs for both FME and AFU file descriptor */
+
+/**
+ * FPGA_GET_API_VERSION - _IO(FPGA_MAGIC, FPGA_BASE + 0)
+ *
+ * Report the version of the driver API.
+ * Return: Driver API Version.
+ */
+
+#define FPGA_GET_API_VERSION	_IO(FPGA_MAGIC, FPGA_BASE + 0)
+
+/**
+ * FPGA_CHECK_EXTENSION - _IO(FPGA_MAGIC, FPGA_BASE + 1)
+ *
+ * Check whether an extension is supported.
+ * Return: 0 if not supported, otherwise the extension is supported.
+ */
+
+#define FPGA_CHECK_EXTENSION	_IO(FPGA_MAGIC, FPGA_BASE + 1)
+
+/* IOCTLs for AFU file descriptor */
+
+/**
+ * FPGA_PORT_RESET - _IO(FPGA_MAGIC, PORT_BASE + 0)
+ *
+ * Reset the FPGA AFU Port. No parameters are supported.
+ * Return: 0 on success, -errno of failure
+ */
+
+#define FPGA_PORT_RESET		_IO(FPGA_MAGIC, PORT_BASE + 0)
+
+/**
+ * FPGA_PORT_GET_INFO - _IOR(FPGA_MAGIC, PORT_BASE + 1, struct fpga_port_info)
+ *
+ * Retrieve information about the fpga port.
+ * Driver fills the info in provided struct fpga_port_info.
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_port_info {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	/* Output */
+	__u32 flags;		/* Zero for now */
+	__u32 num_regions;	/* The number of supported regions */
+	__u32 num_umsgs;	/* The number of allocated umsgs */
+};
+
+#define FPGA_PORT_GET_INFO	_IO(FPGA_MAGIC, PORT_BASE + 1)
+
+/**
+ * FPGA_PORT_GET_REGION_INFO - _IOWR(FPGA_MAGIC, PORT_BASE + 2,
+ *						struct fpga_port_region_info)
+ *
+ * Retrieve information about a device region.
+ * Caller provides struct fpga_port_region_info with index value set.
+ * Driver returns the region info in other fields.
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_port_region_info {
+	/* input */
+	__u32 argsz;		/* Structure length */
+	/* Output */
+	__u32 flags;		/* Access permission */
+#define FPGA_REGION_READ	(1 << 0)	/* Region is readable */
+#define FPGA_REGION_WRITE	(1 << 1)	/* Region is writable */
+#define FPGA_REGION_MMAP	(1 << 2)	/* Can be mmaped to userspace */
+	/* Input */
+	__u32 index;		/* Region index */
+#define FPGA_PORT_INDEX_UAFU	0		/* User AFU */
+#define FPGA_PORT_INDEX_STP	1		/* Signal Tap */
+	__u32 padding;
+	/* Output */
+	__u64 size;		/* Region size (bytes) */
+	__u64 offset;		/* Region offset from start of device fd */
+};
+
+#define FPGA_PORT_GET_REGION_INFO	_IO(FPGA_MAGIC, PORT_BASE + 2)
+
+/**
+ * FPGA_PORT_DMA_MAP - _IOWR(FPGA_MAGIC, PORT_BASE + 3,
+ *						struct fpga_port_dma_map)
+ *
+ * Map the dma memory per user_addr and length which are provided by caller.
+ * Driver fills the iova in provided struct afu_port_dma_map.
+ * This interface only accepts page-size aligned user memory for dma mapping.
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_port_dma_map {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__u64 user_addr;        /* Process virtual address */
+	__u64 length;           /* Length of mapping (bytes)*/
+	/* Output */
+	__u64 iova;             /* IO virtual address */
+};
+
+#define FPGA_PORT_DMA_MAP	_IO(FPGA_MAGIC, PORT_BASE + 3)
+
+/**
+ * FPGA_PORT_DMA_UNMAP - _IOW(FPGA_MAGIC, PORT_BASE + 4,
+ *						struct fpga_port_dma_unmap)
+ *
+ * Unmap the dma memory per iova provided by caller.
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_port_dma_unmap {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__u64 iova;		/* IO virtual address */
+};
+
+#define FPGA_PORT_DMA_UNMAP	_IO(FPGA_MAGIC, PORT_BASE + 4)
+
+/**
+ * FPGA_PORT_UMSG_ENABLE - _IO(FPGA_MAGIC, PORT_BASE + 5)
+ * FPGA_PORT_UMSG_DISABLE - _IO(FPGA_MAGIC, PORT_BASE + 6)
+ *
+ * Interfaces to control UMSG function. No parameters are supported.
+ * Return: 0 on success, -errno on failure.
+ */
+
+#define FPGA_PORT_UMSG_ENABLE	_IO(FPGA_MAGIC, PORT_BASE + 5)
+#define FPGA_PORT_UMSG_DISABLE	_IO(FPGA_MAGIC, PORT_BASE + 6)
+
+/**
+ * FPGA_PORT_UMSG_SET_MODE - _IOW(FPGA_MAGIC, PORT_BASE + 7,
+ *						struct fpga_port_umsg_cfg)
+ *
+ * Set Hint Mode per bitmap provided by caller. One bit for each page
+ * in hint_bitmap. 0 - Disable and 1 - Enable Hint Mode.
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_port_umsg_cfg {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__u32 hint_bitmap;	/* UMSG Hint Mode Bitmap */
+};
+
+#define FPGA_PORT_UMSG_SET_MODE		_IO(FPGA_MAGIC, PORT_BASE + 7)
+
+/**
+ * FPGA_PORT_UMSG_SET_BASE_ADDR - _IOW(FPGA_MAGIC, PORT_BASE + 8,
+ *						struct afu_port_umsg_base_addr)
+ *
+ * Set UMSG base address per iova provided by caller. Driver configures the
+ * UMSG base address with the iova, but only accept iova which get from the
+ * DMA_MAP IOCTL interface and the DMA region is big enough for all UMSGs
+ * (num_umsg * PAGE_SIZE)
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_port_umsg_base_addr {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__u64 iova;		/* IO virtual address */
+};
+
+#define FPGA_PORT_UMSG_SET_BASE_ADDR	_IO(FPGA_MAGIC, PORT_BASE + 8)
+
+/* IOCTLs for FME file descriptor */
+
+/**
+ * FPGA_FME_PORT_PR - _IOWR(FPGA_MAGIC, FME_BASE + 0, struct fpga_fme_port_pr)
+ *
+ * Driver does Partial Reconfiguration based on Port ID and Buffer (Image)
+ * provided by caller.
+ * Return: 0 on success, -errno on failure.
+ * If FPGA_FME_PORT_PR returns -EIO, that indicates the HW has detected
+ * some errors during PR, under this case, the user can fetch HW error code
+ * from fpga_fme_port_pr.status. Each bit on the error code is used as the
+ * index for the array created by DEFINE_FPGA_PR_ERR_MSG().
+ * Otherwise, it is always zero.
+ */
+
+#define DEFINE_FPGA_PR_ERR_MSG(_name_)			\
+static const char * const _name_[] = {			\
+	"PR operation error detected",			\
+	"PR CRC error detected",			\
+	"PR incompatiable bitstream error detected",	\
+	"PR IP protocol error detected",		\
+	"PR FIFO overflow error detected",		\
+	"PR timeout error detected",			\
+	"PR secure load error detected",		\
+}
+
+#define PR_MAX_ERR_NUM	7
+
+struct fpga_fme_port_pr {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__u32 port_id;
+	__u32 buffer_size;
+	__u64 buffer_address;	/* Userspace address to the buffer for PR */
+	/* Output */
+	__u64 status;		/* HW error code if ioctl returns -EIO */
+};
+
+#define FPGA_FME_PORT_PR	_IO(FPGA_MAGIC, FME_BASE + 0)
+
+/**
+ * FPGA_FME_PORT_RELEASE - _IOW(FPGA_MAGIC, FME_BASE + 1,
+ *						struct fpga_fme_port_release)
+ *
+ * Driver releases the port per Port ID provided by caller.
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_fme_port_release {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__u32 port_id;
+};
+
+#define FPGA_FME_PORT_RELEASE	_IO(FPGA_MAGIC, FME_BASE + 1)
+
+/**
+ * FPGA_FME_PORT_ASSIGN - _IOW(FPGA_MAGIC, FME_BASE + 2,
+ *						struct fpga_fme_port_assign)
+ *
+ * Driver assigns the port per Port ID provided by caller.
+ * Return: 0 on success, -errno on failure.
+ */
+struct fpga_fme_port_assign {
+	/* Input */
+	__u32 argsz;		/* Structure length */
+	__u32 flags;		/* Zero for now */
+	__u32 port_id;
+};
+
+#define FPGA_FME_PORT_ASSIGN	_IO(FPGA_MAGIC, FME_BASE + 2)
+
+#endif /* _UAPI_INTEL_FPGA_H */
-- 
2.15.1

